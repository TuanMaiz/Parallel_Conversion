{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive ANN-SNN Conversion Experiments\n",
    "\n",
    "This notebook implements comprehensive experiments for ANN-SNN conversion with the following configurations:\n",
    "- **Datasets**: imdb, ag_news\n",
    "- **Models**: bert-base-uncased, distilbert-base-uncased\n",
    "- **Timesteps**: 2, 4\n",
    "- **Neuron Types**: ParaInfNeuron_Text (parallel), IFNeuron_Text (sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Dependencies\n",
    "!rm -rf /content/Parallel_Conversion\n",
    "!git clone -b add-calib https://github.com/TuanMaiz/Parallel_Conversion.git\n",
    "\n",
    "# Install required packages\n",
    "!pip install transformers datasets torch tqdm fvcore psutil pandas matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    \"datasets\": [\"imdb\", \"ag_news\"],\n",
    "    \"models\": [\"bert_base_qcfs\", \"distilbert_base_qcfs\"],\n",
    "    \"timesteps\": [2, 4],\n",
    "    \"neuron_types\": [\"ParaInfNeuron_Text\", \"IFNeuron_Text\"],\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 0.00001,\n",
    "    \"epochs\": 5,\n",
    "    \"text_max_len\": 256,\n",
    "    \"gpu_type\": \"A100\"\n",
    "}\n",
    "\n",
    "# Data Collection Framework\n",
    "class ExperimentDataCollector:\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        self.experiment_log = \"experiment_results.json\"\n",
    "        \n",
    "    def add_result(self, config, metrics):\n",
    "        result = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"config\": config,\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        self.save_results()\n",
    "        \n",
    "    def save_results(self):\n",
    "        with open(self.experiment_log, \"w\") as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "            \n",
    "    def load_results(self):\n",
    "        if os.path.exists(self.experiment_log):\n",
    "            with open(self.experiment_log, \"r\") as f:\n",
    "                self.results = json.load(f)\n",
    "                \n",
    "    def get_dataframe(self):\n",
    "        return pd.json_normalize(self.results)\n",
    "\n",
    "# Initialize collector\n",
    "collector = ExperimentDataCollector()\n",
    "collector.load_results()\n",
    "\n",
    "print(f\"Loaded {len(collector.results)} previous results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Execution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(dataset, model, timestep, neuron_type, phase=\"training\"):\n",
    "    \n",
    "    # Construct save directory\n",
    "    savedir = f\"./checkpoints\"\n",
    "    \n",
    "    # Base command\n",
    "    if phase == \"training\":\n",
    "        cmd = f\"\"\"python Parallel_Conversion/main.py \\\n",
    "            --dataset TextCLS \\\n",
    "            --net_arch {model} \\\n",
    "            --savedir {savedir} \\\n",
    "            --neuron_type {neuron_type} \\\n",
    "            --text_dataset {dataset} \\\n",
    "            --text_max_len {EXPERIMENT_CONFIG['text_max_len']} \\\n",
    "            --time_step {timestep} \\\n",
    "            --trainsnn_epochs {EXPERIMENT_CONFIG['epochs']} \\\n",
    "            --batchsize {EXPERIMENT_CONFIG['batch_size']} \\\n",
    "            --lr {EXPERIMENT_CONFIG['learning_rate']} \\\n",
    "            --measure_efficiency \\\n",
    "            --gpu_type {EXPERIMENT_CONFIG['gpu_type']} \\\n",
    "            --dev 0\"\"\"\n",
    "    \n",
    "    elif phase == \"calibration\":\n",
    "        # Fix the checkpoint path format to match the actual naming pattern\n",
    "        checkpoint_path = f\"{savedir}TextCLS-{model}-T{timestep}/{neuron_type}_lr{EXPERIMENT_CONFIG['learning_rate']}_wd0.0005_epoch{EXPERIMENT_CONFIG['epochs']}_mixup_False_weights_epoch_{EXPERIMENT_CONFIG['epochs']-1}.pth\"\n",
    "        \n",
    "        cmd = f\"\"\"python Parallel_Conversion/main.py \\\n",
    "            --dataset TextCLS \\\n",
    "            --net_arch {model} \\\n",
    "            --neuron_type {neuron_type} \\\n",
    "            --text_dataset {dataset} \\\n",
    "            --text_max_len {EXPERIMENT_CONFIG['text_max_len']} \\\n",
    "            --time_step {timestep} \\\n",
    "            --batchsize {EXPERIMENT_CONFIG['batch_size']} \\\n",
    "            --measure_efficiency \\\n",
    "            --gpu_type {EXPERIMENT_CONFIG['gpu_type']} \\\n",
    "            --dev 0 \\\n",
    "            --calibrate_th \\\n",
    "            --direct_inference \\\n",
    "            --pretrained_model \\\n",
    "            --checkpoint_path {checkpoint_path}\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== Running {phase} for {dataset} + {model} + {neuron_type} + T{timestep} ===\")\n",
    "    \n",
    "    # Create progress bar for this phase\n",
    "    phase_pbar = tqdm(total=100, desc=f\"{phase.capitalize()}\", unit=\"%\", leave=False)\n",
    "    \n",
    "    # Execute and capture output\n",
    "    start_time = time.time()\n",
    "    \n",
    "    def update_progress_bar():\n",
    "        \"\"\"Update progress bar while waiting for subprocess to complete\"\"\"\n",
    "        elapsed = time.time() - start_time\n",
    "        # Simulate progress based on time (rough estimate)\n",
    "        if phase == \"training\":\n",
    "            # Training takes longer, estimate based on 2 hours max\n",
    "            progress = min(100, (elapsed / 7200) * 100)\n",
    "        else:\n",
    "            # Calibration is faster, estimate based on 30 minutes max\n",
    "            progress = min(100, (elapsed / 1800) * 100)\n",
    "        \n",
    "        phase_pbar.update(int(progress - phase_pbar.n))\n",
    "        return progress < 100\n",
    "    \n",
    "    try:\n",
    "        # Start subprocess\n",
    "        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        \n",
    "        # Monitor process and update progress bar\n",
    "        while process.poll() is None:\n",
    "            if not update_progress_bar():\n",
    "                process.terminate()\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # Get final result\n",
    "        stdout, stderr = process.communicate()\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Final progress bar update\n",
    "        phase_pbar.update(100 - phase_pbar.n)\n",
    "        phase_pbar.close()\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            print(f\"✅ {phase} completed successfully in {execution_time:.2f}s\")\n",
    "            return True, stdout, execution_time\n",
    "        else:\n",
    "            print(f\"❌ {phase} failed with error:\")\n",
    "            print(stderr)\n",
    "            return False, stderr, execution_time\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        phase_pbar.close()\n",
    "        print(f\"❌ {phase} timed out after 2 hours\")\n",
    "        return False, \"Timeout\", 7200\n",
    "    except Exception as e:\n",
    "        phase_pbar.close()\n",
    "        print(f\"❌ {phase} failed with exception: {e}\")\n",
    "        return False, str(e), 0\n",
    "\n",
    "def parse_metrics_from_output(output_text):\n",
    "    metrics = {}\n",
    "    import re\n",
    "    \n",
    "    accuracy_patterns = [\n",
    "        r'Accuracy\\s*:?\\s*(\\d+\\.\\d+)',\n",
    "        r'acc\\s*:?\\s*(\\d+\\.\\d+)',\n",
    "        r'test_accuracy\\s*:?\\s*(\\d+\\.\\d+)',\n",
    "        r'val_accuracy\\s*:?\\s*(\\d+\\.\\d+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in accuracy_patterns:\n",
    "        matches = re.findall(pattern, output_text)\n",
    "        if matches:\n",
    "            metrics['accuracy'] = float(matches[-1])\n",
    "            break\n",
    "    \n",
    "    if 'FLOPs' in output_text:\n",
    "        flops_match = re.search(r'FLOPs\\s*:?\\s*(\\d+\\.?\\d*\\s*[eE]?[+-]?\\d*)', output_text)\n",
    "        if flops_match:\n",
    "            metrics['flops'] = float(flops_match.group(1))\n",
    "    \n",
    "    if 'Parameters' in output_text:\n",
    "        params_match = re.search(r'Parameters\\s*:?\\s*(\\d+\\.?\\d*\\s*[eE]?[+-]?\\d*)', output_text)\n",
    "        if params_match:\n",
    "            metrics['parameters'] = float(params_match.group(1))\n",
    "    \n",
    "    if 'Memory' in output_text:\n",
    "        memory_match = re.search(r'Memory\\s*:?\\s*(\\d+\\.\\d+)\\s*GB', output_text)\n",
    "        if memory_match:\n",
    "            metrics['memory_gb'] = float(memory_match.group(1))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def run_complete_experiment(dataset, model, timestep, neuron_type):\n",
    "    \n",
    "    config = {\n",
    "        \"dataset\": dataset,\n",
    "        \"model\": model,\n",
    "        \"timestep\": timestep,\n",
    "        \"neuron_type\": neuron_type,\n",
    "        \"batch_size\": EXPERIMENT_CONFIG['batch_size'],\n",
    "        \"learning_rate\": EXPERIMENT_CONFIG['learning_rate'],\n",
    "        \"epochs\": EXPERIMENT_CONFIG['epochs']\n",
    "    }\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Phase 1: Training\n",
    "    success, output, training_time = run_single_experiment(dataset, model, timestep, neuron_type, \"training\")\n",
    "    \n",
    "    if success:\n",
    "        training_metrics = parse_metrics_from_output(output)\n",
    "        metrics.update(training_metrics)\n",
    "        metrics['training_time'] = training_time\n",
    "        metrics['training_success'] = True\n",
    "        \n",
    "        # Phase 2: Calibration\n",
    "        success, calib_output, calib_time = run_single_experiment(dataset, model, timestep, neuron_type, \"calibration\")\n",
    "        \n",
    "        if success:\n",
    "            calib_metrics = parse_metrics_from_output(calib_output)\n",
    "            metrics.update({f\"calib_{k}\": v for k, v in calib_metrics.items()})\n",
    "            metrics['calibration_time'] = calib_time\n",
    "            metrics['calibration_success'] = True\n",
    "        else:\n",
    "            metrics['calibration_success'] = False\n",
    "            metrics['calibration_time'] = calib_time\n",
    "    else:\n",
    "        metrics['training_success'] = False\n",
    "        metrics['training_time'] = training_time\n",
    "    \n",
    "    # Store results\n",
    "    collector.add_result(config, metrics)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test run - single experiment\n",
    "test_config = {\n",
    "    \"dataset\": \"imdb\",\n",
    "    \"model\": \"distilbert_base_qcfs\",\n",
    "    \"timestep\": 2,\n",
    "    \"neuron_type\": \"ParaInfNeuron_Text\"\n",
    "}\n",
    "\n",
    "print(\"Running test experiment...\")\n",
    "test_metrics = run_complete_experiment(**test_config)\n",
    "\n",
    "print(\"\\nTest experiment results:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Experiment Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all experiment combinations\n",
    "from itertools import product\n",
    "\n",
    "all_experiments = list(product(\n",
    "    EXPERIMENT_CONFIG['datasets'],\n",
    "    EXPERIMENT_CONFIG['models'], \n",
    "    EXPERIMENT_CONFIG['timesteps'],\n",
    "    EXPERIMENT_CONFIG['neuron_types']\n",
    "))\n",
    "\n",
    "total_experiments = len(all_experiments)\n",
    "print(f\"Total experiments to run: {total_experiments}\")\n",
    "print(\"Experiment combinations:\")\n",
    "for i, (dataset, model, timestep, neuron_type) in enumerate(all_experiments):\n",
    "    print(f\"{i+1:2d}. {dataset} + {model} + {neuron_type} + T{timestep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments (uncomment to execute)\n",
    "def run_all_experiments():\n",
    "    # Create tqdm progress bar for all experiments\n",
    "    experiment_pbar = tqdm(all_experiments, desc=\"Overall Progress\", unit=\"experiment\")\n",
    "    \n",
    "    for i, (dataset, model, timestep, neuron_type) in enumerate(experiment_pbar):\n",
    "        # Update progress bar description\n",
    "        experiment_pbar.set_description(f\"Exp {i+1}/{total_experiments}: {dataset}+{model}+{neuron_type}+T{timestep}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Experiment {i+1}/{total_experiments}: {dataset} + {model} + {neuron_type} + T{timestep}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        metrics = run_complete_experiment(dataset, model, timestep, neuron_type)\n",
    "        \n",
    "        print(f\"\\n✅ Experiment {i+1} completed\")\n",
    "        print(f\"   Training: {'✅' if metrics.get('training_success', False) else '❌'}\")\n",
    "        print(f\"   Calibration: {'✅' if metrics.get('calibration_success', False) else '❌'}\")\n",
    "        if 'accuracy' in metrics:\n",
    "            print(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # Update progress bar with status\n",
    "        status = \"✅\" if metrics.get('training_success', False) and metrics.get('calibration_success', False) else \"⚠️\"\n",
    "        experiment_pbar.set_postfix_str(f\"Status: {status}\")\n",
    "        \n",
    "        # Clear output periodically\n",
    "        if (i+1) % 4 == 0:\n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    experiment_pbar.close()\n",
    "    print(f\"\\n🎉 All experiments completed! Results saved to {collector.experiment_log}\")\n",
    "\n",
    "# Uncomment to run all experiments\n",
    "# run_all_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze results\n",
    "df = collector.get_dataframe()\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"Loaded {len(df)} experiment results\")\n",
    "    print(\"\\nAvailable columns:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  {col}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n=== Experiment Summary ===\")\n",
    "    successful_experiments = df[df['metrics.training_success'] == True]\n",
    "    print(f\"Successful training experiments: {len(successful_experiments)}\")\n",
    "    \n",
    "    if 'metrics.accuracy' in df.columns:\n",
    "        accuracy_data = df[df['metrics.accuracy'].notna()]\n",
    "        print(f\"Experiments with accuracy data: {len(accuracy_data)}\")\n",
    "        if len(accuracy_data) > 0:\n",
    "            print(f\"Accuracy range: {accuracy_data['metrics.accuracy'].min():.4f} - {accuracy_data['metrics.accuracy'].max():.4f}\")\n",
    "            print(f\"Mean accuracy: {accuracy_data['metrics.accuracy'].mean():.4f}\")\n",
    "else:\n",
    "    print(\"No experiment results found. Run some experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "if len(df) > 0:\n",
    "    df.to_csv('experiment_results.csv', index=False)\n",
    "    print(\"Results exported to 'experiment_results.csv'\")\n",
    "    \n",
    "    # Create summary\n",
    "    summary_df = df.copy()\n",
    "    if 'metrics.accuracy' in summary_df.columns:\n",
    "        summary_table = summary_df.groupby(['config.dataset', 'config.model', 'config.neuron_type', 'config.timestep']).agg({\n",
    "            'metrics.accuracy': ['mean', 'std', 'count'],\n",
    "            'metrics.training_success': 'mean',\n",
    "            'metrics.calibration_success': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        print(\"\\n=== Summary Statistics ===\")\n",
    "        print(summary_table)\n",
    "        \n",
    "        summary_table.to_csv('experiment_summary.csv')\n",
    "        print(\"Summary saved to 'experiment_summary.csv'\")\n",
    "else:\n",
    "    print(\"No results to export\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
