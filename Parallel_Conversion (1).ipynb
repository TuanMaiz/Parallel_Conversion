{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/Parallel_Conversion"
      ],
      "metadata": {
        "id": "9juFbwW-qei3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b add-calib https://github.com/TuanMaiz/Parallel_Conversion.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCUsJqHxHPYd",
        "outputId": "3c492b5c-1074-4b3a-c23a-07f8a479a9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Parallel_Conversion'...\n",
            "remote: Enumerating objects: 222, done.\u001b[K\n",
            "remote: Total 222 (delta 0), reused 0 (delta 0), pack-reused 222 (from 1)\u001b[K\n",
            "Receiving objects: 100% (222/222), 25.39 MiB | 11.09 MiB/s, done.\n",
            "Resolving deltas: 100% (138/138), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normal Bert"
      ],
      "metadata": {
        "id": "trCTsRActXQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python Parallel_Conversion/main.py \\\n",
        "    --dataset TextCLS \\\n",
        "    --net_arch distilbert_base \\\n",
        "    --savedir ./checkpoints-bert-standard \\\n",
        "    --text_dataset imdb \\\n",
        "    --text_max_len 256 \\\n",
        "    --trainsnn_epochs 5 \\\n",
        "    --batchsize 16 \\\n",
        "    --lr 0.00001 \\\n",
        "    --dev 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUkOZZ6Etbej",
        "outputId": "bfa4b2a1-6293-4d5b-f787-fef85b8beadf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception ignored in: <function _get_module_lock.<locals>.cb at 0x7e1801d83880>\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 445, in cb\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Parallel_Conversion/main.py\", line 5, in <module>\n",
            "    from models.Bert_QCFS import BertForSequenceClassificationQCFS\n",
            "  File \"/content/Parallel_Conversion/models/Bert_QCFS.py\", line 3, in <module>\n",
            "    from transformers import BertModel, BertConfig\n",
            "  File \"<frozen importlib._bootstrap>\", line 1412, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2302, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2330, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/__init__.py\", line 382, in <module>\n",
            "    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)\n",
            "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2851, in define_import_structure\n",
            "    import_structure = create_import_structure_from_path(module_path)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2564, in create_import_structure_from_path\n",
            "    import_structure[f] = create_import_structure_from_path(os.path.join(module_path, f))\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2588, in create_import_structure_from_path\n",
            "    with open(os.path.join(directory, module_name), encoding=\"utf-8\") as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen codecs>\", line 309, in __init__\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert SNN"
      ],
      "metadata": {
        "id": "j05yfOS4tVRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 timestep then calib"
      ],
      "metadata": {
        "id": "pX188txZqWtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python Parallel_Conversion/main.py \\\n",
        "      --dataset TextCLS \\\n",
        "      --net_arch distilbert_base_qcfs \\\n",
        "      --savedir ./checkpoints \\\n",
        "      --neuron_type ParaInfNeuron \\\n",
        "      --text_dataset imdb \\\n",
        "      --text_max_len 256 \\\n",
        "      --time_step 2 \\\n",
        "      --trainsnn_epochs 5 \\\n",
        "      --batchsize 16 \\\n",
        "      --lr 0.00001 \\\n",
        "      --measure_efficiency \\\n",
        "      --gpu_type A100 \\\n",
        "      --dev 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0oYp_2UqZrs",
        "outputId": "7dc85432-b9bd-4320-a1c7-bc708ba39426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-24 17:02:22.027654: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-09-24 17:02:22.045176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758733342.066825    4978 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758733342.073324    4978 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758733342.089844    4978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758733342.089874    4978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758733342.089876    4978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758733342.089879    4978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-24 17:02:22.094661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Not using distributed mode\n",
            "README.md: 7.81kB [00:00, 28.8MB/s]\n",
            "plain_text/train-00000-of-00001.parquet: 100% 21.0M/21.0M [00:01<00:00, 16.9MB/s]\n",
            "plain_text/test-00000-of-00001.parquet: 100% 20.5M/20.5M [00:00<00:00, 36.4MB/s]\n",
            "plain_text/unsupervised-00000-of-00001.p(â€¦): 100% 42.0M/42.0M [00:00<00:00, 57.5MB/s]\n",
            "Generating train split: 100% 25000/25000 [00:00<00:00, 147309.57 examples/s]\n",
            "Generating test split: 100% 25000/25000 [00:00<00:00, 217944.34 examples/s]\n",
            "Generating unsupervised split: 100% 50000/50000 [00:00<00:00, 199267.22 examples/s]\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 419kB/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 5.21MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 49.3MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 1.99MB/s]\n",
            "config.json: 100% 483/483 [00:00<00:00, 4.21MB/s]\n",
            "model.safetensors: 100% 268M/268M [00:01<00:00, 237MB/s]\n",
            "total parameters: 66.365956 M.\n",
            "Starting training with 1563 batches...\n",
            "Batch [10/1563] - Loss: 1.1953, Avg Loss: 0.0733\n",
            "Batch [20/1563] - Loss: 1.1247, Avg Loss: 0.0724\n",
            "Batch [30/1563] - Loss: 1.1553, Avg Loss: 0.0718\n",
            "Batch [40/1563] - Loss: 1.0852, Avg Loss: 0.0711\n",
            "Batch [50/1563] - Loss: 1.0976, Avg Loss: 0.0704\n",
            "Batch [60/1563] - Loss: 1.0528, Avg Loss: 0.0698\n",
            "Batch [70/1563] - Loss: 1.0283, Avg Loss: 0.0692\n",
            "Batch [80/1563] - Loss: 1.0305, Avg Loss: 0.0685\n",
            "Batch [90/1563] - Loss: 1.0056, Avg Loss: 0.0680\n",
            "Batch [100/1563] - Loss: 0.9871, Avg Loss: 0.0675\n",
            "Batch [110/1563] - Loss: 0.9641, Avg Loss: 0.0670\n",
            "Batch [120/1563] - Loss: 0.9746, Avg Loss: 0.0664\n",
            "Batch [130/1563] - Loss: 0.9589, Avg Loss: 0.0659\n",
            "Batch [140/1563] - Loss: 0.9233, Avg Loss: 0.0654\n",
            "Batch [150/1563] - Loss: 0.9192, Avg Loss: 0.0650\n",
            "Batch [160/1563] - Loss: 0.9331, Avg Loss: 0.0646\n",
            "Batch [170/1563] - Loss: 0.9326, Avg Loss: 0.0641\n",
            "Batch [180/1563] - Loss: 0.8846, Avg Loss: 0.0637\n",
            "Batch [190/1563] - Loss: 0.8814, Avg Loss: 0.0633\n",
            "Batch [200/1563] - Loss: 0.8971, Avg Loss: 0.0630\n",
            "Batch [210/1563] - Loss: 0.8853, Avg Loss: 0.0626\n",
            "Batch [220/1563] - Loss: 0.8647, Avg Loss: 0.0623\n",
            "Batch [230/1563] - Loss: 0.8695, Avg Loss: 0.0620\n",
            "Batch [240/1563] - Loss: 0.8483, Avg Loss: 0.0617\n",
            "Batch [250/1563] - Loss: 0.8824, Avg Loss: 0.0614\n",
            "Batch [260/1563] - Loss: 0.8805, Avg Loss: 0.0611\n",
            "Batch [270/1563] - Loss: 0.8452, Avg Loss: 0.0609\n",
            "Batch [280/1563] - Loss: 0.8403, Avg Loss: 0.0606\n",
            "Batch [290/1563] - Loss: 0.8822, Avg Loss: 0.0603\n",
            "Batch [300/1563] - Loss: 0.8416, Avg Loss: 0.0601\n",
            "Batch [310/1563] - Loss: 0.8311, Avg Loss: 0.0598\n",
            "Batch [320/1563] - Loss: 0.8380, Avg Loss: 0.0596\n",
            "Batch [330/1563] - Loss: 0.8462, Avg Loss: 0.0594\n",
            "Batch [340/1563] - Loss: 0.8226, Avg Loss: 0.0592\n",
            "Batch [350/1563] - Loss: 0.8505, Avg Loss: 0.0590\n",
            "Batch [360/1563] - Loss: 0.8248, Avg Loss: 0.0587\n",
            "Batch [370/1563] - Loss: 0.8231, Avg Loss: 0.0585\n",
            "Batch [380/1563] - Loss: 0.8094, Avg Loss: 0.0583\n",
            "Batch [390/1563] - Loss: 0.8310, Avg Loss: 0.0582\n",
            "Batch [400/1563] - Loss: 0.8166, Avg Loss: 0.0580\n",
            "Batch [410/1563] - Loss: 0.8066, Avg Loss: 0.0578\n",
            "Batch [420/1563] - Loss: 0.8033, Avg Loss: 0.0576\n",
            "Batch [430/1563] - Loss: 0.8156, Avg Loss: 0.0574\n",
            "Batch [440/1563] - Loss: 0.8183, Avg Loss: 0.0573\n",
            "Batch [450/1563] - Loss: 0.7961, Avg Loss: 0.0571\n",
            "Batch [460/1563] - Loss: 0.7885, Avg Loss: 0.0569\n",
            "Batch [470/1563] - Loss: 0.7884, Avg Loss: 0.0568\n",
            "Batch [480/1563] - Loss: 0.7729, Avg Loss: 0.0566\n",
            "Batch [490/1563] - Loss: 0.7817, Avg Loss: 0.0565\n",
            "Batch [500/1563] - Loss: 0.7904, Avg Loss: 0.0563\n",
            "Batch [510/1563] - Loss: 0.7828, Avg Loss: 0.0562\n",
            "Batch [520/1563] - Loss: 0.7942, Avg Loss: 0.0561\n",
            "Batch [530/1563] - Loss: 0.8050, Avg Loss: 0.0559\n",
            "Batch [540/1563] - Loss: 0.7572, Avg Loss: 0.0558\n",
            "Batch [550/1563] - Loss: 0.7673, Avg Loss: 0.0557\n",
            "Batch [560/1563] - Loss: 0.8070, Avg Loss: 0.0555\n",
            "Batch [570/1563] - Loss: 0.7671, Avg Loss: 0.0554\n",
            "Batch [580/1563] - Loss: 0.7571, Avg Loss: 0.0553\n",
            "Batch [590/1563] - Loss: 0.7726, Avg Loss: 0.0552\n",
            "Batch [600/1563] - Loss: 0.7742, Avg Loss: 0.0551\n",
            "Batch [610/1563] - Loss: 0.7769, Avg Loss: 0.0549\n",
            "Batch [620/1563] - Loss: 0.7539, Avg Loss: 0.0548\n",
            "Batch [630/1563] - Loss: 0.7885, Avg Loss: 0.0547\n",
            "Batch [640/1563] - Loss: 0.7495, Avg Loss: 0.0546\n",
            "Batch [650/1563] - Loss: 0.7640, Avg Loss: 0.0545\n",
            "Batch [660/1563] - Loss: 0.7585, Avg Loss: 0.0544\n",
            "Batch [670/1563] - Loss: 0.7541, Avg Loss: 0.0543\n",
            "Batch [680/1563] - Loss: 0.7618, Avg Loss: 0.0542\n",
            "Batch [690/1563] - Loss: 0.7328, Avg Loss: 0.0541\n",
            "Batch [700/1563] - Loss: 0.7633, Avg Loss: 0.0540\n",
            "Batch [710/1563] - Loss: 0.7305, Avg Loss: 0.0539\n",
            "Batch [720/1563] - Loss: 0.7484, Avg Loss: 0.0538\n",
            "Batch [730/1563] - Loss: 0.7361, Avg Loss: 0.0537\n",
            "Batch [740/1563] - Loss: 0.7330, Avg Loss: 0.0536\n",
            "Batch [750/1563] - Loss: 0.7371, Avg Loss: 0.0535\n",
            "Batch [760/1563] - Loss: 0.7685, Avg Loss: 0.0534\n",
            "Batch [770/1563] - Loss: 0.7713, Avg Loss: 0.0534\n",
            "Batch [780/1563] - Loss: 0.7709, Avg Loss: 0.0533\n",
            "Batch [790/1563] - Loss: 0.7584, Avg Loss: 0.0532\n",
            "Batch [800/1563] - Loss: 0.7692, Avg Loss: 0.0531\n",
            "Batch [810/1563] - Loss: 0.7491, Avg Loss: 0.0530\n",
            "Batch [820/1563] - Loss: 0.7341, Avg Loss: 0.0530\n",
            "Batch [830/1563] - Loss: 0.7547, Avg Loss: 0.0529\n",
            "Batch [840/1563] - Loss: 0.7132, Avg Loss: 0.0528\n",
            "Batch [850/1563] - Loss: 0.7552, Avg Loss: 0.0527\n",
            "Batch [860/1563] - Loss: 0.7498, Avg Loss: 0.0526\n",
            "Batch [870/1563] - Loss: 0.7280, Avg Loss: 0.0526\n",
            "Batch [880/1563] - Loss: 0.7128, Avg Loss: 0.0525\n",
            "Batch [890/1563] - Loss: 0.7495, Avg Loss: 0.0524\n",
            "Batch [900/1563] - Loss: 0.7333, Avg Loss: 0.0524\n",
            "Batch [910/1563] - Loss: 0.7274, Avg Loss: 0.0523\n",
            "Batch [920/1563] - Loss: 0.7358, Avg Loss: 0.0522\n",
            "Batch [930/1563] - Loss: 0.7407, Avg Loss: 0.0522\n",
            "Batch [940/1563] - Loss: 0.7515, Avg Loss: 0.0521\n",
            "Batch [950/1563] - Loss: 0.7226, Avg Loss: 0.0520\n",
            "Batch [960/1563] - Loss: 0.7076, Avg Loss: 0.0520\n",
            "Batch [970/1563] - Loss: 0.7191, Avg Loss: 0.0519\n",
            "Batch [980/1563] - Loss: 0.7231, Avg Loss: 0.0518\n",
            "Batch [990/1563] - Loss: 0.7093, Avg Loss: 0.0518\n",
            "Batch [1000/1563] - Loss: 0.7374, Avg Loss: 0.0517\n",
            "Batch [1010/1563] - Loss: 0.7137, Avg Loss: 0.0516\n",
            "Batch [1020/1563] - Loss: 0.7297, Avg Loss: 0.0516\n",
            "Batch [1030/1563] - Loss: 0.7418, Avg Loss: 0.0515\n",
            "Batch [1040/1563] - Loss: 0.7317, Avg Loss: 0.0515\n",
            "Batch [1050/1563] - Loss: 0.7412, Avg Loss: 0.0514\n",
            "Batch [1060/1563] - Loss: 0.7198, Avg Loss: 0.0513\n",
            "Batch [1070/1563] - Loss: 0.7230, Avg Loss: 0.0513\n",
            "Batch [1080/1563] - Loss: 0.7242, Avg Loss: 0.0512\n",
            "Batch [1090/1563] - Loss: 0.6858, Avg Loss: 0.0512\n",
            "Batch [1100/1563] - Loss: 0.7107, Avg Loss: 0.0511\n",
            "Batch [1110/1563] - Loss: 0.7037, Avg Loss: 0.0511\n",
            "Batch [1120/1563] - Loss: 0.7378, Avg Loss: 0.0510\n",
            "Batch [1130/1563] - Loss: 0.7041, Avg Loss: 0.0510\n",
            "Batch [1140/1563] - Loss: 0.7129, Avg Loss: 0.0509\n",
            "Batch [1150/1563] - Loss: 0.7290, Avg Loss: 0.0509\n",
            "Batch [1160/1563] - Loss: 0.7195, Avg Loss: 0.0508\n",
            "Batch [1170/1563] - Loss: 0.6882, Avg Loss: 0.0508\n",
            "Batch [1180/1563] - Loss: 0.7105, Avg Loss: 0.0507\n",
            "Batch [1190/1563] - Loss: 0.7062, Avg Loss: 0.0507\n",
            "Batch [1200/1563] - Loss: 0.7120, Avg Loss: 0.0506\n",
            "Batch [1210/1563] - Loss: 0.7225, Avg Loss: 0.0506\n",
            "Batch [1220/1563] - Loss: 0.7209, Avg Loss: 0.0505\n",
            "Batch [1230/1563] - Loss: 0.6913, Avg Loss: 0.0505\n",
            "Batch [1240/1563] - Loss: 0.7354, Avg Loss: 0.0504\n",
            "Batch [1250/1563] - Loss: 0.7249, Avg Loss: 0.0504\n",
            "Batch [1260/1563] - Loss: 0.6914, Avg Loss: 0.0503\n",
            "Batch [1270/1563] - Loss: 0.7023, Avg Loss: 0.0503\n",
            "Batch [1280/1563] - Loss: 0.7089, Avg Loss: 0.0502\n",
            "Batch [1290/1563] - Loss: 0.6842, Avg Loss: 0.0502\n",
            "Batch [1300/1563] - Loss: 0.7047, Avg Loss: 0.0501\n",
            "Batch [1310/1563] - Loss: 0.6765, Avg Loss: 0.0501\n",
            "Batch [1320/1563] - Loss: 0.7309, Avg Loss: 0.0501\n",
            "Batch [1330/1563] - Loss: 0.6759, Avg Loss: 0.0500\n",
            "Batch [1340/1563] - Loss: 0.7184, Avg Loss: 0.0500\n",
            "Batch [1350/1563] - Loss: 0.7139, Avg Loss: 0.0499\n",
            "Batch [1360/1563] - Loss: 0.7169, Avg Loss: 0.0499\n",
            "Batch [1370/1563] - Loss: 0.7157, Avg Loss: 0.0499\n",
            "Batch [1380/1563] - Loss: 0.6992, Avg Loss: 0.0498\n",
            "Batch [1390/1563] - Loss: 0.6838, Avg Loss: 0.0498\n",
            "Batch [1400/1563] - Loss: 0.6762, Avg Loss: 0.0497\n",
            "Batch [1410/1563] - Loss: 0.7153, Avg Loss: 0.0497\n",
            "Batch [1420/1563] - Loss: 0.7093, Avg Loss: 0.0497\n",
            "Batch [1430/1563] - Loss: 0.6727, Avg Loss: 0.0496\n",
            "Batch [1440/1563] - Loss: 0.7086, Avg Loss: 0.0496\n",
            "Batch [1450/1563] - Loss: 0.7072, Avg Loss: 0.0495\n",
            "Batch [1460/1563] - Loss: 0.7073, Avg Loss: 0.0495\n",
            "Batch [1470/1563] - Loss: 0.6463, Avg Loss: 0.0494\n",
            "Batch [1480/1563] - Loss: 0.6765, Avg Loss: 0.0494\n",
            "Batch [1490/1563] - Loss: 0.7175, Avg Loss: 0.0494\n",
            "Batch [1500/1563] - Loss: 0.7058, Avg Loss: 0.0493\n",
            "Batch [1510/1563] - Loss: 0.6787, Avg Loss: 0.0493\n",
            "Batch [1520/1563] - Loss: 0.7026, Avg Loss: 0.0493\n",
            "Batch [1530/1563] - Loss: 0.6719, Avg Loss: 0.0492\n",
            "Batch [1540/1563] - Loss: 0.7087, Avg Loss: 0.0492\n",
            "Batch [1550/1563] - Loss: 0.6840, Avg Loss: 0.0491\n",
            "Batch [1560/1563] - Loss: 0.7184, Avg Loss: 0.0491\n",
            "Epoch completed - Total samples: 25000, Average Loss: 0.0491\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:45, 34.25it/s]Eval Batch [20/1563] - Current Accuracy: 0.7375\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.21it/s]Eval Batch [40/1563] - Current Accuracy: 0.7188\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 38.68it/s]Eval Batch [60/1563] - Current Accuracy: 0.7146\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:38, 38.95it/s]Eval Batch [80/1563] - Current Accuracy: 0.7227\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 38.98it/s]Eval Batch [100/1563] - Current Accuracy: 0.7288\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:37, 38.97it/s]Eval Batch [120/1563] - Current Accuracy: 0.7318\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 39.12it/s]Eval Batch [140/1563] - Current Accuracy: 0.7357\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:36, 39.01it/s]Eval Batch [160/1563] - Current Accuracy: 0.7285\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 39.02it/s]Eval Batch [180/1563] - Current Accuracy: 0.7229\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:34, 39.09it/s]Eval Batch [200/1563] - Current Accuracy: 0.7253\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 39.17it/s]Eval Batch [220/1563] - Current Accuracy: 0.7250\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:33, 39.02it/s]Eval Batch [240/1563] - Current Accuracy: 0.7258\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 39.11it/s]Eval Batch [260/1563] - Current Accuracy: 0.7279\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:32, 39.14it/s]Eval Batch [280/1563] - Current Accuracy: 0.7277\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 39.13it/s]Eval Batch [300/1563] - Current Accuracy: 0.7277\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:31, 39.22it/s]Eval Batch [320/1563] - Current Accuracy: 0.7271\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 38.85it/s]Eval Batch [340/1563] - Current Accuracy: 0.7250\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:31, 38.56it/s]Eval Batch [360/1563] - Current Accuracy: 0.7278\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:31, 38.22it/s]Eval Batch [380/1563] - Current Accuracy: 0.7257\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:30, 38.70it/s]Eval Batch [400/1563] - Current Accuracy: 0.7217\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 38.80it/s]Eval Batch [420/1563] - Current Accuracy: 0.7251\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:28, 38.94it/s]Eval Batch [440/1563] - Current Accuracy: 0.7260\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 38.93it/s]Eval Batch [460/1563] - Current Accuracy: 0.7258\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:27, 39.00it/s]Eval Batch [480/1563] - Current Accuracy: 0.7258\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 39.02it/s]Eval Batch [500/1563] - Current Accuracy: 0.7265\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 38.93it/s]Eval Batch [520/1563] - Current Accuracy: 0.7264\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 38.92it/s]Eval Batch [540/1563] - Current Accuracy: 0.7269\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 38.95it/s]Eval Batch [560/1563] - Current Accuracy: 0.7267\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 39.09it/s]Eval Batch [580/1563] - Current Accuracy: 0.7236\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 39.14it/s]Eval Batch [600/1563] - Current Accuracy: 0.7229\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 39.15it/s]Eval Batch [620/1563] - Current Accuracy: 0.7216\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 39.19it/s]Eval Batch [640/1563] - Current Accuracy: 0.7226\n",
            "Evaluating SNN:  42% 657/1563 [00:17<00:23, 38.93it/s]Eval Batch [660/1563] - Current Accuracy: 0.7254\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 39.10it/s]Eval Batch [680/1563] - Current Accuracy: 0.7263\n",
            "Evaluating SNN:  45% 697/1563 [00:18<00:22, 39.06it/s]Eval Batch [700/1563] - Current Accuracy: 0.7271\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 39.11it/s]Eval Batch [720/1563] - Current Accuracy: 0.7265\n",
            "Evaluating SNN:  47% 737/1563 [00:19<00:21, 39.11it/s]Eval Batch [740/1563] - Current Accuracy: 0.7261\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 39.18it/s]Eval Batch [760/1563] - Current Accuracy: 0.7274\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 38.92it/s]Eval Batch [780/1563] - Current Accuracy: 0.7273\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 38.96it/s]Eval Batch [800/1563] - Current Accuracy: 0.7268\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 38.72it/s]Eval Batch [820/1563] - Current Accuracy: 0.7248\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 38.66it/s]Eval Batch [840/1563] - Current Accuracy: 0.7201\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 38.96it/s]Eval Batch [860/1563] - Current Accuracy: 0.7170\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 38.99it/s]Eval Batch [880/1563] - Current Accuracy: 0.7147\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 39.06it/s]Eval Batch [900/1563] - Current Accuracy: 0.7103\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 39.15it/s]Eval Batch [920/1563] - Current Accuracy: 0.7072\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:16, 38.92it/s]Eval Batch [940/1563] - Current Accuracy: 0.7060\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 38.89it/s]Eval Batch [960/1563] - Current Accuracy: 0.7018\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:14, 39.13it/s]Eval Batch [980/1563] - Current Accuracy: 0.6989\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 39.19it/s]Eval Batch [1000/1563] - Current Accuracy: 0.6966\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:13, 39.00it/s]Eval Batch [1020/1563] - Current Accuracy: 0.6934\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 39.13it/s]Eval Batch [1040/1563] - Current Accuracy: 0.6904\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:12, 38.92it/s]Eval Batch [1060/1563] - Current Accuracy: 0.6881\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 39.06it/s]Eval Batch [1080/1563] - Current Accuracy: 0.6859\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:11, 39.00it/s]Eval Batch [1100/1563] - Current Accuracy: 0.6845\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 38.91it/s]Eval Batch [1120/1563] - Current Accuracy: 0.6825\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 38.97it/s]Eval Batch [1140/1563] - Current Accuracy: 0.6804\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 39.05it/s]Eval Batch [1160/1563] - Current Accuracy: 0.6790\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 39.22it/s]Eval Batch [1180/1563] - Current Accuracy: 0.6787\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 39.08it/s]Eval Batch [1200/1563] - Current Accuracy: 0.6755\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 39.10it/s]Eval Batch [1220/1563] - Current Accuracy: 0.6737\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 38.91it/s]Eval Batch [1240/1563] - Current Accuracy: 0.6714\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 38.83it/s]Eval Batch [1260/1563] - Current Accuracy: 0.6681\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 38.61it/s]Eval Batch [1280/1563] - Current Accuracy: 0.6682\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 38.76it/s]Eval Batch [1300/1563] - Current Accuracy: 0.6677\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 39.02it/s]Eval Batch [1320/1563] - Current Accuracy: 0.6663\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 39.03it/s]Eval Batch [1340/1563] - Current Accuracy: 0.6658\n",
            "Evaluating SNN:  87% 1357/1563 [00:34<00:05, 39.13it/s]Eval Batch [1360/1563] - Current Accuracy: 0.6642\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 39.14it/s]Eval Batch [1380/1563] - Current Accuracy: 0.6624\n",
            "Evaluating SNN:  89% 1397/1563 [00:35<00:04, 39.01it/s]Eval Batch [1400/1563] - Current Accuracy: 0.6613\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 39.15it/s]Eval Batch [1420/1563] - Current Accuracy: 0.6615\n",
            "Evaluating SNN:  92% 1437/1563 [00:37<00:03, 39.24it/s]Eval Batch [1440/1563] - Current Accuracy: 0.6603\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 39.24it/s]Eval Batch [1460/1563] - Current Accuracy: 0.6608\n",
            "Evaluating SNN:  94% 1477/1563 [00:38<00:02, 39.20it/s]Eval Batch [1480/1563] - Current Accuracy: 0.6604\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 39.24it/s]Eval Batch [1500/1563] - Current Accuracy: 0.6589\n",
            "Evaluating SNN:  97% 1517/1563 [00:39<00:01, 38.96it/s]Eval Batch [1520/1563] - Current Accuracy: 0.6585\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 39.02it/s]Eval Batch [1540/1563] - Current Accuracy: 0.6572\n",
            "Evaluating SNN: 100% 1557/1563 [00:40<00:00, 39.05it/s]Eval Batch [1560/1563] - Current Accuracy: 0.6568\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.83it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.6566\n",
            "INFO:root:Saved model weights at epoch 0 to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_0.pth\n",
            "[2025-09-24 17:05:36,938][main.py][line:639][INFO] Saved model weights at epoch 0 to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_0.pth\n",
            "INFO:root:Save directory exists: True\n",
            "[2025-09-24 17:05:36,939][main.py][line:640][INFO] Save directory exists: True\n",
            "INFO:root:File size: 265511595 bytes\n",
            "[2025-09-24 17:05:36,939][main.py][line:641][INFO] File size: 265511595 bytes\n",
            "INFO:root:SNNs training Epoch 0 [distilbert_base_qcfs]: Val_loss: 0.04911061213493347\n",
            "[2025-09-24 17:05:36,939][main.py][line:644][INFO] SNNs training Epoch 0 [distilbert_base_qcfs]: Val_loss: 0.04911061213493347\n",
            "INFO:root:SNNs training Epoch 0 [distilbert_base_qcfs]: Test Acc: 0.6566 Best Acc: 0.6566\n",
            "[2025-09-24 17:05:36,939][main.py][line:645][INFO] SNNs training Epoch 0 [distilbert_base_qcfs]: Test Acc: 0.6566 Best Acc: 0.6566\n",
            "Starting training with 1563 batches...\n",
            "Batch [10/1563] - Loss: 0.6826, Avg Loss: 0.0439\n",
            "Batch [20/1563] - Loss: 0.7248, Avg Loss: 0.0436\n",
            "Batch [30/1563] - Loss: 0.6801, Avg Loss: 0.0436\n",
            "Batch [40/1563] - Loss: 0.6994, Avg Loss: 0.0436\n",
            "Batch [50/1563] - Loss: 0.6997, Avg Loss: 0.0435\n",
            "Batch [60/1563] - Loss: 0.6891, Avg Loss: 0.0436\n",
            "Batch [70/1563] - Loss: 0.7238, Avg Loss: 0.0436\n",
            "Batch [80/1563] - Loss: 0.7208, Avg Loss: 0.0436\n",
            "Batch [90/1563] - Loss: 0.7191, Avg Loss: 0.0435\n",
            "Batch [100/1563] - Loss: 0.7082, Avg Loss: 0.0435\n",
            "Batch [110/1563] - Loss: 0.7164, Avg Loss: 0.0435\n",
            "Batch [120/1563] - Loss: 0.6765, Avg Loss: 0.0435\n",
            "Batch [130/1563] - Loss: 0.6948, Avg Loss: 0.0434\n",
            "Batch [140/1563] - Loss: 0.6724, Avg Loss: 0.0434\n",
            "Batch [150/1563] - Loss: 0.6758, Avg Loss: 0.0434\n",
            "Batch [160/1563] - Loss: 0.6874, Avg Loss: 0.0434\n",
            "Batch [170/1563] - Loss: 0.6663, Avg Loss: 0.0434\n",
            "Batch [180/1563] - Loss: 0.7001, Avg Loss: 0.0434\n",
            "Batch [190/1563] - Loss: 0.7048, Avg Loss: 0.0434\n",
            "Batch [200/1563] - Loss: 0.6808, Avg Loss: 0.0434\n",
            "Batch [210/1563] - Loss: 0.6754, Avg Loss: 0.0434\n",
            "Batch [220/1563] - Loss: 0.6884, Avg Loss: 0.0433\n",
            "Batch [230/1563] - Loss: 0.6945, Avg Loss: 0.0433\n",
            "Batch [240/1563] - Loss: 0.6982, Avg Loss: 0.0433\n",
            "Batch [250/1563] - Loss: 0.6724, Avg Loss: 0.0433\n",
            "Batch [260/1563] - Loss: 0.6755, Avg Loss: 0.0432\n",
            "Batch [270/1563] - Loss: 0.7013, Avg Loss: 0.0432\n",
            "Batch [280/1563] - Loss: 0.6884, Avg Loss: 0.0432\n",
            "Batch [290/1563] - Loss: 0.6297, Avg Loss: 0.0431\n",
            "Batch [300/1563] - Loss: 0.7213, Avg Loss: 0.0431\n",
            "Batch [310/1563] - Loss: 0.6809, Avg Loss: 0.0431\n",
            "Batch [320/1563] - Loss: 0.6684, Avg Loss: 0.0431\n",
            "Batch [330/1563] - Loss: 0.6720, Avg Loss: 0.0431\n",
            "Batch [340/1563] - Loss: 0.6692, Avg Loss: 0.0431\n",
            "Batch [350/1563] - Loss: 0.7133, Avg Loss: 0.0431\n",
            "Batch [360/1563] - Loss: 0.6501, Avg Loss: 0.0431\n",
            "Batch [370/1563] - Loss: 0.6855, Avg Loss: 0.0430\n",
            "Batch [380/1563] - Loss: 0.6914, Avg Loss: 0.0430\n",
            "Batch [390/1563] - Loss: 0.6812, Avg Loss: 0.0430\n",
            "Batch [400/1563] - Loss: 0.6740, Avg Loss: 0.0430\n",
            "Batch [410/1563] - Loss: 0.6754, Avg Loss: 0.0430\n",
            "Batch [420/1563] - Loss: 0.6527, Avg Loss: 0.0430\n",
            "Batch [430/1563] - Loss: 0.6550, Avg Loss: 0.0429\n",
            "Batch [440/1563] - Loss: 0.6901, Avg Loss: 0.0429\n",
            "Batch [450/1563] - Loss: 0.6612, Avg Loss: 0.0429\n",
            "Batch [460/1563] - Loss: 0.6716, Avg Loss: 0.0429\n",
            "Batch [470/1563] - Loss: 0.7245, Avg Loss: 0.0429\n",
            "Batch [480/1563] - Loss: 0.6694, Avg Loss: 0.0429\n",
            "Batch [490/1563] - Loss: 0.6842, Avg Loss: 0.0428\n",
            "Batch [500/1563] - Loss: 0.6981, Avg Loss: 0.0428\n",
            "Batch [510/1563] - Loss: 0.6875, Avg Loss: 0.0428\n",
            "Batch [520/1563] - Loss: 0.6715, Avg Loss: 0.0428\n",
            "Batch [530/1563] - Loss: 0.6772, Avg Loss: 0.0428\n",
            "Batch [540/1563] - Loss: 0.6548, Avg Loss: 0.0428\n",
            "Batch [550/1563] - Loss: 0.6612, Avg Loss: 0.0428\n",
            "Batch [560/1563] - Loss: 0.7342, Avg Loss: 0.0428\n",
            "Batch [570/1563] - Loss: 0.6401, Avg Loss: 0.0427\n",
            "Batch [580/1563] - Loss: 0.7034, Avg Loss: 0.0427\n",
            "Batch [590/1563] - Loss: 0.6789, Avg Loss: 0.0427\n",
            "Batch [600/1563] - Loss: 0.7002, Avg Loss: 0.0427\n",
            "Batch [610/1563] - Loss: 0.6570, Avg Loss: 0.0427\n",
            "Batch [620/1563] - Loss: 0.6608, Avg Loss: 0.0427\n",
            "Batch [630/1563] - Loss: 0.6707, Avg Loss: 0.0427\n",
            "Batch [640/1563] - Loss: 0.6437, Avg Loss: 0.0426\n",
            "Batch [650/1563] - Loss: 0.7035, Avg Loss: 0.0426\n",
            "Batch [660/1563] - Loss: 0.6755, Avg Loss: 0.0426\n",
            "Batch [670/1563] - Loss: 0.6821, Avg Loss: 0.0426\n",
            "Batch [680/1563] - Loss: 0.6566, Avg Loss: 0.0426\n",
            "Batch [690/1563] - Loss: 0.6980, Avg Loss: 0.0426\n",
            "Batch [700/1563] - Loss: 0.7038, Avg Loss: 0.0426\n",
            "Batch [710/1563] - Loss: 0.6119, Avg Loss: 0.0426\n",
            "Batch [720/1563] - Loss: 0.6203, Avg Loss: 0.0425\n",
            "Batch [730/1563] - Loss: 0.7105, Avg Loss: 0.0425\n",
            "Batch [740/1563] - Loss: 0.6996, Avg Loss: 0.0425\n",
            "Batch [750/1563] - Loss: 0.6675, Avg Loss: 0.0425\n",
            "Batch [760/1563] - Loss: 0.6849, Avg Loss: 0.0425\n",
            "Batch [770/1563] - Loss: 0.6121, Avg Loss: 0.0425\n",
            "Batch [780/1563] - Loss: 0.5901, Avg Loss: 0.0425\n",
            "Batch [790/1563] - Loss: 0.6443, Avg Loss: 0.0424\n",
            "Batch [800/1563] - Loss: 0.6818, Avg Loss: 0.0424\n",
            "Batch [810/1563] - Loss: 0.6752, Avg Loss: 0.0424\n",
            "Batch [820/1563] - Loss: 0.6525, Avg Loss: 0.0424\n",
            "Batch [830/1563] - Loss: 0.6405, Avg Loss: 0.0424\n",
            "Batch [840/1563] - Loss: 0.6348, Avg Loss: 0.0424\n",
            "Batch [850/1563] - Loss: 0.6909, Avg Loss: 0.0423\n",
            "Batch [860/1563] - Loss: 0.6637, Avg Loss: 0.0423\n",
            "Batch [870/1563] - Loss: 0.6382, Avg Loss: 0.0423\n",
            "Batch [880/1563] - Loss: 0.5966, Avg Loss: 0.0423\n",
            "Batch [890/1563] - Loss: 0.6393, Avg Loss: 0.0423\n",
            "Batch [900/1563] - Loss: 0.6763, Avg Loss: 0.0423\n",
            "Batch [910/1563] - Loss: 0.6719, Avg Loss: 0.0423\n",
            "Batch [920/1563] - Loss: 0.6913, Avg Loss: 0.0422\n",
            "Batch [930/1563] - Loss: 0.6835, Avg Loss: 0.0422\n",
            "Batch [940/1563] - Loss: 0.6205, Avg Loss: 0.0422\n",
            "Batch [950/1563] - Loss: 0.6629, Avg Loss: 0.0422\n",
            "Batch [960/1563] - Loss: 0.6197, Avg Loss: 0.0422\n",
            "Batch [970/1563] - Loss: 0.6164, Avg Loss: 0.0421\n",
            "Batch [980/1563] - Loss: 0.6101, Avg Loss: 0.0421\n",
            "Batch [990/1563] - Loss: 0.7049, Avg Loss: 0.0421\n",
            "Batch [1000/1563] - Loss: 0.6634, Avg Loss: 0.0421\n",
            "Batch [1010/1563] - Loss: 0.6201, Avg Loss: 0.0421\n",
            "Batch [1020/1563] - Loss: 0.6845, Avg Loss: 0.0421\n",
            "Batch [1030/1563] - Loss: 0.6190, Avg Loss: 0.0420\n",
            "Batch [1040/1563] - Loss: 0.6159, Avg Loss: 0.0420\n",
            "Batch [1050/1563] - Loss: 0.6964, Avg Loss: 0.0420\n",
            "Batch [1060/1563] - Loss: 0.6397, Avg Loss: 0.0420\n",
            "Batch [1070/1563] - Loss: 0.5639, Avg Loss: 0.0420\n",
            "Batch [1080/1563] - Loss: 0.5891, Avg Loss: 0.0419\n",
            "Batch [1090/1563] - Loss: 0.6657, Avg Loss: 0.0419\n",
            "Batch [1100/1563] - Loss: 0.5960, Avg Loss: 0.0419\n",
            "Batch [1110/1563] - Loss: 0.6639, Avg Loss: 0.0419\n",
            "Batch [1120/1563] - Loss: 0.6487, Avg Loss: 0.0419\n",
            "Batch [1130/1563] - Loss: 0.6435, Avg Loss: 0.0419\n",
            "Batch [1140/1563] - Loss: 0.7085, Avg Loss: 0.0419\n",
            "Batch [1150/1563] - Loss: 0.6208, Avg Loss: 0.0418\n",
            "Batch [1160/1563] - Loss: 0.6444, Avg Loss: 0.0418\n",
            "Batch [1170/1563] - Loss: 0.5702, Avg Loss: 0.0418\n",
            "Batch [1180/1563] - Loss: 0.6130, Avg Loss: 0.0418\n",
            "Batch [1190/1563] - Loss: 0.6500, Avg Loss: 0.0418\n",
            "Batch [1200/1563] - Loss: 0.5773, Avg Loss: 0.0418\n",
            "Batch [1210/1563] - Loss: 0.5845, Avg Loss: 0.0417\n",
            "Batch [1220/1563] - Loss: 0.6404, Avg Loss: 0.0417\n",
            "Batch [1230/1563] - Loss: 0.6533, Avg Loss: 0.0417\n",
            "Batch [1240/1563] - Loss: 0.5771, Avg Loss: 0.0417\n",
            "Batch [1250/1563] - Loss: 0.6653, Avg Loss: 0.0417\n",
            "Batch [1260/1563] - Loss: 0.6564, Avg Loss: 0.0417\n",
            "Batch [1270/1563] - Loss: 0.7050, Avg Loss: 0.0416\n",
            "Batch [1280/1563] - Loss: 0.6545, Avg Loss: 0.0416\n",
            "Batch [1290/1563] - Loss: 0.6393, Avg Loss: 0.0416\n",
            "Batch [1300/1563] - Loss: 0.6204, Avg Loss: 0.0416\n",
            "Batch [1310/1563] - Loss: 0.5911, Avg Loss: 0.0416\n",
            "Batch [1320/1563] - Loss: 0.6075, Avg Loss: 0.0415\n",
            "Batch [1330/1563] - Loss: 0.6489, Avg Loss: 0.0415\n",
            "Batch [1340/1563] - Loss: 0.6146, Avg Loss: 0.0415\n",
            "Batch [1350/1563] - Loss: 0.6208, Avg Loss: 0.0415\n",
            "Batch [1360/1563] - Loss: 0.6656, Avg Loss: 0.0415\n",
            "Batch [1370/1563] - Loss: 0.6198, Avg Loss: 0.0414\n",
            "Batch [1380/1563] - Loss: 0.6026, Avg Loss: 0.0414\n",
            "Batch [1390/1563] - Loss: 0.6241, Avg Loss: 0.0414\n",
            "Batch [1400/1563] - Loss: 0.6115, Avg Loss: 0.0414\n",
            "Batch [1410/1563] - Loss: 0.6000, Avg Loss: 0.0414\n",
            "Batch [1420/1563] - Loss: 0.6000, Avg Loss: 0.0413\n",
            "Batch [1430/1563] - Loss: 0.6326, Avg Loss: 0.0413\n",
            "Batch [1440/1563] - Loss: 0.5930, Avg Loss: 0.0413\n",
            "Batch [1450/1563] - Loss: 0.6532, Avg Loss: 0.0413\n",
            "Batch [1460/1563] - Loss: 0.5997, Avg Loss: 0.0413\n",
            "Batch [1470/1563] - Loss: 0.6166, Avg Loss: 0.0412\n",
            "Batch [1480/1563] - Loss: 0.6633, Avg Loss: 0.0412\n",
            "Batch [1490/1563] - Loss: 0.6048, Avg Loss: 0.0412\n",
            "Batch [1500/1563] - Loss: 0.7268, Avg Loss: 0.0412\n",
            "Batch [1510/1563] - Loss: 0.6693, Avg Loss: 0.0412\n",
            "Batch [1520/1563] - Loss: 0.6739, Avg Loss: 0.0411\n",
            "Batch [1530/1563] - Loss: 0.5631, Avg Loss: 0.0411\n",
            "Batch [1540/1563] - Loss: 0.6341, Avg Loss: 0.0411\n",
            "Batch [1550/1563] - Loss: 0.6204, Avg Loss: 0.0411\n",
            "Batch [1560/1563] - Loss: 0.5692, Avg Loss: 0.0411\n",
            "Epoch completed - Total samples: 25000, Average Loss: 0.0411\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:43, 35.16it/s]Eval Batch [20/1563] - Current Accuracy: 0.8125\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.47it/s]Eval Batch [40/1563] - Current Accuracy: 0.8047\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 39.19it/s]Eval Batch [60/1563] - Current Accuracy: 0.7844\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:37, 39.32it/s]Eval Batch [80/1563] - Current Accuracy: 0.7977\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 39.22it/s]Eval Batch [100/1563] - Current Accuracy: 0.8069\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:36, 39.13it/s]Eval Batch [120/1563] - Current Accuracy: 0.8010\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 39.09it/s]Eval Batch [140/1563] - Current Accuracy: 0.7982\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:36, 38.74it/s]Eval Batch [160/1563] - Current Accuracy: 0.7937\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 39.02it/s]Eval Batch [180/1563] - Current Accuracy: 0.7948\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:34, 39.04it/s]Eval Batch [200/1563] - Current Accuracy: 0.7987\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 39.05it/s]Eval Batch [220/1563] - Current Accuracy: 0.7994\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:33, 39.19it/s]Eval Batch [240/1563] - Current Accuracy: 0.7987\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 38.76it/s]Eval Batch [260/1563] - Current Accuracy: 0.7993\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:33, 38.96it/s]Eval Batch [280/1563] - Current Accuracy: 0.8000\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 38.59it/s]Eval Batch [300/1563] - Current Accuracy: 0.7998\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:32, 38.81it/s]Eval Batch [320/1563] - Current Accuracy: 0.7996\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 38.93it/s]Eval Batch [340/1563] - Current Accuracy: 0.7982\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:31, 38.90it/s]Eval Batch [360/1563] - Current Accuracy: 0.8003\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 38.97it/s]Eval Batch [380/1563] - Current Accuracy: 0.7988\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:30, 38.59it/s]Eval Batch [400/1563] - Current Accuracy: 0.7950\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 38.46it/s]Eval Batch [420/1563] - Current Accuracy: 0.7972\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:29, 38.53it/s]Eval Batch [440/1563] - Current Accuracy: 0.7969\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 38.47it/s]Eval Batch [460/1563] - Current Accuracy: 0.7967\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:27, 38.93it/s]Eval Batch [480/1563] - Current Accuracy: 0.7971\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 39.19it/s]Eval Batch [500/1563] - Current Accuracy: 0.7961\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 39.26it/s]Eval Batch [520/1563] - Current Accuracy: 0.7962\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 39.22it/s]Eval Batch [540/1563] - Current Accuracy: 0.7955\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 39.27it/s]Eval Batch [560/1563] - Current Accuracy: 0.7949\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 39.14it/s]Eval Batch [580/1563] - Current Accuracy: 0.7909\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 39.17it/s]Eval Batch [600/1563] - Current Accuracy: 0.7907\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 39.21it/s]Eval Batch [620/1563] - Current Accuracy: 0.7896\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 39.16it/s]Eval Batch [640/1563] - Current Accuracy: 0.7903\n",
            "Evaluating SNN:  42% 657/1563 [00:16<00:23, 39.21it/s]Eval Batch [660/1563] - Current Accuracy: 0.7928\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 39.20it/s]Eval Batch [680/1563] - Current Accuracy: 0.7947\n",
            "Evaluating SNN:  45% 697/1563 [00:17<00:22, 39.13it/s]Eval Batch [700/1563] - Current Accuracy: 0.7953\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 38.98it/s]Eval Batch [720/1563] - Current Accuracy: 0.7945\n",
            "Evaluating SNN:  47% 737/1563 [00:19<00:21, 38.96it/s]Eval Batch [740/1563] - Current Accuracy: 0.7938\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 38.69it/s]Eval Batch [760/1563] - Current Accuracy: 0.7945\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 38.72it/s]Eval Batch [780/1563] - Current Accuracy: 0.7940\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 39.02it/s]Eval Batch [800/1563] - Current Accuracy: 0.7941\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 39.05it/s]Eval Batch [820/1563] - Current Accuracy: 0.7939\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 39.05it/s]Eval Batch [840/1563] - Current Accuracy: 0.7910\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 39.06it/s]Eval Batch [860/1563] - Current Accuracy: 0.7900\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 38.97it/s]Eval Batch [880/1563] - Current Accuracy: 0.7880\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 39.17it/s]Eval Batch [900/1563] - Current Accuracy: 0.7860\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 39.24it/s]Eval Batch [920/1563] - Current Accuracy: 0.7850\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:15, 39.23it/s]Eval Batch [940/1563] - Current Accuracy: 0.7843\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 39.14it/s]Eval Batch [960/1563] - Current Accuracy: 0.7826\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:14, 39.21it/s]Eval Batch [980/1563] - Current Accuracy: 0.7810\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 39.21it/s]Eval Batch [1000/1563] - Current Accuracy: 0.7805\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:13, 39.17it/s]Eval Batch [1020/1563] - Current Accuracy: 0.7776\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 39.23it/s]Eval Batch [1040/1563] - Current Accuracy: 0.7752\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:12, 39.17it/s]Eval Batch [1060/1563] - Current Accuracy: 0.7737\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 39.11it/s]Eval Batch [1080/1563] - Current Accuracy: 0.7719\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:11, 39.10it/s]Eval Batch [1100/1563] - Current Accuracy: 0.7717\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 39.13it/s]Eval Batch [1120/1563] - Current Accuracy: 0.7721\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 39.07it/s]Eval Batch [1140/1563] - Current Accuracy: 0.7705\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 39.07it/s]Eval Batch [1160/1563] - Current Accuracy: 0.7690\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 39.20it/s]Eval Batch [1180/1563] - Current Accuracy: 0.7685\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 39.01it/s]Eval Batch [1200/1563] - Current Accuracy: 0.7671\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 38.63it/s]Eval Batch [1220/1563] - Current Accuracy: 0.7657\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 38.78it/s]Eval Batch [1240/1563] - Current Accuracy: 0.7644\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 38.80it/s]Eval Batch [1260/1563] - Current Accuracy: 0.7627\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 38.81it/s]Eval Batch [1280/1563] - Current Accuracy: 0.7634\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 38.98it/s]Eval Batch [1300/1563] - Current Accuracy: 0.7638\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 38.97it/s]Eval Batch [1320/1563] - Current Accuracy: 0.7624\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 38.90it/s]Eval Batch [1340/1563] - Current Accuracy: 0.7618\n",
            "Evaluating SNN:  87% 1357/1563 [00:34<00:05, 38.99it/s]Eval Batch [1360/1563] - Current Accuracy: 0.7609\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 39.14it/s]Eval Batch [1380/1563] - Current Accuracy: 0.7597\n",
            "Evaluating SNN:  89% 1397/1563 [00:35<00:04, 39.02it/s]Eval Batch [1400/1563] - Current Accuracy: 0.7583\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 39.09it/s]Eval Batch [1420/1563] - Current Accuracy: 0.7585\n",
            "Evaluating SNN:  92% 1437/1563 [00:36<00:03, 39.11it/s]Eval Batch [1440/1563] - Current Accuracy: 0.7579\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 39.13it/s]Eval Batch [1460/1563] - Current Accuracy: 0.7585\n",
            "Evaluating SNN:  94% 1477/1563 [00:37<00:02, 39.17it/s]Eval Batch [1480/1563] - Current Accuracy: 0.7587\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 39.16it/s]Eval Batch [1500/1563] - Current Accuracy: 0.7578\n",
            "Evaluating SNN:  97% 1517/1563 [00:39<00:01, 39.14it/s]Eval Batch [1520/1563] - Current Accuracy: 0.7578\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 39.13it/s]Eval Batch [1540/1563] - Current Accuracy: 0.7568\n",
            "Evaluating SNN: 100% 1557/1563 [00:40<00:00, 39.20it/s]Eval Batch [1560/1563] - Current Accuracy: 0.7563\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.87it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.7560\n",
            "INFO:root:Saved model weights at epoch 1 to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_1.pth\n",
            "[2025-09-24 17:08:21,000][main.py][line:639][INFO] Saved model weights at epoch 1 to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_1.pth\n",
            "INFO:root:Save directory exists: True\n",
            "[2025-09-24 17:08:21,000][main.py][line:640][INFO] Save directory exists: True\n",
            "INFO:root:File size: 265511595 bytes\n",
            "[2025-09-24 17:08:21,000][main.py][line:641][INFO] File size: 265511595 bytes\n",
            "INFO:root:SNNs training Epoch 1 [distilbert_base_qcfs]: Val_loss: 0.041066366617679595\n",
            "[2025-09-24 17:08:21,000][main.py][line:644][INFO] SNNs training Epoch 1 [distilbert_base_qcfs]: Val_loss: 0.041066366617679595\n",
            "INFO:root:SNNs training Epoch 1 [distilbert_base_qcfs]: Test Acc: 0.7560 Best Acc: 0.7560\n",
            "[2025-09-24 17:08:21,000][main.py][line:645][INFO] SNNs training Epoch 1 [distilbert_base_qcfs]: Test Acc: 0.7560 Best Acc: 0.7560\n",
            "Starting training with 1563 batches...\n",
            "Batch [10/1563] - Loss: 0.6029, Avg Loss: 0.0357\n",
            "Batch [20/1563] - Loss: 0.5467, Avg Loss: 0.0366\n",
            "Batch [30/1563] - Loss: 0.6328, Avg Loss: 0.0368\n",
            "Batch [40/1563] - Loss: 0.5098, Avg Loss: 0.0370\n",
            "Batch [50/1563] - Loss: 0.5444, Avg Loss: 0.0370\n",
            "Batch [60/1563] - Loss: 0.6211, Avg Loss: 0.0371\n",
            "Batch [70/1563] - Loss: 0.5739, Avg Loss: 0.0372\n",
            "Batch [80/1563] - Loss: 0.5378, Avg Loss: 0.0372\n",
            "Batch [90/1563] - Loss: 0.5359, Avg Loss: 0.0374\n",
            "Batch [100/1563] - Loss: 0.5530, Avg Loss: 0.0373\n",
            "Batch [110/1563] - Loss: 0.5808, Avg Loss: 0.0374\n",
            "Batch [120/1563] - Loss: 0.6377, Avg Loss: 0.0374\n",
            "Batch [130/1563] - Loss: 0.5825, Avg Loss: 0.0375\n",
            "Batch [140/1563] - Loss: 0.6184, Avg Loss: 0.0375\n",
            "Batch [150/1563] - Loss: 0.6248, Avg Loss: 0.0374\n",
            "Batch [160/1563] - Loss: 0.6320, Avg Loss: 0.0374\n",
            "Batch [170/1563] - Loss: 0.5413, Avg Loss: 0.0372\n",
            "Batch [180/1563] - Loss: 0.5432, Avg Loss: 0.0373\n",
            "Batch [190/1563] - Loss: 0.5304, Avg Loss: 0.0371\n",
            "Batch [200/1563] - Loss: 0.5473, Avg Loss: 0.0371\n",
            "Batch [210/1563] - Loss: 0.5906, Avg Loss: 0.0371\n",
            "Batch [220/1563] - Loss: 0.6105, Avg Loss: 0.0370\n",
            "Batch [230/1563] - Loss: 0.5938, Avg Loss: 0.0369\n",
            "Batch [240/1563] - Loss: 0.6391, Avg Loss: 0.0369\n",
            "Batch [250/1563] - Loss: 0.6149, Avg Loss: 0.0369\n",
            "Batch [260/1563] - Loss: 0.5518, Avg Loss: 0.0368\n",
            "Batch [270/1563] - Loss: 0.5796, Avg Loss: 0.0368\n",
            "Batch [280/1563] - Loss: 0.5518, Avg Loss: 0.0368\n",
            "Batch [290/1563] - Loss: 0.5271, Avg Loss: 0.0367\n",
            "Batch [300/1563] - Loss: 0.4801, Avg Loss: 0.0367\n",
            "Batch [310/1563] - Loss: 0.5701, Avg Loss: 0.0367\n",
            "Batch [320/1563] - Loss: 0.6284, Avg Loss: 0.0367\n",
            "Batch [330/1563] - Loss: 0.5589, Avg Loss: 0.0367\n",
            "Batch [340/1563] - Loss: 0.5160, Avg Loss: 0.0366\n",
            "Batch [350/1563] - Loss: 0.5522, Avg Loss: 0.0366\n",
            "Batch [360/1563] - Loss: 0.5462, Avg Loss: 0.0366\n",
            "Batch [370/1563] - Loss: 0.5003, Avg Loss: 0.0365\n",
            "Batch [380/1563] - Loss: 0.6040, Avg Loss: 0.0366\n",
            "Batch [390/1563] - Loss: 0.5701, Avg Loss: 0.0366\n",
            "Batch [400/1563] - Loss: 0.5947, Avg Loss: 0.0365\n",
            "Batch [410/1563] - Loss: 0.5560, Avg Loss: 0.0365\n",
            "Batch [420/1563] - Loss: 0.5461, Avg Loss: 0.0364\n",
            "Batch [430/1563] - Loss: 0.5119, Avg Loss: 0.0364\n",
            "Batch [440/1563] - Loss: 0.5304, Avg Loss: 0.0364\n",
            "Batch [450/1563] - Loss: 0.5378, Avg Loss: 0.0364\n",
            "Batch [460/1563] - Loss: 0.4547, Avg Loss: 0.0364\n",
            "Batch [470/1563] - Loss: 0.5038, Avg Loss: 0.0363\n",
            "Batch [480/1563] - Loss: 0.5135, Avg Loss: 0.0363\n",
            "Batch [490/1563] - Loss: 0.5060, Avg Loss: 0.0363\n",
            "Batch [500/1563] - Loss: 0.5867, Avg Loss: 0.0363\n",
            "Batch [510/1563] - Loss: 0.5971, Avg Loss: 0.0362\n",
            "Batch [520/1563] - Loss: 0.5998, Avg Loss: 0.0362\n",
            "Batch [530/1563] - Loss: 0.6065, Avg Loss: 0.0362\n",
            "Batch [540/1563] - Loss: 0.5288, Avg Loss: 0.0362\n",
            "Batch [550/1563] - Loss: 0.5144, Avg Loss: 0.0361\n",
            "Batch [560/1563] - Loss: 0.5677, Avg Loss: 0.0361\n",
            "Batch [570/1563] - Loss: 0.6071, Avg Loss: 0.0361\n",
            "Batch [580/1563] - Loss: 0.6326, Avg Loss: 0.0360\n",
            "Batch [590/1563] - Loss: 0.5772, Avg Loss: 0.0360\n",
            "Batch [600/1563] - Loss: 0.7211, Avg Loss: 0.0360\n",
            "Batch [610/1563] - Loss: 0.5723, Avg Loss: 0.0359\n",
            "Batch [620/1563] - Loss: 0.5012, Avg Loss: 0.0359\n",
            "Batch [630/1563] - Loss: 0.5861, Avg Loss: 0.0359\n",
            "Batch [640/1563] - Loss: 0.5740, Avg Loss: 0.0359\n",
            "Batch [650/1563] - Loss: 0.5112, Avg Loss: 0.0359\n",
            "Batch [660/1563] - Loss: 0.5208, Avg Loss: 0.0358\n",
            "Batch [670/1563] - Loss: 0.5570, Avg Loss: 0.0358\n",
            "Batch [680/1563] - Loss: 0.5111, Avg Loss: 0.0357\n",
            "Batch [690/1563] - Loss: 0.6451, Avg Loss: 0.0357\n",
            "Batch [700/1563] - Loss: 0.5330, Avg Loss: 0.0357\n",
            "Batch [710/1563] - Loss: 0.4842, Avg Loss: 0.0357\n",
            "Batch [720/1563] - Loss: 0.5846, Avg Loss: 0.0356\n",
            "Batch [730/1563] - Loss: 0.5734, Avg Loss: 0.0356\n",
            "Batch [740/1563] - Loss: 0.5676, Avg Loss: 0.0356\n",
            "Batch [750/1563] - Loss: 0.4937, Avg Loss: 0.0355\n",
            "Batch [760/1563] - Loss: 0.7299, Avg Loss: 0.0355\n",
            "Batch [770/1563] - Loss: 0.5333, Avg Loss: 0.0355\n",
            "Batch [780/1563] - Loss: 0.6183, Avg Loss: 0.0355\n",
            "Batch [790/1563] - Loss: 0.4649, Avg Loss: 0.0355\n",
            "Batch [800/1563] - Loss: 0.5097, Avg Loss: 0.0355\n",
            "Batch [810/1563] - Loss: 0.5172, Avg Loss: 0.0354\n",
            "Batch [820/1563] - Loss: 0.5500, Avg Loss: 0.0354\n",
            "Batch [830/1563] - Loss: 0.5870, Avg Loss: 0.0353\n",
            "Batch [840/1563] - Loss: 0.4200, Avg Loss: 0.0353\n",
            "Batch [850/1563] - Loss: 0.5358, Avg Loss: 0.0353\n",
            "Batch [860/1563] - Loss: 0.4550, Avg Loss: 0.0352\n",
            "Batch [870/1563] - Loss: 0.5717, Avg Loss: 0.0352\n",
            "Batch [880/1563] - Loss: 0.5287, Avg Loss: 0.0351\n",
            "Batch [890/1563] - Loss: 0.4164, Avg Loss: 0.0351\n",
            "Batch [900/1563] - Loss: 0.4613, Avg Loss: 0.0351\n",
            "Batch [910/1563] - Loss: 0.4773, Avg Loss: 0.0350\n",
            "Batch [920/1563] - Loss: 0.4672, Avg Loss: 0.0350\n",
            "Batch [930/1563] - Loss: 0.4313, Avg Loss: 0.0349\n",
            "Batch [940/1563] - Loss: 0.5064, Avg Loss: 0.0349\n",
            "Batch [950/1563] - Loss: 0.4684, Avg Loss: 0.0349\n",
            "Batch [960/1563] - Loss: 0.6055, Avg Loss: 0.0349\n",
            "Batch [970/1563] - Loss: 0.5766, Avg Loss: 0.0348\n",
            "Batch [980/1563] - Loss: 0.4085, Avg Loss: 0.0348\n",
            "Batch [990/1563] - Loss: 0.4651, Avg Loss: 0.0348\n",
            "Batch [1000/1563] - Loss: 0.4905, Avg Loss: 0.0347\n",
            "Batch [1010/1563] - Loss: 0.5191, Avg Loss: 0.0347\n",
            "Batch [1020/1563] - Loss: 0.5166, Avg Loss: 0.0347\n",
            "Batch [1030/1563] - Loss: 0.5496, Avg Loss: 0.0346\n",
            "Batch [1040/1563] - Loss: 0.5651, Avg Loss: 0.0346\n",
            "Batch [1050/1563] - Loss: 0.4149, Avg Loss: 0.0346\n",
            "Batch [1060/1563] - Loss: 0.5084, Avg Loss: 0.0346\n",
            "Batch [1070/1563] - Loss: 0.4530, Avg Loss: 0.0345\n",
            "Batch [1080/1563] - Loss: 0.4703, Avg Loss: 0.0345\n",
            "Batch [1090/1563] - Loss: 0.5500, Avg Loss: 0.0345\n",
            "Batch [1100/1563] - Loss: 0.6435, Avg Loss: 0.0345\n",
            "Batch [1110/1563] - Loss: 0.4339, Avg Loss: 0.0344\n",
            "Batch [1120/1563] - Loss: 0.4220, Avg Loss: 0.0344\n",
            "Batch [1130/1563] - Loss: 0.5099, Avg Loss: 0.0343\n",
            "Batch [1140/1563] - Loss: 0.4304, Avg Loss: 0.0343\n",
            "Batch [1150/1563] - Loss: 0.4727, Avg Loss: 0.0343\n",
            "Batch [1160/1563] - Loss: 0.4957, Avg Loss: 0.0342\n",
            "Batch [1170/1563] - Loss: 0.5392, Avg Loss: 0.0342\n",
            "Batch [1180/1563] - Loss: 0.4455, Avg Loss: 0.0342\n",
            "Batch [1190/1563] - Loss: 0.4682, Avg Loss: 0.0342\n",
            "Batch [1200/1563] - Loss: 0.4919, Avg Loss: 0.0341\n",
            "Batch [1210/1563] - Loss: 0.4592, Avg Loss: 0.0341\n",
            "Batch [1220/1563] - Loss: 0.4799, Avg Loss: 0.0340\n",
            "Batch [1230/1563] - Loss: 0.3887, Avg Loss: 0.0340\n",
            "Batch [1240/1563] - Loss: 0.3799, Avg Loss: 0.0340\n",
            "Batch [1250/1563] - Loss: 0.5034, Avg Loss: 0.0339\n",
            "Batch [1260/1563] - Loss: 0.5468, Avg Loss: 0.0339\n",
            "Batch [1270/1563] - Loss: 0.5683, Avg Loss: 0.0339\n",
            "Batch [1280/1563] - Loss: 0.4861, Avg Loss: 0.0339\n",
            "Batch [1290/1563] - Loss: 0.4295, Avg Loss: 0.0339\n",
            "Batch [1300/1563] - Loss: 0.4633, Avg Loss: 0.0338\n",
            "Batch [1310/1563] - Loss: 0.4432, Avg Loss: 0.0338\n",
            "Batch [1320/1563] - Loss: 0.4865, Avg Loss: 0.0337\n",
            "Batch [1330/1563] - Loss: 0.5166, Avg Loss: 0.0337\n",
            "Batch [1340/1563] - Loss: 0.5119, Avg Loss: 0.0337\n",
            "Batch [1350/1563] - Loss: 0.4732, Avg Loss: 0.0337\n",
            "Batch [1360/1563] - Loss: 0.6136, Avg Loss: 0.0336\n",
            "Batch [1370/1563] - Loss: 0.5226, Avg Loss: 0.0336\n",
            "Batch [1380/1563] - Loss: 0.4445, Avg Loss: 0.0336\n",
            "Batch [1390/1563] - Loss: 0.4495, Avg Loss: 0.0335\n",
            "Batch [1400/1563] - Loss: 0.5561, Avg Loss: 0.0335\n",
            "Batch [1410/1563] - Loss: 0.3749, Avg Loss: 0.0335\n",
            "Batch [1420/1563] - Loss: 0.3883, Avg Loss: 0.0334\n",
            "Batch [1430/1563] - Loss: 0.3863, Avg Loss: 0.0334\n",
            "Batch [1440/1563] - Loss: 0.4384, Avg Loss: 0.0334\n",
            "Batch [1450/1563] - Loss: 0.4342, Avg Loss: 0.0333\n",
            "Batch [1460/1563] - Loss: 0.3928, Avg Loss: 0.0333\n",
            "Batch [1470/1563] - Loss: 0.5035, Avg Loss: 0.0333\n",
            "Batch [1480/1563] - Loss: 0.3561, Avg Loss: 0.0332\n",
            "Batch [1490/1563] - Loss: 0.4702, Avg Loss: 0.0332\n",
            "Batch [1500/1563] - Loss: 0.3695, Avg Loss: 0.0331\n",
            "Batch [1510/1563] - Loss: 0.4499, Avg Loss: 0.0331\n",
            "Batch [1520/1563] - Loss: 0.5512, Avg Loss: 0.0331\n",
            "Batch [1530/1563] - Loss: 0.3542, Avg Loss: 0.0330\n",
            "Batch [1540/1563] - Loss: 0.3237, Avg Loss: 0.0330\n",
            "Batch [1550/1563] - Loss: 0.3438, Avg Loss: 0.0330\n",
            "Batch [1560/1563] - Loss: 0.4012, Avg Loss: 0.0329\n",
            "Epoch completed - Total samples: 25000, Average Loss: 0.0329\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:44, 35.04it/s]Eval Batch [20/1563] - Current Accuracy: 0.8625\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.58it/s]Eval Batch [40/1563] - Current Accuracy: 0.8688\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 38.98it/s]Eval Batch [60/1563] - Current Accuracy: 0.8521\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:37, 39.18it/s]Eval Batch [80/1563] - Current Accuracy: 0.8602\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 39.17it/s]Eval Batch [100/1563] - Current Accuracy: 0.8662\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:37, 38.97it/s]Eval Batch [120/1563] - Current Accuracy: 0.8656\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 39.08it/s]Eval Batch [140/1563] - Current Accuracy: 0.8656\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:35, 39.11it/s]Eval Batch [160/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 38.98it/s]Eval Batch [180/1563] - Current Accuracy: 0.8566\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:34, 39.04it/s]Eval Batch [200/1563] - Current Accuracy: 0.8600\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 39.23it/s]Eval Batch [220/1563] - Current Accuracy: 0.8591\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:33, 39.25it/s]Eval Batch [240/1563] - Current Accuracy: 0.8607\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 39.32it/s]Eval Batch [260/1563] - Current Accuracy: 0.8625\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:32, 39.30it/s]Eval Batch [280/1563] - Current Accuracy: 0.8656\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 39.30it/s]Eval Batch [300/1563] - Current Accuracy: 0.8646\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:31, 39.14it/s]Eval Batch [320/1563] - Current Accuracy: 0.8658\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 39.24it/s]Eval Batch [340/1563] - Current Accuracy: 0.8642\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:30, 39.06it/s]Eval Batch [360/1563] - Current Accuracy: 0.8653\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 39.04it/s]Eval Batch [380/1563] - Current Accuracy: 0.8646\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:29, 39.06it/s]Eval Batch [400/1563] - Current Accuracy: 0.8628\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 38.96it/s]Eval Batch [420/1563] - Current Accuracy: 0.8634\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:28, 39.12it/s]Eval Batch [440/1563] - Current Accuracy: 0.8621\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 39.06it/s]Eval Batch [460/1563] - Current Accuracy: 0.8617\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:27, 39.05it/s]Eval Batch [480/1563] - Current Accuracy: 0.8626\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 39.15it/s]Eval Batch [500/1563] - Current Accuracy: 0.8624\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 39.20it/s]Eval Batch [520/1563] - Current Accuracy: 0.8624\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 39.19it/s]Eval Batch [540/1563] - Current Accuracy: 0.8620\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 39.22it/s]Eval Batch [560/1563] - Current Accuracy: 0.8612\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 39.05it/s]Eval Batch [580/1563] - Current Accuracy: 0.8587\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:25, 38.61it/s]Eval Batch [600/1563] - Current Accuracy: 0.8579\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 38.94it/s]Eval Batch [620/1563] - Current Accuracy: 0.8578\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 38.95it/s]Eval Batch [640/1563] - Current Accuracy: 0.8581\n",
            "Evaluating SNN:  42% 657/1563 [00:16<00:23, 39.15it/s]Eval Batch [660/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 39.28it/s]Eval Batch [680/1563] - Current Accuracy: 0.8603\n",
            "Evaluating SNN:  45% 697/1563 [00:17<00:22, 39.25it/s]Eval Batch [700/1563] - Current Accuracy: 0.8599\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 39.25it/s]Eval Batch [720/1563] - Current Accuracy: 0.8593\n",
            "Evaluating SNN:  47% 737/1563 [00:18<00:21, 39.26it/s]Eval Batch [740/1563] - Current Accuracy: 0.8590\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 39.22it/s]Eval Batch [760/1563] - Current Accuracy: 0.8594\n",
            "Evaluating SNN:  50% 777/1563 [00:19<00:20, 39.26it/s]Eval Batch [780/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 39.17it/s]Eval Batch [800/1563] - Current Accuracy: 0.8582\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 38.99it/s]Eval Batch [820/1563] - Current Accuracy: 0.8579\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 38.79it/s]Eval Batch [840/1563] - Current Accuracy: 0.8553\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 38.83it/s]Eval Batch [860/1563] - Current Accuracy: 0.8547\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 38.83it/s]Eval Batch [880/1563] - Current Accuracy: 0.8528\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 38.94it/s]Eval Batch [900/1563] - Current Accuracy: 0.8515\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 39.16it/s]Eval Batch [920/1563] - Current Accuracy: 0.8514\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:15, 39.18it/s]Eval Batch [940/1563] - Current Accuracy: 0.8505\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 39.14it/s]Eval Batch [960/1563] - Current Accuracy: 0.8499\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:14, 39.14it/s]Eval Batch [980/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 39.11it/s]Eval Batch [1000/1563] - Current Accuracy: 0.8494\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:13, 39.12it/s]Eval Batch [1020/1563] - Current Accuracy: 0.8475\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 38.91it/s]Eval Batch [1040/1563] - Current Accuracy: 0.8462\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:13, 38.77it/s]Eval Batch [1060/1563] - Current Accuracy: 0.8452\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 38.91it/s]Eval Batch [1080/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:11, 39.08it/s]Eval Batch [1100/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 39.22it/s]Eval Batch [1120/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 39.16it/s]Eval Batch [1140/1563] - Current Accuracy: 0.8421\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 39.29it/s]Eval Batch [1160/1563] - Current Accuracy: 0.8409\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 39.29it/s]Eval Batch [1180/1563] - Current Accuracy: 0.8408\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 39.30it/s]Eval Batch [1200/1563] - Current Accuracy: 0.8402\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 39.25it/s]Eval Batch [1220/1563] - Current Accuracy: 0.8393\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 39.19it/s]Eval Batch [1240/1563] - Current Accuracy: 0.8387\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 39.22it/s]Eval Batch [1260/1563] - Current Accuracy: 0.8382\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 39.21it/s]Eval Batch [1280/1563] - Current Accuracy: 0.8387\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 39.14it/s]Eval Batch [1300/1563] - Current Accuracy: 0.8387\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 39.14it/s]Eval Batch [1320/1563] - Current Accuracy: 0.8379\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 39.23it/s]Eval Batch [1340/1563] - Current Accuracy: 0.8373\n",
            "Evaluating SNN:  87% 1357/1563 [00:34<00:05, 39.12it/s]Eval Batch [1360/1563] - Current Accuracy: 0.8372\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 39.29it/s]Eval Batch [1380/1563] - Current Accuracy: 0.8365\n",
            "Evaluating SNN:  89% 1397/1563 [00:35<00:04, 39.27it/s]Eval Batch [1400/1563] - Current Accuracy: 0.8352\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 39.31it/s]Eval Batch [1420/1563] - Current Accuracy: 0.8346\n",
            "Evaluating SNN:  92% 1437/1563 [00:36<00:03, 39.25it/s]Eval Batch [1440/1563] - Current Accuracy: 0.8342\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 39.20it/s]Eval Batch [1460/1563] - Current Accuracy: 0.8345\n",
            "Evaluating SNN:  94% 1477/1563 [00:37<00:02, 39.22it/s]Eval Batch [1480/1563] - Current Accuracy: 0.8347\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 39.02it/s]Eval Batch [1500/1563] - Current Accuracy: 0.8339\n",
            "Evaluating SNN:  97% 1517/1563 [00:38<00:01, 38.77it/s]Eval Batch [1520/1563] - Current Accuracy: 0.8337\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 38.52it/s]Eval Batch [1540/1563] - Current Accuracy: 0.8332\n",
            "Evaluating SNN: 100% 1557/1563 [00:39<00:00, 39.06it/s]Eval Batch [1560/1563] - Current Accuracy: 0.8324\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.96it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.8321\n",
            "INFO:root:Saved model weights at epoch 2 to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_2.pth\n",
            "[2025-09-24 17:11:04,909][main.py][line:639][INFO] Saved model weights at epoch 2 to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_2.pth\n",
            "INFO:root:Save directory exists: True\n",
            "[2025-09-24 17:11:04,910][main.py][line:640][INFO] Save directory exists: True\n",
            "INFO:root:File size: 265511595 bytes\n",
            "[2025-09-24 17:11:04,910][main.py][line:641][INFO] File size: 265511595 bytes\n",
            "INFO:root:SNNs training Epoch 2 [distilbert_base_qcfs]: Val_loss: 0.03292086650848389\n",
            "[2025-09-24 17:11:04,910][main.py][line:644][INFO] SNNs training Epoch 2 [distilbert_base_qcfs]: Val_loss: 0.03292086650848389\n",
            "INFO:root:SNNs training Epoch 2 [distilbert_base_qcfs]: Test Acc: 0.8321 Best Acc: 0.8321\n",
            "[2025-09-24 17:11:04,910][main.py][line:645][INFO] SNNs training Epoch 2 [distilbert_base_qcfs]: Test Acc: 0.8321 Best Acc: 0.8321\n",
            "Starting training with 1563 batches...\n",
            "Batch [10/1563] - Loss: 0.6499, Avg Loss: 0.0290\n",
            "Batch [20/1563] - Loss: 0.4736, Avg Loss: 0.0294\n",
            "Batch [30/1563] - Loss: 0.3961, Avg Loss: 0.0284\n",
            "Batch [40/1563] - Loss: 0.2943, Avg Loss: 0.0282\n",
            "Batch [50/1563] - Loss: 0.4372, Avg Loss: 0.0281\n",
            "Batch [60/1563] - Loss: 0.3443, Avg Loss: 0.0276\n",
            "Batch [70/1563] - Loss: 0.3393, Avg Loss: 0.0274\n",
            "Batch [80/1563] - Loss: 0.3660, Avg Loss: 0.0272\n",
            "Batch [90/1563] - Loss: 0.6048, Avg Loss: 0.0274\n",
            "Batch [100/1563] - Loss: 0.6488, Avg Loss: 0.0277\n",
            "Batch [110/1563] - Loss: 0.5413, Avg Loss: 0.0279\n",
            "Batch [120/1563] - Loss: 0.4674, Avg Loss: 0.0280\n",
            "Batch [130/1563] - Loss: 0.4905, Avg Loss: 0.0280\n",
            "Batch [140/1563] - Loss: 0.3012, Avg Loss: 0.0280\n",
            "Batch [150/1563] - Loss: 0.5704, Avg Loss: 0.0281\n",
            "Batch [160/1563] - Loss: 0.4565, Avg Loss: 0.0281\n",
            "Batch [170/1563] - Loss: 0.3470, Avg Loss: 0.0279\n",
            "Batch [180/1563] - Loss: 0.5994, Avg Loss: 0.0278\n",
            "Batch [190/1563] - Loss: 0.5992, Avg Loss: 0.0278\n",
            "Batch [200/1563] - Loss: 0.4551, Avg Loss: 0.0277\n",
            "Batch [210/1563] - Loss: 0.2323, Avg Loss: 0.0276\n",
            "Batch [220/1563] - Loss: 0.3830, Avg Loss: 0.0276\n",
            "Batch [230/1563] - Loss: 0.3936, Avg Loss: 0.0277\n",
            "Batch [240/1563] - Loss: 0.6467, Avg Loss: 0.0277\n",
            "Batch [250/1563] - Loss: 0.5711, Avg Loss: 0.0278\n",
            "Batch [260/1563] - Loss: 0.3405, Avg Loss: 0.0278\n",
            "Batch [270/1563] - Loss: 0.4874, Avg Loss: 0.0278\n",
            "Batch [280/1563] - Loss: 0.3451, Avg Loss: 0.0277\n",
            "Batch [290/1563] - Loss: 0.3548, Avg Loss: 0.0276\n",
            "Batch [300/1563] - Loss: 0.2626, Avg Loss: 0.0276\n",
            "Batch [310/1563] - Loss: 0.4558, Avg Loss: 0.0275\n",
            "Batch [320/1563] - Loss: 0.5003, Avg Loss: 0.0276\n",
            "Batch [330/1563] - Loss: 0.4795, Avg Loss: 0.0276\n",
            "Batch [340/1563] - Loss: 0.4418, Avg Loss: 0.0276\n",
            "Batch [350/1563] - Loss: 0.4057, Avg Loss: 0.0275\n",
            "Batch [360/1563] - Loss: 0.3593, Avg Loss: 0.0275\n",
            "Batch [370/1563] - Loss: 0.4999, Avg Loss: 0.0275\n",
            "Batch [380/1563] - Loss: 0.6301, Avg Loss: 0.0276\n",
            "Batch [390/1563] - Loss: 0.3341, Avg Loss: 0.0275\n",
            "Batch [400/1563] - Loss: 0.4673, Avg Loss: 0.0274\n",
            "Batch [410/1563] - Loss: 0.4161, Avg Loss: 0.0274\n",
            "Batch [420/1563] - Loss: 0.3617, Avg Loss: 0.0274\n",
            "Batch [430/1563] - Loss: 0.4328, Avg Loss: 0.0273\n",
            "Batch [440/1563] - Loss: 0.4263, Avg Loss: 0.0273\n",
            "Batch [450/1563] - Loss: 0.5884, Avg Loss: 0.0273\n",
            "Batch [460/1563] - Loss: 0.2961, Avg Loss: 0.0273\n",
            "Batch [470/1563] - Loss: 0.4627, Avg Loss: 0.0272\n",
            "Batch [480/1563] - Loss: 0.5937, Avg Loss: 0.0273\n",
            "Batch [490/1563] - Loss: 0.6122, Avg Loss: 0.0273\n",
            "Batch [500/1563] - Loss: 0.3038, Avg Loss: 0.0272\n",
            "Batch [510/1563] - Loss: 0.4194, Avg Loss: 0.0272\n",
            "Batch [520/1563] - Loss: 0.4526, Avg Loss: 0.0272\n",
            "Batch [530/1563] - Loss: 0.4932, Avg Loss: 0.0271\n",
            "Batch [540/1563] - Loss: 0.3395, Avg Loss: 0.0272\n",
            "Batch [550/1563] - Loss: 0.4370, Avg Loss: 0.0272\n",
            "Batch [560/1563] - Loss: 0.4344, Avg Loss: 0.0272\n",
            "Batch [570/1563] - Loss: 0.3693, Avg Loss: 0.0272\n",
            "Batch [580/1563] - Loss: 0.5364, Avg Loss: 0.0272\n",
            "Batch [590/1563] - Loss: 0.3327, Avg Loss: 0.0272\n",
            "Batch [600/1563] - Loss: 0.4063, Avg Loss: 0.0272\n",
            "Batch [610/1563] - Loss: 0.4774, Avg Loss: 0.0272\n",
            "Batch [620/1563] - Loss: 0.3835, Avg Loss: 0.0272\n",
            "Batch [630/1563] - Loss: 0.5508, Avg Loss: 0.0272\n",
            "Batch [640/1563] - Loss: 0.4962, Avg Loss: 0.0272\n",
            "Batch [650/1563] - Loss: 0.3617, Avg Loss: 0.0271\n",
            "Batch [660/1563] - Loss: 0.5186, Avg Loss: 0.0271\n",
            "Batch [670/1563] - Loss: 0.3796, Avg Loss: 0.0271\n",
            "Batch [680/1563] - Loss: 0.2975, Avg Loss: 0.0271\n",
            "Batch [690/1563] - Loss: 0.5392, Avg Loss: 0.0270\n",
            "Batch [700/1563] - Loss: 0.5030, Avg Loss: 0.0270\n",
            "Batch [710/1563] - Loss: 0.4861, Avg Loss: 0.0271\n",
            "Batch [720/1563] - Loss: 0.4462, Avg Loss: 0.0271\n",
            "Batch [730/1563] - Loss: 0.3675, Avg Loss: 0.0271\n",
            "Batch [740/1563] - Loss: 0.4699, Avg Loss: 0.0271\n",
            "Batch [750/1563] - Loss: 0.2631, Avg Loss: 0.0271\n",
            "Batch [760/1563] - Loss: 0.4714, Avg Loss: 0.0271\n",
            "Batch [770/1563] - Loss: 0.4314, Avg Loss: 0.0270\n",
            "Batch [780/1563] - Loss: 0.3832, Avg Loss: 0.0271\n",
            "Batch [790/1563] - Loss: 0.4020, Avg Loss: 0.0271\n",
            "Batch [800/1563] - Loss: 0.4159, Avg Loss: 0.0271\n",
            "Batch [810/1563] - Loss: 0.4002, Avg Loss: 0.0270\n",
            "Batch [820/1563] - Loss: 0.5626, Avg Loss: 0.0270\n",
            "Batch [830/1563] - Loss: 0.3382, Avg Loss: 0.0270\n",
            "Batch [840/1563] - Loss: 0.3123, Avg Loss: 0.0269\n",
            "Batch [850/1563] - Loss: 0.2766, Avg Loss: 0.0269\n",
            "Batch [860/1563] - Loss: 0.3186, Avg Loss: 0.0269\n",
            "Batch [870/1563] - Loss: 0.3551, Avg Loss: 0.0269\n",
            "Batch [880/1563] - Loss: 0.3255, Avg Loss: 0.0269\n",
            "Batch [890/1563] - Loss: 0.3927, Avg Loss: 0.0269\n",
            "Batch [900/1563] - Loss: 0.4091, Avg Loss: 0.0268\n",
            "Batch [910/1563] - Loss: 0.4475, Avg Loss: 0.0268\n",
            "Batch [920/1563] - Loss: 0.3370, Avg Loss: 0.0268\n",
            "Batch [930/1563] - Loss: 0.4246, Avg Loss: 0.0267\n",
            "Batch [940/1563] - Loss: 0.5265, Avg Loss: 0.0267\n",
            "Batch [950/1563] - Loss: 0.5137, Avg Loss: 0.0267\n",
            "Batch [960/1563] - Loss: 0.2768, Avg Loss: 0.0267\n",
            "Batch [970/1563] - Loss: 0.4283, Avg Loss: 0.0266\n",
            "Batch [980/1563] - Loss: 0.3204, Avg Loss: 0.0266\n",
            "Batch [990/1563] - Loss: 0.3893, Avg Loss: 0.0266\n",
            "Batch [1000/1563] - Loss: 0.3467, Avg Loss: 0.0266\n",
            "Batch [1010/1563] - Loss: 0.3139, Avg Loss: 0.0266\n",
            "Batch [1020/1563] - Loss: 0.5766, Avg Loss: 0.0266\n",
            "Batch [1030/1563] - Loss: 0.3577, Avg Loss: 0.0266\n",
            "Batch [1040/1563] - Loss: 0.2725, Avg Loss: 0.0265\n",
            "Batch [1050/1563] - Loss: 0.5307, Avg Loss: 0.0265\n",
            "Batch [1060/1563] - Loss: 0.3825, Avg Loss: 0.0265\n",
            "Batch [1070/1563] - Loss: 0.5167, Avg Loss: 0.0265\n",
            "Batch [1080/1563] - Loss: 0.2617, Avg Loss: 0.0264\n",
            "Batch [1090/1563] - Loss: 0.5328, Avg Loss: 0.0264\n",
            "Batch [1100/1563] - Loss: 0.4841, Avg Loss: 0.0264\n",
            "Batch [1110/1563] - Loss: 0.2937, Avg Loss: 0.0264\n",
            "Batch [1120/1563] - Loss: 0.4444, Avg Loss: 0.0264\n",
            "Batch [1130/1563] - Loss: 0.4353, Avg Loss: 0.0264\n",
            "Batch [1140/1563] - Loss: 0.5136, Avg Loss: 0.0263\n",
            "Batch [1150/1563] - Loss: 0.3535, Avg Loss: 0.0263\n",
            "Batch [1160/1563] - Loss: 0.3120, Avg Loss: 0.0263\n",
            "Batch [1170/1563] - Loss: 0.3043, Avg Loss: 0.0263\n",
            "Batch [1180/1563] - Loss: 0.4014, Avg Loss: 0.0263\n",
            "Batch [1190/1563] - Loss: 0.3758, Avg Loss: 0.0263\n",
            "Batch [1200/1563] - Loss: 0.4403, Avg Loss: 0.0262\n",
            "Batch [1210/1563] - Loss: 0.2311, Avg Loss: 0.0262\n",
            "Batch [1220/1563] - Loss: 0.2463, Avg Loss: 0.0262\n",
            "Batch [1230/1563] - Loss: 0.2525, Avg Loss: 0.0262\n",
            "Batch [1240/1563] - Loss: 0.4874, Avg Loss: 0.0262\n",
            "Batch [1250/1563] - Loss: 0.3049, Avg Loss: 0.0262\n",
            "Batch [1260/1563] - Loss: 0.4150, Avg Loss: 0.0262\n",
            "Batch [1270/1563] - Loss: 0.3525, Avg Loss: 0.0262\n",
            "Batch [1280/1563] - Loss: 0.3296, Avg Loss: 0.0262\n",
            "Batch [1290/1563] - Loss: 0.6080, Avg Loss: 0.0262\n",
            "Batch [1300/1563] - Loss: 0.2750, Avg Loss: 0.0261\n",
            "Batch [1310/1563] - Loss: 0.3538, Avg Loss: 0.0261\n",
            "Batch [1320/1563] - Loss: 0.4164, Avg Loss: 0.0261\n",
            "Batch [1330/1563] - Loss: 0.4853, Avg Loss: 0.0261\n",
            "Batch [1340/1563] - Loss: 0.4184, Avg Loss: 0.0261\n",
            "Batch [1350/1563] - Loss: 0.3105, Avg Loss: 0.0260\n",
            "Batch [1360/1563] - Loss: 0.6012, Avg Loss: 0.0261\n",
            "Batch [1370/1563] - Loss: 0.2462, Avg Loss: 0.0261\n",
            "Batch [1380/1563] - Loss: 0.3556, Avg Loss: 0.0260\n",
            "Batch [1390/1563] - Loss: 0.3497, Avg Loss: 0.0260\n",
            "Batch [1400/1563] - Loss: 0.5871, Avg Loss: 0.0260\n",
            "Batch [1410/1563] - Loss: 0.4532, Avg Loss: 0.0260\n",
            "Batch [1420/1563] - Loss: 0.5252, Avg Loss: 0.0261\n",
            "Batch [1430/1563] - Loss: 0.4083, Avg Loss: 0.0260\n",
            "Batch [1440/1563] - Loss: 0.2293, Avg Loss: 0.0260\n",
            "Batch [1450/1563] - Loss: 0.4698, Avg Loss: 0.0260\n",
            "Batch [1460/1563] - Loss: 0.2895, Avg Loss: 0.0260\n",
            "Batch [1470/1563] - Loss: 0.3124, Avg Loss: 0.0260\n",
            "Batch [1480/1563] - Loss: 0.3443, Avg Loss: 0.0260\n",
            "Batch [1490/1563] - Loss: 0.2837, Avg Loss: 0.0260\n",
            "Batch [1500/1563] - Loss: 0.3368, Avg Loss: 0.0259\n",
            "Batch [1510/1563] - Loss: 0.2563, Avg Loss: 0.0259\n",
            "Batch [1520/1563] - Loss: 0.4509, Avg Loss: 0.0259\n",
            "Batch [1530/1563] - Loss: 0.3001, Avg Loss: 0.0259\n",
            "Batch [1540/1563] - Loss: 0.3133, Avg Loss: 0.0259\n",
            "Batch [1550/1563] - Loss: 0.4159, Avg Loss: 0.0259\n",
            "Batch [1560/1563] - Loss: 0.4811, Avg Loss: 0.0259\n",
            "Epoch completed - Total samples: 25000, Average Loss: 0.0259\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:44, 34.38it/s]Eval Batch [20/1563] - Current Accuracy: 0.8281\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.23it/s]Eval Batch [40/1563] - Current Accuracy: 0.8359\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 38.73it/s]Eval Batch [60/1563] - Current Accuracy: 0.8260\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:38, 38.99it/s]Eval Batch [80/1563] - Current Accuracy: 0.8352\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 39.09it/s]Eval Batch [100/1563] - Current Accuracy: 0.8431\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:36, 39.26it/s]Eval Batch [120/1563] - Current Accuracy: 0.8438\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 39.27it/s]Eval Batch [140/1563] - Current Accuracy: 0.8442\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:35, 39.23it/s]Eval Batch [160/1563] - Current Accuracy: 0.8344\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 39.20it/s]Eval Batch [180/1563] - Current Accuracy: 0.8354\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:34, 39.16it/s]Eval Batch [200/1563] - Current Accuracy: 0.8394\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 39.13it/s]Eval Batch [220/1563] - Current Accuracy: 0.8401\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:33, 39.08it/s]Eval Batch [240/1563] - Current Accuracy: 0.8430\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 38.99it/s]Eval Batch [260/1563] - Current Accuracy: 0.8445\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:32, 39.10it/s]Eval Batch [280/1563] - Current Accuracy: 0.8462\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 38.96it/s]Eval Batch [300/1563] - Current Accuracy: 0.8454\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:31, 39.05it/s]Eval Batch [320/1563] - Current Accuracy: 0.8465\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 39.05it/s]Eval Batch [340/1563] - Current Accuracy: 0.8458\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:30, 39.07it/s]Eval Batch [360/1563] - Current Accuracy: 0.8458\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 39.04it/s]Eval Batch [380/1563] - Current Accuracy: 0.8442\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:29, 39.16it/s]Eval Batch [400/1563] - Current Accuracy: 0.8441\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 39.03it/s]Eval Batch [420/1563] - Current Accuracy: 0.8454\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:28, 39.29it/s]Eval Batch [440/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 39.25it/s]Eval Batch [460/1563] - Current Accuracy: 0.8423\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:27, 39.06it/s]Eval Batch [480/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 39.02it/s]Eval Batch [500/1563] - Current Accuracy: 0.8434\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:27, 38.64it/s]Eval Batch [520/1563] - Current Accuracy: 0.8431\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 38.78it/s]Eval Batch [540/1563] - Current Accuracy: 0.8428\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 39.13it/s]Eval Batch [560/1563] - Current Accuracy: 0.8417\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 39.26it/s]Eval Batch [580/1563] - Current Accuracy: 0.8391\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 39.07it/s]Eval Batch [600/1563] - Current Accuracy: 0.8375\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 39.06it/s]Eval Batch [620/1563] - Current Accuracy: 0.8371\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 39.08it/s]Eval Batch [640/1563] - Current Accuracy: 0.8375\n",
            "Evaluating SNN:  42% 657/1563 [00:16<00:23, 39.02it/s]Eval Batch [660/1563] - Current Accuracy: 0.8384\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 38.98it/s]Eval Batch [680/1563] - Current Accuracy: 0.8398\n",
            "Evaluating SNN:  45% 697/1563 [00:17<00:22, 39.01it/s]Eval Batch [700/1563] - Current Accuracy: 0.8396\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 38.77it/s]Eval Batch [720/1563] - Current Accuracy: 0.8388\n",
            "Evaluating SNN:  47% 737/1563 [00:19<00:21, 38.43it/s]Eval Batch [740/1563] - Current Accuracy: 0.8387\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 38.60it/s]Eval Batch [760/1563] - Current Accuracy: 0.8390\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 38.40it/s]Eval Batch [780/1563] - Current Accuracy: 0.8387\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 38.76it/s]Eval Batch [800/1563] - Current Accuracy: 0.8394\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 38.91it/s]Eval Batch [820/1563] - Current Accuracy: 0.8405\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 38.71it/s]Eval Batch [840/1563] - Current Accuracy: 0.8398\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 38.95it/s]Eval Batch [860/1563] - Current Accuracy: 0.8412\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 38.92it/s]Eval Batch [880/1563] - Current Accuracy: 0.8403\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 38.78it/s]Eval Batch [900/1563] - Current Accuracy: 0.8403\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 38.89it/s]Eval Batch [920/1563] - Current Accuracy: 0.8411\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:16, 38.80it/s]Eval Batch [940/1563] - Current Accuracy: 0.8418\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 38.69it/s]Eval Batch [960/1563] - Current Accuracy: 0.8419\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:15, 38.68it/s]Eval Batch [980/1563] - Current Accuracy: 0.8426\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 38.70it/s]Eval Batch [1000/1563] - Current Accuracy: 0.8437\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:14, 38.72it/s]Eval Batch [1020/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 38.89it/s]Eval Batch [1040/1563] - Current Accuracy: 0.8427\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:12, 38.97it/s]Eval Batch [1060/1563] - Current Accuracy: 0.8422\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 38.75it/s]Eval Batch [1080/1563] - Current Accuracy: 0.8413\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:11, 38.87it/s]Eval Batch [1100/1563] - Current Accuracy: 0.8417\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 38.83it/s]Eval Batch [1120/1563] - Current Accuracy: 0.8425\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:11, 38.72it/s]Eval Batch [1140/1563] - Current Accuracy: 0.8420\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 38.93it/s]Eval Batch [1160/1563] - Current Accuracy: 0.8418\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:10, 38.59it/s]Eval Batch [1180/1563] - Current Accuracy: 0.8426\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 38.63it/s]Eval Batch [1200/1563] - Current Accuracy: 0.8427\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 38.86it/s]Eval Batch [1220/1563] - Current Accuracy: 0.8427\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 38.72it/s]Eval Batch [1240/1563] - Current Accuracy: 0.8428\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 38.66it/s]Eval Batch [1260/1563] - Current Accuracy: 0.8430\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 39.17it/s]Eval Batch [1280/1563] - Current Accuracy: 0.8437\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 39.07it/s]Eval Batch [1300/1563] - Current Accuracy: 0.8442\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 38.94it/s]Eval Batch [1320/1563] - Current Accuracy: 0.8441\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 39.15it/s]Eval Batch [1340/1563] - Current Accuracy: 0.8440\n",
            "Evaluating SNN:  87% 1357/1563 [00:34<00:05, 38.95it/s]Eval Batch [1360/1563] - Current Accuracy: 0.8448\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 38.70it/s]Eval Batch [1380/1563] - Current Accuracy: 0.8447\n",
            "Evaluating SNN:  89% 1397/1563 [00:36<00:04, 38.88it/s]Eval Batch [1400/1563] - Current Accuracy: 0.8440\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 38.69it/s]Eval Batch [1420/1563] - Current Accuracy: 0.8441\n",
            "Evaluating SNN:  92% 1437/1563 [00:37<00:03, 38.79it/s]Eval Batch [1440/1563] - Current Accuracy: 0.8442\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 39.09it/s]Eval Batch [1460/1563] - Current Accuracy: 0.8447\n",
            "Evaluating SNN:  94% 1477/1563 [00:38<00:02, 39.08it/s]Eval Batch [1480/1563] - Current Accuracy: 0.8451\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 39.09it/s]Eval Batch [1500/1563] - Current Accuracy: 0.8448\n",
            "Evaluating SNN:  97% 1517/1563 [00:39<00:01, 39.17it/s]Eval Batch [1520/1563] - Current Accuracy: 0.8450\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 38.98it/s]Eval Batch [1540/1563] - Current Accuracy: 0.8450\n",
            "Evaluating SNN: 100% 1557/1563 [00:40<00:00, 39.12it/s]Eval Batch [1560/1563] - Current Accuracy: 0.8443\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.78it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.8442\n",
            "INFO:root:Saved model weights at epoch 3 to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_3.pth\n",
            "[2025-09-24 17:13:49,062][main.py][line:639][INFO] Saved model weights at epoch 3 to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_3.pth\n",
            "INFO:root:Save directory exists: True\n",
            "[2025-09-24 17:13:49,062][main.py][line:640][INFO] Save directory exists: True\n",
            "INFO:root:File size: 265511595 bytes\n",
            "[2025-09-24 17:13:49,062][main.py][line:641][INFO] File size: 265511595 bytes\n",
            "INFO:root:SNNs training Epoch 3 [distilbert_base_qcfs]: Val_loss: 0.025914347901940346\n",
            "[2025-09-24 17:13:49,062][main.py][line:644][INFO] SNNs training Epoch 3 [distilbert_base_qcfs]: Val_loss: 0.025914347901940346\n",
            "INFO:root:SNNs training Epoch 3 [distilbert_base_qcfs]: Test Acc: 0.8442 Best Acc: 0.8442\n",
            "[2025-09-24 17:13:49,063][main.py][line:645][INFO] SNNs training Epoch 3 [distilbert_base_qcfs]: Test Acc: 0.8442 Best Acc: 0.8442\n",
            "Starting training with 1563 batches...\n",
            "Batch [10/1563] - Loss: 0.3407, Avg Loss: 0.0223\n",
            "Batch [20/1563] - Loss: 0.2979, Avg Loss: 0.0227\n",
            "Batch [30/1563] - Loss: 0.5220, Avg Loss: 0.0232\n",
            "Batch [40/1563] - Loss: 0.4249, Avg Loss: 0.0231\n",
            "Batch [50/1563] - Loss: 0.4997, Avg Loss: 0.0243\n",
            "Batch [60/1563] - Loss: 0.2994, Avg Loss: 0.0239\n",
            "Batch [70/1563] - Loss: 0.3533, Avg Loss: 0.0246\n",
            "Batch [80/1563] - Loss: 0.4220, Avg Loss: 0.0245\n",
            "Batch [90/1563] - Loss: 0.3226, Avg Loss: 0.0244\n",
            "Batch [100/1563] - Loss: 0.3199, Avg Loss: 0.0243\n",
            "Batch [110/1563] - Loss: 0.4844, Avg Loss: 0.0243\n",
            "Batch [120/1563] - Loss: 0.3082, Avg Loss: 0.0239\n",
            "Batch [130/1563] - Loss: 0.3415, Avg Loss: 0.0238\n",
            "Batch [140/1563] - Loss: 0.5420, Avg Loss: 0.0237\n",
            "Batch [150/1563] - Loss: 0.5129, Avg Loss: 0.0239\n",
            "Batch [160/1563] - Loss: 0.5072, Avg Loss: 0.0239\n",
            "Batch [170/1563] - Loss: 0.3610, Avg Loss: 0.0239\n",
            "Batch [180/1563] - Loss: 0.2751, Avg Loss: 0.0238\n",
            "Batch [190/1563] - Loss: 0.4133, Avg Loss: 0.0238\n",
            "Batch [200/1563] - Loss: 0.6535, Avg Loss: 0.0239\n",
            "Batch [210/1563] - Loss: 0.4155, Avg Loss: 0.0240\n",
            "Batch [220/1563] - Loss: 0.2513, Avg Loss: 0.0241\n",
            "Batch [230/1563] - Loss: 0.3715, Avg Loss: 0.0240\n",
            "Batch [240/1563] - Loss: 0.6575, Avg Loss: 0.0241\n",
            "Batch [250/1563] - Loss: 0.4505, Avg Loss: 0.0242\n",
            "Batch [260/1563] - Loss: 0.4505, Avg Loss: 0.0242\n",
            "Batch [270/1563] - Loss: 0.3807, Avg Loss: 0.0242\n",
            "Batch [280/1563] - Loss: 0.3148, Avg Loss: 0.0241\n",
            "Batch [290/1563] - Loss: 0.4323, Avg Loss: 0.0241\n",
            "Batch [300/1563] - Loss: 0.2559, Avg Loss: 0.0241\n",
            "Batch [310/1563] - Loss: 0.4100, Avg Loss: 0.0241\n",
            "Batch [320/1563] - Loss: 0.3043, Avg Loss: 0.0241\n",
            "Batch [330/1563] - Loss: 0.2755, Avg Loss: 0.0240\n",
            "Batch [340/1563] - Loss: 0.3133, Avg Loss: 0.0241\n",
            "Batch [350/1563] - Loss: 0.3464, Avg Loss: 0.0241\n",
            "Batch [360/1563] - Loss: 0.4174, Avg Loss: 0.0241\n",
            "Batch [370/1563] - Loss: 0.2882, Avg Loss: 0.0240\n",
            "Batch [380/1563] - Loss: 0.4410, Avg Loss: 0.0240\n",
            "Batch [390/1563] - Loss: 0.2966, Avg Loss: 0.0240\n",
            "Batch [400/1563] - Loss: 0.2876, Avg Loss: 0.0241\n",
            "Batch [410/1563] - Loss: 0.5072, Avg Loss: 0.0241\n",
            "Batch [420/1563] - Loss: 0.3758, Avg Loss: 0.0241\n",
            "Batch [430/1563] - Loss: 0.2245, Avg Loss: 0.0242\n",
            "Batch [440/1563] - Loss: 0.3972, Avg Loss: 0.0241\n",
            "Batch [450/1563] - Loss: 0.5047, Avg Loss: 0.0242\n",
            "Batch [460/1563] - Loss: 0.3249, Avg Loss: 0.0242\n",
            "Batch [470/1563] - Loss: 0.4560, Avg Loss: 0.0242\n",
            "Batch [480/1563] - Loss: 0.4559, Avg Loss: 0.0242\n",
            "Batch [490/1563] - Loss: 0.3570, Avg Loss: 0.0242\n",
            "Batch [500/1563] - Loss: 0.3450, Avg Loss: 0.0241\n",
            "Batch [510/1563] - Loss: 0.4144, Avg Loss: 0.0241\n",
            "Batch [520/1563] - Loss: 0.5205, Avg Loss: 0.0241\n",
            "Batch [530/1563] - Loss: 0.2518, Avg Loss: 0.0241\n",
            "Batch [540/1563] - Loss: 0.6340, Avg Loss: 0.0242\n",
            "Batch [550/1563] - Loss: 0.4704, Avg Loss: 0.0242\n",
            "Batch [560/1563] - Loss: 0.1949, Avg Loss: 0.0242\n",
            "Batch [570/1563] - Loss: 0.2839, Avg Loss: 0.0242\n",
            "Batch [580/1563] - Loss: 0.3947, Avg Loss: 0.0242\n",
            "Batch [590/1563] - Loss: 0.4345, Avg Loss: 0.0242\n",
            "Batch [600/1563] - Loss: 0.3928, Avg Loss: 0.0242\n",
            "Batch [610/1563] - Loss: 0.3690, Avg Loss: 0.0242\n",
            "Batch [620/1563] - Loss: 0.4232, Avg Loss: 0.0242\n",
            "Batch [630/1563] - Loss: 0.4796, Avg Loss: 0.0242\n",
            "Batch [640/1563] - Loss: 0.4680, Avg Loss: 0.0243\n",
            "Batch [650/1563] - Loss: 0.4091, Avg Loss: 0.0243\n",
            "Batch [660/1563] - Loss: 0.4742, Avg Loss: 0.0243\n",
            "Batch [670/1563] - Loss: 0.5659, Avg Loss: 0.0243\n",
            "Batch [680/1563] - Loss: 0.4463, Avg Loss: 0.0243\n",
            "Batch [690/1563] - Loss: 0.3568, Avg Loss: 0.0243\n",
            "Batch [700/1563] - Loss: 0.4095, Avg Loss: 0.0244\n",
            "Batch [710/1563] - Loss: 0.2041, Avg Loss: 0.0243\n",
            "Batch [720/1563] - Loss: 0.4321, Avg Loss: 0.0243\n",
            "Batch [730/1563] - Loss: 0.6009, Avg Loss: 0.0244\n",
            "Batch [740/1563] - Loss: 0.2401, Avg Loss: 0.0243\n",
            "Batch [750/1563] - Loss: 0.4535, Avg Loss: 0.0243\n",
            "Batch [760/1563] - Loss: 0.3036, Avg Loss: 0.0243\n",
            "Batch [770/1563] - Loss: 0.4561, Avg Loss: 0.0243\n",
            "Batch [780/1563] - Loss: 0.4249, Avg Loss: 0.0242\n",
            "Batch [790/1563] - Loss: 0.3127, Avg Loss: 0.0243\n",
            "Batch [800/1563] - Loss: 0.4089, Avg Loss: 0.0242\n",
            "Batch [810/1563] - Loss: 0.5382, Avg Loss: 0.0242\n",
            "Batch [820/1563] - Loss: 0.4368, Avg Loss: 0.0243\n",
            "Batch [830/1563] - Loss: 0.3901, Avg Loss: 0.0243\n",
            "Batch [840/1563] - Loss: 0.5166, Avg Loss: 0.0243\n",
            "Batch [850/1563] - Loss: 0.3586, Avg Loss: 0.0243\n",
            "Batch [860/1563] - Loss: 0.3964, Avg Loss: 0.0243\n",
            "Batch [870/1563] - Loss: 0.3413, Avg Loss: 0.0243\n",
            "Batch [880/1563] - Loss: 0.2761, Avg Loss: 0.0243\n",
            "Batch [890/1563] - Loss: 0.4711, Avg Loss: 0.0243\n",
            "Batch [900/1563] - Loss: 0.3215, Avg Loss: 0.0243\n",
            "Batch [910/1563] - Loss: 0.2783, Avg Loss: 0.0243\n",
            "Batch [920/1563] - Loss: 0.3171, Avg Loss: 0.0243\n",
            "Batch [930/1563] - Loss: 0.3477, Avg Loss: 0.0243\n",
            "Batch [940/1563] - Loss: 0.5825, Avg Loss: 0.0243\n",
            "Batch [950/1563] - Loss: 0.3769, Avg Loss: 0.0243\n",
            "Batch [960/1563] - Loss: 0.4846, Avg Loss: 0.0243\n",
            "Batch [970/1563] - Loss: 0.2245, Avg Loss: 0.0243\n",
            "Batch [980/1563] - Loss: 0.3507, Avg Loss: 0.0243\n",
            "Batch [990/1563] - Loss: 0.3131, Avg Loss: 0.0243\n",
            "Batch [1000/1563] - Loss: 0.4192, Avg Loss: 0.0243\n",
            "Batch [1010/1563] - Loss: 0.3541, Avg Loss: 0.0243\n",
            "Batch [1020/1563] - Loss: 0.3151, Avg Loss: 0.0243\n",
            "Batch [1030/1563] - Loss: 0.3196, Avg Loss: 0.0243\n",
            "Batch [1040/1563] - Loss: 0.2184, Avg Loss: 0.0243\n",
            "Batch [1050/1563] - Loss: 0.3987, Avg Loss: 0.0243\n",
            "Batch [1060/1563] - Loss: 0.1999, Avg Loss: 0.0243\n",
            "Batch [1070/1563] - Loss: 0.3533, Avg Loss: 0.0243\n",
            "Batch [1080/1563] - Loss: 0.3946, Avg Loss: 0.0243\n",
            "Batch [1090/1563] - Loss: 0.3160, Avg Loss: 0.0243\n",
            "Batch [1100/1563] - Loss: 0.3067, Avg Loss: 0.0243\n",
            "Batch [1110/1563] - Loss: 0.3597, Avg Loss: 0.0243\n",
            "Batch [1120/1563] - Loss: 0.4935, Avg Loss: 0.0243\n",
            "Batch [1130/1563] - Loss: 0.4479, Avg Loss: 0.0242\n",
            "Batch [1140/1563] - Loss: 0.5533, Avg Loss: 0.0242\n",
            "Batch [1150/1563] - Loss: 0.3770, Avg Loss: 0.0242\n",
            "Batch [1160/1563] - Loss: 0.3236, Avg Loss: 0.0242\n",
            "Batch [1170/1563] - Loss: 0.2643, Avg Loss: 0.0242\n",
            "Batch [1180/1563] - Loss: 0.5342, Avg Loss: 0.0242\n",
            "Batch [1190/1563] - Loss: 0.4358, Avg Loss: 0.0242\n",
            "Batch [1200/1563] - Loss: 0.3201, Avg Loss: 0.0242\n",
            "Batch [1210/1563] - Loss: 0.3075, Avg Loss: 0.0242\n",
            "Batch [1220/1563] - Loss: 0.2568, Avg Loss: 0.0242\n",
            "Batch [1230/1563] - Loss: 0.2342, Avg Loss: 0.0241\n",
            "Batch [1240/1563] - Loss: 0.3789, Avg Loss: 0.0242\n",
            "Batch [1250/1563] - Loss: 0.4253, Avg Loss: 0.0242\n",
            "Batch [1260/1563] - Loss: 0.4360, Avg Loss: 0.0242\n",
            "Batch [1270/1563] - Loss: 0.3488, Avg Loss: 0.0241\n",
            "Batch [1280/1563] - Loss: 0.2883, Avg Loss: 0.0241\n",
            "Batch [1290/1563] - Loss: 0.5812, Avg Loss: 0.0241\n",
            "Batch [1300/1563] - Loss: 0.2427, Avg Loss: 0.0241\n",
            "Batch [1310/1563] - Loss: 0.3027, Avg Loss: 0.0241\n",
            "Batch [1320/1563] - Loss: 0.4042, Avg Loss: 0.0241\n",
            "Batch [1330/1563] - Loss: 0.2114, Avg Loss: 0.0241\n",
            "Batch [1340/1563] - Loss: 0.2333, Avg Loss: 0.0241\n",
            "Batch [1350/1563] - Loss: 0.5497, Avg Loss: 0.0241\n",
            "Batch [1360/1563] - Loss: 0.3573, Avg Loss: 0.0241\n",
            "Batch [1370/1563] - Loss: 0.4309, Avg Loss: 0.0241\n",
            "Batch [1380/1563] - Loss: 0.3133, Avg Loss: 0.0241\n",
            "Batch [1390/1563] - Loss: 0.3742, Avg Loss: 0.0241\n",
            "Batch [1400/1563] - Loss: 0.4116, Avg Loss: 0.0241\n",
            "Batch [1410/1563] - Loss: 0.3679, Avg Loss: 0.0241\n",
            "Batch [1420/1563] - Loss: 0.7812, Avg Loss: 0.0241\n",
            "Batch [1430/1563] - Loss: 0.6656, Avg Loss: 0.0241\n",
            "Batch [1440/1563] - Loss: 0.3614, Avg Loss: 0.0241\n",
            "Batch [1450/1563] - Loss: 0.4103, Avg Loss: 0.0241\n",
            "Batch [1460/1563] - Loss: 0.4713, Avg Loss: 0.0241\n",
            "Batch [1470/1563] - Loss: 0.2510, Avg Loss: 0.0241\n",
            "Batch [1480/1563] - Loss: 0.7089, Avg Loss: 0.0241\n",
            "Batch [1490/1563] - Loss: 0.4950, Avg Loss: 0.0241\n",
            "Batch [1500/1563] - Loss: 0.2545, Avg Loss: 0.0241\n",
            "Batch [1510/1563] - Loss: 0.4847, Avg Loss: 0.0241\n",
            "Batch [1520/1563] - Loss: 0.2442, Avg Loss: 0.0240\n",
            "Batch [1530/1563] - Loss: 0.4300, Avg Loss: 0.0240\n",
            "Batch [1540/1563] - Loss: 0.2845, Avg Loss: 0.0240\n",
            "Batch [1550/1563] - Loss: 0.3725, Avg Loss: 0.0241\n",
            "Batch [1560/1563] - Loss: 0.6107, Avg Loss: 0.0241\n",
            "Epoch completed - Total samples: 25000, Average Loss: 0.0241\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:44, 35.09it/s]Eval Batch [20/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.26it/s]Eval Batch [40/1563] - Current Accuracy: 0.8594\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 38.84it/s]Eval Batch [60/1563] - Current Accuracy: 0.8448\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:38, 39.05it/s]Eval Batch [80/1563] - Current Accuracy: 0.8516\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 39.07it/s]Eval Batch [100/1563] - Current Accuracy: 0.8581\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:37, 38.99it/s]Eval Batch [120/1563] - Current Accuracy: 0.8599\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 38.90it/s]Eval Batch [140/1563] - Current Accuracy: 0.8603\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:36, 39.00it/s]Eval Batch [160/1563] - Current Accuracy: 0.8516\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 38.95it/s]Eval Batch [180/1563] - Current Accuracy: 0.8517\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:35, 38.97it/s]Eval Batch [200/1563] - Current Accuracy: 0.8550\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 39.18it/s]Eval Batch [220/1563] - Current Accuracy: 0.8551\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:33, 39.10it/s]Eval Batch [240/1563] - Current Accuracy: 0.8576\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 39.03it/s]Eval Batch [260/1563] - Current Accuracy: 0.8596\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:32, 39.19it/s]Eval Batch [280/1563] - Current Accuracy: 0.8612\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 39.12it/s]Eval Batch [300/1563] - Current Accuracy: 0.8598\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:32, 38.85it/s]Eval Batch [320/1563] - Current Accuracy: 0.8607\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 38.95it/s]Eval Batch [340/1563] - Current Accuracy: 0.8608\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:31, 38.84it/s]Eval Batch [360/1563] - Current Accuracy: 0.8611\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 38.86it/s]Eval Batch [380/1563] - Current Accuracy: 0.8597\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:30, 38.87it/s]Eval Batch [400/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 38.73it/s]Eval Batch [420/1563] - Current Accuracy: 0.8607\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:28, 38.84it/s]Eval Batch [440/1563] - Current Accuracy: 0.8594\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 38.94it/s]Eval Batch [460/1563] - Current Accuracy: 0.8584\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:27, 38.91it/s]Eval Batch [480/1563] - Current Accuracy: 0.8591\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 38.86it/s]Eval Batch [500/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 38.87it/s]Eval Batch [520/1563] - Current Accuracy: 0.8599\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 38.70it/s]Eval Batch [540/1563] - Current Accuracy: 0.8593\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 38.79it/s]Eval Batch [560/1563] - Current Accuracy: 0.8583\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 38.81it/s]Eval Batch [580/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 38.76it/s]Eval Batch [600/1563] - Current Accuracy: 0.8553\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 38.85it/s]Eval Batch [620/1563] - Current Accuracy: 0.8552\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 38.85it/s]Eval Batch [640/1563] - Current Accuracy: 0.8556\n",
            "Evaluating SNN:  42% 657/1563 [00:17<00:23, 38.68it/s]Eval Batch [660/1563] - Current Accuracy: 0.8566\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 38.94it/s]Eval Batch [680/1563] - Current Accuracy: 0.8579\n",
            "Evaluating SNN:  45% 697/1563 [00:18<00:22, 38.75it/s]Eval Batch [700/1563] - Current Accuracy: 0.8578\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 38.77it/s]Eval Batch [720/1563] - Current Accuracy: 0.8570\n",
            "Evaluating SNN:  47% 737/1563 [00:19<00:21, 38.69it/s]Eval Batch [740/1563] - Current Accuracy: 0.8568\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 38.70it/s]Eval Batch [760/1563] - Current Accuracy: 0.8571\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 38.84it/s]Eval Batch [780/1563] - Current Accuracy: 0.8567\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 39.11it/s]Eval Batch [800/1563] - Current Accuracy: 0.8567\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 38.84it/s]Eval Batch [820/1563] - Current Accuracy: 0.8572\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 38.99it/s]Eval Batch [840/1563] - Current Accuracy: 0.8554\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 38.92it/s]Eval Batch [860/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 38.85it/s]Eval Batch [880/1563] - Current Accuracy: 0.8548\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 38.80it/s]Eval Batch [900/1563] - Current Accuracy: 0.8542\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 38.98it/s]Eval Batch [920/1563] - Current Accuracy: 0.8545\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:16, 38.89it/s]Eval Batch [940/1563] - Current Accuracy: 0.8545\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 38.90it/s]Eval Batch [960/1563] - Current Accuracy: 0.8540\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:15, 38.88it/s]Eval Batch [980/1563] - Current Accuracy: 0.8541\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 38.83it/s]Eval Batch [1000/1563] - Current Accuracy: 0.8548\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:14, 38.94it/s]Eval Batch [1020/1563] - Current Accuracy: 0.8536\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 38.71it/s]Eval Batch [1040/1563] - Current Accuracy: 0.8526\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:13, 38.49it/s]Eval Batch [1060/1563] - Current Accuracy: 0.8519\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 38.75it/s]Eval Batch [1080/1563] - Current Accuracy: 0.8505\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:12, 38.55it/s]Eval Batch [1100/1563] - Current Accuracy: 0.8506\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 38.82it/s]Eval Batch [1120/1563] - Current Accuracy: 0.8512\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 38.97it/s]Eval Batch [1140/1563] - Current Accuracy: 0.8503\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 39.08it/s]Eval Batch [1160/1563] - Current Accuracy: 0.8495\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 38.86it/s]Eval Batch [1180/1563] - Current Accuracy: 0.8498\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 39.08it/s]Eval Batch [1200/1563] - Current Accuracy: 0.8498\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 39.06it/s]Eval Batch [1220/1563] - Current Accuracy: 0.8494\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 39.05it/s]Eval Batch [1240/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 38.90it/s]Eval Batch [1260/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 38.95it/s]Eval Batch [1280/1563] - Current Accuracy: 0.8497\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 38.50it/s]Eval Batch [1300/1563] - Current Accuracy: 0.8500\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 38.53it/s]Eval Batch [1320/1563] - Current Accuracy: 0.8496\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 38.93it/s]Eval Batch [1340/1563] - Current Accuracy: 0.8493\n",
            "Evaluating SNN:  87% 1357/1563 [00:35<00:05, 38.87it/s]Eval Batch [1360/1563] - Current Accuracy: 0.8499\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 39.04it/s]Eval Batch [1380/1563] - Current Accuracy: 0.8497\n",
            "Evaluating SNN:  89% 1397/1563 [00:36<00:04, 38.86it/s]Eval Batch [1400/1563] - Current Accuracy: 0.8488\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 38.89it/s]Eval Batch [1420/1563] - Current Accuracy: 0.8486\n",
            "Evaluating SNN:  92% 1437/1563 [00:37<00:03, 38.86it/s]Eval Batch [1440/1563] - Current Accuracy: 0.8484\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 38.81it/s]Eval Batch [1460/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  94% 1477/1563 [00:38<00:02, 38.80it/s]Eval Batch [1480/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 39.08it/s]Eval Batch [1500/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  97% 1517/1563 [00:39<00:01, 39.06it/s]Eval Batch [1520/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 38.95it/s]Eval Batch [1540/1563] - Current Accuracy: 0.8486\n",
            "Evaluating SNN: 100% 1557/1563 [00:40<00:00, 39.11it/s]Eval Batch [1560/1563] - Current Accuracy: 0.8478\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.75it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.8476\n",
            "INFO:root:Saved model weights at epoch 4 to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_4.pth\n",
            "[2025-09-24 17:16:33,501][main.py][line:639][INFO] Saved model weights at epoch 4 to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_4.pth\n",
            "INFO:root:Save directory exists: True\n",
            "[2025-09-24 17:16:33,501][main.py][line:640][INFO] Save directory exists: True\n",
            "INFO:root:File size: 265511595 bytes\n",
            "[2025-09-24 17:16:33,501][main.py][line:641][INFO] File size: 265511595 bytes\n",
            "INFO:root:SNNs training Epoch 4 [distilbert_base_qcfs]: Val_loss: 0.02406073850274086\n",
            "[2025-09-24 17:16:33,502][main.py][line:644][INFO] SNNs training Epoch 4 [distilbert_base_qcfs]: Val_loss: 0.02406073850274086\n",
            "INFO:root:SNNs training Epoch 4 [distilbert_base_qcfs]: Test Acc: 0.8476 Best Acc: 0.8476\n",
            "[2025-09-24 17:16:33,502][main.py][line:645][INFO] SNNs training Epoch 4 [distilbert_base_qcfs]: Test Acc: 0.8476 Best Acc: 0.8476\n",
            "INFO:root:Starting efficiency measurement...\n",
            "[2025-09-24 17:16:33,502][main.py][line:655][INFO] Starting efficiency measurement...\n",
            "Installing required packages for efficiency calculation...\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore) (4.15.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=eed7ff7b6bfe85ac46f56ce90c31b8ab12cf9d15d788661f874425cfd9773623\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=c77c9844d3d436ade47d3b11ba9023ae1955f330b464adaafa43efb696eb42c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n",
            "FLOPs calculation failed: 'dict' object has no attribute 'size'\n",
            "INFO:root:============================================================\n",
            "[2025-09-24 17:16:44,675][main.py][line:159][INFO] ============================================================\n",
            "INFO:root:MODEL EFFICIENCY ANALYSIS\n",
            "[2025-09-24 17:16:44,676][main.py][line:160][INFO] MODEL EFFICIENCY ANALYSIS\n",
            "INFO:root:============================================================\n",
            "[2025-09-24 17:16:44,676][main.py][line:161][INFO] ============================================================\n",
            "INFO:root:GPU Type: A100\n",
            "[2025-09-24 17:16:44,676][main.py][line:162][INFO] GPU Type: A100\n",
            "INFO:root:FLOPs: 0.00 GFLOPs\n",
            "[2025-09-24 17:16:44,676][main.py][line:163][INFO] FLOPs: 0.00 GFLOPs\n",
            "INFO:root:Parameters: 66,365,956 total, 66,365,956 trainable\n",
            "[2025-09-24 17:16:44,676][main.py][line:164][INFO] Parameters: 66,365,956 total, 66,365,956 trainable\n",
            "INFO:root:Memory Usage: 1820.21 MB peak, 0.00 MB delta\n",
            "[2025-09-24 17:16:44,676][main.py][line:165][INFO] Memory Usage: 1820.21 MB peak, 0.00 MB delta\n",
            "INFO:root:Energy Estimate: 0.000000 Joules per inference\n",
            "[2025-09-24 17:16:44,676][main.py][line:166][INFO] Energy Estimate: 0.000000 Joules per inference\n",
            "INFO:root:Power Estimate: 0.000 Watts\n",
            "[2025-09-24 17:16:44,676][main.py][line:167][INFO] Power Estimate: 0.000 Watts\n",
            "INFO:root:FLOPs per Parameter: 0.00\n",
            "[2025-09-24 17:16:44,676][main.py][line:168][INFO] FLOPs per Parameter: 0.00\n",
            "INFO:root:Memory Intensity: 0.00 FLOPs/byte\n",
            "[2025-09-24 17:16:44,676][main.py][line:169][INFO] Memory Intensity: 0.00 FLOPs/byte\n",
            "INFO:root:Theoretical Peak: 19.5 TFLOPS\n",
            "[2025-09-24 17:16:44,676][main.py][line:170][INFO] Theoretical Peak: 19.5 TFLOPS\n",
            "INFO:root:Efficiency: 0.00% of theoretical peak\n",
            "[2025-09-24 17:16:44,676][main.py][line:171][INFO] Efficiency: 0.00% of theoretical peak\n",
            "INFO:root:============================================================\n",
            "[2025-09-24 17:16:44,676][main.py][line:172][INFO] ============================================================\n",
            "INFO:root:Efficiency results saved to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_efficiency.txt\n",
            "[2025-09-24 17:16:44,677][main.py][line:689][INFO] Efficiency results saved to: ./checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_efficiency.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load checkpoint and run calibration\n",
        "!python Parallel_Conversion/main.py \\\n",
        "    --dataset TextCLS \\\n",
        "    --net_arch distilbert_base_qcfs \\\n",
        "    --neuron_type ParaInfNeuron \\\n",
        "    --text_dataset imdb \\\n",
        "    --text_max_len 256 \\\n",
        "    --time_step 4 \\\n",
        "    --batchsize 16 \\\n",
        "    --measure_efficiency \\\n",
        "    --gpu_type A100 \\\n",
        "    --dev 0 \\\n",
        "    --calibrate_th \\\n",
        "    --direct_inference \\\n",
        "    --pretrained_model \\\n",
        "    --checkpoint_path /content/checkpointsTextCLS-distilbert_base_qcfs-T2/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_4.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHWmQthCqbus",
        "outputId": "4f4f6d3e-653d-45bc-fe6c-5ac277d3b640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-24 17:16:52.536329: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-09-24 17:16:52.553935: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758734212.575323    9341 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758734212.581769    9341 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758734212.597994    9341 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758734212.598020    9341 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758734212.598023    9341 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758734212.598026    9341 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-24 17:16:52.602977: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Not using distributed mode\n",
            "total parameters: 66.365956 M.\n",
            "INFO:root:Calibrate Inference (Text) [distilbert_base_qcfs]: Test Acc: 0.8494\n",
            "[2025-09-24 17:18:22,534][main.py][line:507][INFO] Calibrate Inference (Text) [distilbert_base_qcfs]: Test Acc: 0.8494\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:43, 35.14it/s]Eval Batch [20/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:40, 38.10it/s]Eval Batch [40/1563] - Current Accuracy: 0.8594\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:39, 38.61it/s]Eval Batch [60/1563] - Current Accuracy: 0.8448\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:38, 38.54it/s]Eval Batch [80/1563] - Current Accuracy: 0.8516\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 38.85it/s]Eval Batch [100/1563] - Current Accuracy: 0.8581\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:37, 39.03it/s]Eval Batch [120/1563] - Current Accuracy: 0.8599\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 38.85it/s]Eval Batch [140/1563] - Current Accuracy: 0.8603\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:36, 38.70it/s]Eval Batch [160/1563] - Current Accuracy: 0.8516\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 38.65it/s]Eval Batch [180/1563] - Current Accuracy: 0.8517\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:35, 38.39it/s]Eval Batch [200/1563] - Current Accuracy: 0.8550\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 38.75it/s]Eval Batch [220/1563] - Current Accuracy: 0.8551\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:34, 38.92it/s]Eval Batch [240/1563] - Current Accuracy: 0.8576\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 38.62it/s]Eval Batch [260/1563] - Current Accuracy: 0.8596\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:33, 38.91it/s]Eval Batch [280/1563] - Current Accuracy: 0.8612\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 38.87it/s]Eval Batch [300/1563] - Current Accuracy: 0.8598\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:32, 38.75it/s]Eval Batch [320/1563] - Current Accuracy: 0.8607\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 39.03it/s]Eval Batch [340/1563] - Current Accuracy: 0.8608\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:31, 38.85it/s]Eval Batch [360/1563] - Current Accuracy: 0.8611\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 38.79it/s]Eval Batch [380/1563] - Current Accuracy: 0.8597\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:29, 38.89it/s]Eval Batch [400/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 38.77it/s]Eval Batch [420/1563] - Current Accuracy: 0.8607\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:29, 38.73it/s]Eval Batch [440/1563] - Current Accuracy: 0.8594\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 38.90it/s]Eval Batch [460/1563] - Current Accuracy: 0.8584\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:27, 38.83it/s]Eval Batch [480/1563] - Current Accuracy: 0.8591\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 38.62it/s]Eval Batch [500/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 38.95it/s]Eval Batch [520/1563] - Current Accuracy: 0.8599\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 38.95it/s]Eval Batch [540/1563] - Current Accuracy: 0.8593\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 38.93it/s]Eval Batch [560/1563] - Current Accuracy: 0.8583\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 39.08it/s]Eval Batch [580/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 38.98it/s]Eval Batch [600/1563] - Current Accuracy: 0.8553\n",
            "Evaluating SNN:  39% 617/1563 [00:16<00:24, 39.00it/s]Eval Batch [620/1563] - Current Accuracy: 0.8552\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 39.13it/s]Eval Batch [640/1563] - Current Accuracy: 0.8556\n",
            "Evaluating SNN:  42% 657/1563 [00:17<00:23, 39.05it/s]Eval Batch [660/1563] - Current Accuracy: 0.8566\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 38.92it/s]Eval Batch [680/1563] - Current Accuracy: 0.8579\n",
            "Evaluating SNN:  45% 697/1563 [00:18<00:22, 39.06it/s]Eval Batch [700/1563] - Current Accuracy: 0.8578\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 38.54it/s]Eval Batch [720/1563] - Current Accuracy: 0.8570\n",
            "Evaluating SNN:  47% 737/1563 [00:19<00:21, 38.33it/s]Eval Batch [740/1563] - Current Accuracy: 0.8568\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 38.60it/s]Eval Batch [760/1563] - Current Accuracy: 0.8571\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 38.78it/s]Eval Batch [780/1563] - Current Accuracy: 0.8567\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 38.65it/s]Eval Batch [800/1563] - Current Accuracy: 0.8567\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 38.88it/s]Eval Batch [820/1563] - Current Accuracy: 0.8572\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 38.90it/s]Eval Batch [840/1563] - Current Accuracy: 0.8554\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 38.95it/s]Eval Batch [860/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 39.04it/s]Eval Batch [880/1563] - Current Accuracy: 0.8548\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 38.98it/s]Eval Batch [900/1563] - Current Accuracy: 0.8542\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 38.94it/s]Eval Batch [920/1563] - Current Accuracy: 0.8545\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:16, 38.99it/s]Eval Batch [940/1563] - Current Accuracy: 0.8545\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 38.92it/s]Eval Batch [960/1563] - Current Accuracy: 0.8540\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:15, 39.01it/s]Eval Batch [980/1563] - Current Accuracy: 0.8541\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 39.05it/s]Eval Batch [1000/1563] - Current Accuracy: 0.8548\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:14, 38.85it/s]Eval Batch [1020/1563] - Current Accuracy: 0.8536\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 38.85it/s]Eval Batch [1040/1563] - Current Accuracy: 0.8526\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:12, 39.01it/s]Eval Batch [1060/1563] - Current Accuracy: 0.8519\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 39.03it/s]Eval Batch [1080/1563] - Current Accuracy: 0.8505\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:11, 39.25it/s]Eval Batch [1100/1563] - Current Accuracy: 0.8506\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 39.03it/s]Eval Batch [1120/1563] - Current Accuracy: 0.8512\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 38.78it/s]Eval Batch [1140/1563] - Current Accuracy: 0.8503\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 38.89it/s]Eval Batch [1160/1563] - Current Accuracy: 0.8495\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 38.82it/s]Eval Batch [1180/1563] - Current Accuracy: 0.8498\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 38.59it/s]Eval Batch [1200/1563] - Current Accuracy: 0.8498\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 38.67it/s]Eval Batch [1220/1563] - Current Accuracy: 0.8494\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 38.83it/s]Eval Batch [1240/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 38.78it/s]Eval Batch [1260/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 38.90it/s]Eval Batch [1280/1563] - Current Accuracy: 0.8497\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 38.77it/s]Eval Batch [1300/1563] - Current Accuracy: 0.8500\n",
            "Evaluating SNN:  84% 1317/1563 [00:34<00:06, 38.97it/s]Eval Batch [1320/1563] - Current Accuracy: 0.8496\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 39.03it/s]Eval Batch [1340/1563] - Current Accuracy: 0.8493\n",
            "Evaluating SNN:  87% 1357/1563 [00:35<00:05, 38.99it/s]Eval Batch [1360/1563] - Current Accuracy: 0.8499\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 39.02it/s]Eval Batch [1380/1563] - Current Accuracy: 0.8497\n",
            "Evaluating SNN:  89% 1397/1563 [00:36<00:04, 39.11it/s]Eval Batch [1400/1563] - Current Accuracy: 0.8488\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 39.04it/s]Eval Batch [1420/1563] - Current Accuracy: 0.8486\n",
            "Evaluating SNN:  92% 1437/1563 [00:37<00:03, 38.87it/s]Eval Batch [1440/1563] - Current Accuracy: 0.8484\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 39.06it/s]Eval Batch [1460/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  94% 1477/1563 [00:38<00:02, 38.96it/s]Eval Batch [1480/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 38.90it/s]Eval Batch [1500/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  97% 1517/1563 [00:39<00:01, 38.90it/s]Eval Batch [1520/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 38.84it/s]Eval Batch [1540/1563] - Current Accuracy: 0.8486\n",
            "Evaluating SNN: 100% 1557/1563 [00:40<00:00, 39.02it/s]Eval Batch [1560/1563] - Current Accuracy: 0.8478\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.73it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.8476\n",
            "INFO:root:Standard BERT Inference [distilbert_base_qcfs]: Test Acc: 0.8476\n",
            "[2025-09-24 17:19:02,894][main.py][line:522][INFO] Standard BERT Inference [distilbert_base_qcfs]: Test Acc: 0.8476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 timestep then calib"
      ],
      "metadata": {
        "id": "gvRxJwHKqLr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python Parallel_Conversion/main.py \\\n",
        "      --dataset TextCLS \\\n",
        "      --net_arch distilbert_base_qcfs \\\n",
        "      --savedir ./checkpoints \\\n",
        "      --neuron_type ParaInfNeuron \\\n",
        "      --text_dataset imdb \\\n",
        "      --text_max_len 256 \\\n",
        "      --time_step 4 \\\n",
        "      --trainsnn_epochs 5 \\\n",
        "      --batchsize 16 \\\n",
        "      --lr 0.00001 \\\n",
        "      --dev 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJG8q6a3HSjW",
        "outputId": "a7303b45-3abc-45eb-cbe4-9da1c4688ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-24 17:50:40.564232: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-09-24 17:50:40.582265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758736240.604088   17805 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758736240.610780   17805 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758736240.627349   17805 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758736240.627378   17805 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758736240.627381   17805 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758736240.627383   17805 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-24 17:50:40.632144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Not using distributed mode\n",
            "total parameters: 66.365956 M.\n",
            "Starting training with 1563 batches...\n",
            "Batch [10/1563] - Loss: 1.1953, Avg Loss: 0.0733\n",
            "Batch [20/1563] - Loss: 1.1247, Avg Loss: 0.0724\n",
            "Batch [30/1563] - Loss: 1.1553, Avg Loss: 0.0718\n",
            "Batch [40/1563] - Loss: 1.0852, Avg Loss: 0.0711\n",
            "Batch [50/1563] - Loss: 1.0976, Avg Loss: 0.0704\n",
            "Batch [60/1563] - Loss: 1.0528, Avg Loss: 0.0698\n",
            "Batch [70/1563] - Loss: 1.0283, Avg Loss: 0.0692\n",
            "Batch [80/1563] - Loss: 1.0305, Avg Loss: 0.0685\n",
            "Batch [90/1563] - Loss: 1.0056, Avg Loss: 0.0680\n",
            "Batch [100/1563] - Loss: 0.9871, Avg Loss: 0.0675\n",
            "Batch [110/1563] - Loss: 0.9641, Avg Loss: 0.0670\n",
            "Batch [120/1563] - Loss: 0.9746, Avg Loss: 0.0664\n",
            "Batch [130/1563] - Loss: 0.9589, Avg Loss: 0.0659\n",
            "Batch [140/1563] - Loss: 0.9233, Avg Loss: 0.0654\n",
            "Batch [150/1563] - Loss: 0.9192, Avg Loss: 0.0650\n",
            "Batch [160/1563] - Loss: 0.9331, Avg Loss: 0.0646\n",
            "Batch [170/1563] - Loss: 0.9326, Avg Loss: 0.0641\n",
            "Batch [180/1563] - Loss: 0.8846, Avg Loss: 0.0637\n",
            "Batch [190/1563] - Loss: 0.8814, Avg Loss: 0.0633\n",
            "Batch [200/1563] - Loss: 0.8971, Avg Loss: 0.0630\n",
            "Batch [210/1563] - Loss: 0.8853, Avg Loss: 0.0626\n",
            "Batch [220/1563] - Loss: 0.8647, Avg Loss: 0.0623\n",
            "Batch [230/1563] - Loss: 0.8695, Avg Loss: 0.0620\n",
            "Batch [240/1563] - Loss: 0.8483, Avg Loss: 0.0617\n",
            "Batch [250/1563] - Loss: 0.8824, Avg Loss: 0.0614\n",
            "Batch [260/1563] - Loss: 0.8805, Avg Loss: 0.0611\n",
            "Batch [270/1563] - Loss: 0.8452, Avg Loss: 0.0609\n",
            "Batch [280/1563] - Loss: 0.8403, Avg Loss: 0.0606\n",
            "Batch [290/1563] - Loss: 0.8822, Avg Loss: 0.0603\n",
            "Batch [300/1563] - Loss: 0.8416, Avg Loss: 0.0601\n",
            "Batch [310/1563] - Loss: 0.8311, Avg Loss: 0.0598\n",
            "Batch [320/1563] - Loss: 0.8380, Avg Loss: 0.0596\n",
            "Batch [330/1563] - Loss: 0.8462, Avg Loss: 0.0594\n",
            "Batch [340/1563] - Loss: 0.8226, Avg Loss: 0.0592\n",
            "Batch [350/1563] - Loss: 0.8505, Avg Loss: 0.0590\n",
            "Batch [360/1563] - Loss: 0.8248, Avg Loss: 0.0587\n",
            "Batch [370/1563] - Loss: 0.8231, Avg Loss: 0.0585\n",
            "Batch [380/1563] - Loss: 0.8094, Avg Loss: 0.0583\n",
            "Batch [390/1563] - Loss: 0.8310, Avg Loss: 0.0582\n",
            "Batch [400/1563] - Loss: 0.8166, Avg Loss: 0.0580\n",
            "Batch [410/1563] - Loss: 0.8066, Avg Loss: 0.0578\n",
            "Batch [420/1563] - Loss: 0.8033, Avg Loss: 0.0576\n",
            "Batch [430/1563] - Loss: 0.8156, Avg Loss: 0.0574\n",
            "Batch [440/1563] - Loss: 0.8183, Avg Loss: 0.0573\n",
            "Batch [450/1563] - Loss: 0.7961, Avg Loss: 0.0571\n",
            "Batch [460/1563] - Loss: 0.7885, Avg Loss: 0.0569\n",
            "Batch [470/1563] - Loss: 0.7884, Avg Loss: 0.0568\n",
            "Batch [480/1563] - Loss: 0.7729, Avg Loss: 0.0566\n",
            "Batch [490/1563] - Loss: 0.7817, Avg Loss: 0.0565\n",
            "Batch [500/1563] - Loss: 0.7904, Avg Loss: 0.0563\n",
            "Batch [510/1563] - Loss: 0.7828, Avg Loss: 0.0562\n",
            "Batch [520/1563] - Loss: 0.7942, Avg Loss: 0.0561\n",
            "Batch [530/1563] - Loss: 0.8050, Avg Loss: 0.0559\n",
            "Batch [540/1563] - Loss: 0.7572, Avg Loss: 0.0558\n",
            "Batch [550/1563] - Loss: 0.7673, Avg Loss: 0.0557\n",
            "Batch [560/1563] - Loss: 0.8070, Avg Loss: 0.0555\n",
            "Batch [570/1563] - Loss: 0.7671, Avg Loss: 0.0554\n",
            "Batch [580/1563] - Loss: 0.7571, Avg Loss: 0.0553\n",
            "Batch [590/1563] - Loss: 0.7726, Avg Loss: 0.0552\n",
            "Batch [600/1563] - Loss: 0.7742, Avg Loss: 0.0551\n",
            "Batch [610/1563] - Loss: 0.7769, Avg Loss: 0.0549\n",
            "Batch [620/1563] - Loss: 0.7539, Avg Loss: 0.0548\n",
            "Batch [630/1563] - Loss: 0.7885, Avg Loss: 0.0547\n",
            "Batch [640/1563] - Loss: 0.7495, Avg Loss: 0.0546\n",
            "Batch [650/1563] - Loss: 0.7640, Avg Loss: 0.0545\n",
            "Batch [660/1563] - Loss: 0.7585, Avg Loss: 0.0544\n",
            "Batch [670/1563] - Loss: 0.7541, Avg Loss: 0.0543\n",
            "Batch [680/1563] - Loss: 0.7618, Avg Loss: 0.0542\n",
            "Batch [690/1563] - Loss: 0.7328, Avg Loss: 0.0541\n",
            "Batch [700/1563] - Loss: 0.7633, Avg Loss: 0.0540\n",
            "Batch [710/1563] - Loss: 0.7305, Avg Loss: 0.0539\n",
            "Batch [720/1563] - Loss: 0.7484, Avg Loss: 0.0538\n",
            "Batch [730/1563] - Loss: 0.7361, Avg Loss: 0.0537\n",
            "Batch [740/1563] - Loss: 0.7330, Avg Loss: 0.0536\n",
            "Batch [750/1563] - Loss: 0.7371, Avg Loss: 0.0535\n",
            "Batch [760/1563] - Loss: 0.7685, Avg Loss: 0.0534\n",
            "Batch [770/1563] - Loss: 0.7713, Avg Loss: 0.0534\n",
            "Batch [780/1563] - Loss: 0.7709, Avg Loss: 0.0533\n",
            "Batch [790/1563] - Loss: 0.7584, Avg Loss: 0.0532\n",
            "Batch [800/1563] - Loss: 0.7692, Avg Loss: 0.0531\n",
            "Batch [810/1563] - Loss: 0.7491, Avg Loss: 0.0530\n",
            "Batch [820/1563] - Loss: 0.7341, Avg Loss: 0.0530\n",
            "Batch [830/1563] - Loss: 0.7547, Avg Loss: 0.0529\n",
            "Batch [840/1563] - Loss: 0.7132, Avg Loss: 0.0528\n",
            "Batch [850/1563] - Loss: 0.7552, Avg Loss: 0.0527\n",
            "Batch [860/1563] - Loss: 0.7498, Avg Loss: 0.0526\n",
            "Batch [870/1563] - Loss: 0.7280, Avg Loss: 0.0526\n",
            "Batch [880/1563] - Loss: 0.7128, Avg Loss: 0.0525\n",
            "Batch [890/1563] - Loss: 0.7495, Avg Loss: 0.0524\n",
            "Batch [900/1563] - Loss: 0.7333, Avg Loss: 0.0524\n",
            "Batch [910/1563] - Loss: 0.7274, Avg Loss: 0.0523\n",
            "Batch [920/1563] - Loss: 0.7358, Avg Loss: 0.0522\n",
            "Batch [930/1563] - Loss: 0.7407, Avg Loss: 0.0522\n",
            "Batch [940/1563] - Loss: 0.7515, Avg Loss: 0.0521\n",
            "Batch [950/1563] - Loss: 0.7226, Avg Loss: 0.0520\n",
            "Batch [960/1563] - Loss: 0.7076, Avg Loss: 0.0520\n",
            "Batch [970/1563] - Loss: 0.7191, Avg Loss: 0.0519\n",
            "Batch [980/1563] - Loss: 0.7231, Avg Loss: 0.0518\n",
            "Batch [990/1563] - Loss: 0.7093, Avg Loss: 0.0518\n",
            "Batch [1000/1563] - Loss: 0.7374, Avg Loss: 0.0517\n",
            "Batch [1010/1563] - Loss: 0.7137, Avg Loss: 0.0516\n",
            "Batch [1020/1563] - Loss: 0.7297, Avg Loss: 0.0516\n",
            "Batch [1030/1563] - Loss: 0.7418, Avg Loss: 0.0515\n",
            "Batch [1040/1563] - Loss: 0.7317, Avg Loss: 0.0515\n",
            "Batch [1050/1563] - Loss: 0.7412, Avg Loss: 0.0514\n",
            "Batch [1060/1563] - Loss: 0.7198, Avg Loss: 0.0513\n",
            "Batch [1070/1563] - Loss: 0.7230, Avg Loss: 0.0513\n",
            "Batch [1080/1563] - Loss: 0.7242, Avg Loss: 0.0512\n",
            "Batch [1090/1563] - Loss: 0.6858, Avg Loss: 0.0512\n",
            "Batch [1100/1563] - Loss: 0.7107, Avg Loss: 0.0511\n",
            "Batch [1110/1563] - Loss: 0.7037, Avg Loss: 0.0511\n",
            "Batch [1120/1563] - Loss: 0.7378, Avg Loss: 0.0510\n",
            "Batch [1130/1563] - Loss: 0.7041, Avg Loss: 0.0510\n",
            "Batch [1140/1563] - Loss: 0.7129, Avg Loss: 0.0509\n",
            "Batch [1150/1563] - Loss: 0.7290, Avg Loss: 0.0509\n",
            "Batch [1160/1563] - Loss: 0.7195, Avg Loss: 0.0508\n",
            "Batch [1170/1563] - Loss: 0.6882, Avg Loss: 0.0508\n",
            "Batch [1180/1563] - Loss: 0.7105, Avg Loss: 0.0507\n",
            "Batch [1190/1563] - Loss: 0.7062, Avg Loss: 0.0507\n",
            "Batch [1200/1563] - Loss: 0.7120, Avg Loss: 0.0506\n",
            "Batch [1210/1563] - Loss: 0.7225, Avg Loss: 0.0506\n",
            "Batch [1220/1563] - Loss: 0.7209, Avg Loss: 0.0505\n",
            "Batch [1230/1563] - Loss: 0.6913, Avg Loss: 0.0505\n",
            "Batch [1240/1563] - Loss: 0.7354, Avg Loss: 0.0504\n",
            "Batch [1250/1563] - Loss: 0.7249, Avg Loss: 0.0504\n",
            "Batch [1260/1563] - Loss: 0.6914, Avg Loss: 0.0503\n",
            "Batch [1270/1563] - Loss: 0.7023, Avg Loss: 0.0503\n",
            "Batch [1280/1563] - Loss: 0.7089, Avg Loss: 0.0502\n",
            "Batch [1290/1563] - Loss: 0.6842, Avg Loss: 0.0502\n",
            "Batch [1300/1563] - Loss: 0.7047, Avg Loss: 0.0501\n",
            "Batch [1310/1563] - Loss: 0.6765, Avg Loss: 0.0501\n",
            "Batch [1320/1563] - Loss: 0.7309, Avg Loss: 0.0501\n",
            "Batch [1330/1563] - Loss: 0.6759, Avg Loss: 0.0500\n",
            "Batch [1340/1563] - Loss: 0.7184, Avg Loss: 0.0500\n",
            "Batch [1350/1563] - Loss: 0.7139, Avg Loss: 0.0499\n",
            "Batch [1360/1563] - Loss: 0.7169, Avg Loss: 0.0499\n",
            "Batch [1370/1563] - Loss: 0.7157, Avg Loss: 0.0499\n",
            "Batch [1380/1563] - Loss: 0.6992, Avg Loss: 0.0498\n",
            "Batch [1390/1563] - Loss: 0.6838, Avg Loss: 0.0498\n",
            "Batch [1400/1563] - Loss: 0.6762, Avg Loss: 0.0497\n",
            "Batch [1410/1563] - Loss: 0.7153, Avg Loss: 0.0497\n",
            "Batch [1420/1563] - Loss: 0.7093, Avg Loss: 0.0497\n",
            "Batch [1430/1563] - Loss: 0.6727, Avg Loss: 0.0496\n",
            "Batch [1440/1563] - Loss: 0.7086, Avg Loss: 0.0496\n",
            "Batch [1450/1563] - Loss: 0.7072, Avg Loss: 0.0495\n",
            "Batch [1460/1563] - Loss: 0.7073, Avg Loss: 0.0495\n",
            "Batch [1470/1563] - Loss: 0.6463, Avg Loss: 0.0494\n",
            "Batch [1480/1563] - Loss: 0.6765, Avg Loss: 0.0494\n",
            "Batch [1490/1563] - Loss: 0.7175, Avg Loss: 0.0494\n",
            "Batch [1500/1563] - Loss: 0.7058, Avg Loss: 0.0493\n",
            "Batch [1510/1563] - Loss: 0.6787, Avg Loss: 0.0493\n",
            "Batch [1520/1563] - Loss: 0.7026, Avg Loss: 0.0493\n",
            "Batch [1530/1563] - Loss: 0.6719, Avg Loss: 0.0492\n",
            "Batch [1540/1563] - Loss: 0.7087, Avg Loss: 0.0492\n",
            "Batch [1550/1563] - Loss: 0.6840, Avg Loss: 0.0491\n",
            "Batch [1560/1563] - Loss: 0.7184, Avg Loss: 0.0491\n",
            "Epoch completed - Total samples: 25000, Average Loss: 0.0491\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:44, 34.89it/s]Eval Batch [20/1563] - Current Accuracy: 0.7375\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.29it/s]Eval Batch [40/1563] - Current Accuracy: 0.7188\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 39.00it/s]Eval Batch [60/1563] - Current Accuracy: 0.7146\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:38, 38.85it/s]Eval Batch [80/1563] - Current Accuracy: 0.7227\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 38.80it/s]Eval Batch [100/1563] - Current Accuracy: 0.7288\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:37, 39.05it/s]Eval Batch [120/1563] - Current Accuracy: 0.7318\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 39.07it/s]Eval Batch [140/1563] - Current Accuracy: 0.7357\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:35, 39.06it/s]Eval Batch [160/1563] - Current Accuracy: 0.7285\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 39.20it/s]Eval Batch [180/1563] - Current Accuracy: 0.7229\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:34, 39.14it/s]Eval Batch [200/1563] - Current Accuracy: 0.7253\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 39.16it/s]Eval Batch [220/1563] - Current Accuracy: 0.7250\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:33, 39.07it/s]Eval Batch [240/1563] - Current Accuracy: 0.7258\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 39.14it/s]Eval Batch [260/1563] - Current Accuracy: 0.7279\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:32, 39.14it/s]Eval Batch [280/1563] - Current Accuracy: 0.7277\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 39.10it/s]Eval Batch [300/1563] - Current Accuracy: 0.7277\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:31, 39.10it/s]Eval Batch [320/1563] - Current Accuracy: 0.7271\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 39.13it/s]Eval Batch [340/1563] - Current Accuracy: 0.7250\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:30, 39.09it/s]Eval Batch [360/1563] - Current Accuracy: 0.7278\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 39.15it/s]Eval Batch [380/1563] - Current Accuracy: 0.7257\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:29, 39.06it/s]Eval Batch [400/1563] - Current Accuracy: 0.7217\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 39.07it/s]Eval Batch [420/1563] - Current Accuracy: 0.7251\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:29, 38.82it/s]Eval Batch [440/1563] - Current Accuracy: 0.7260\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 38.86it/s]Eval Batch [460/1563] - Current Accuracy: 0.7258\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:27, 38.95it/s]Eval Batch [480/1563] - Current Accuracy: 0.7258\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 39.15it/s]Eval Batch [500/1563] - Current Accuracy: 0.7265\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 39.10it/s]Eval Batch [520/1563] - Current Accuracy: 0.7264\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 39.06it/s]Eval Batch [540/1563] - Current Accuracy: 0.7269\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 39.21it/s]Eval Batch [560/1563] - Current Accuracy: 0.7267\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 39.19it/s]Eval Batch [580/1563] - Current Accuracy: 0.7236\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 39.19it/s]Eval Batch [600/1563] - Current Accuracy: 0.7229\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 39.15it/s]Eval Batch [620/1563] - Current Accuracy: 0.7216\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 39.19it/s]Eval Batch [640/1563] - Current Accuracy: 0.7226\n",
            "Evaluating SNN:  42% 657/1563 [00:16<00:23, 39.21it/s]Eval Batch [660/1563] - Current Accuracy: 0.7254\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 39.25it/s]Eval Batch [680/1563] - Current Accuracy: 0.7263\n",
            "Evaluating SNN:  45% 697/1563 [00:17<00:22, 38.95it/s]Eval Batch [700/1563] - Current Accuracy: 0.7271\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 39.15it/s]Eval Batch [720/1563] - Current Accuracy: 0.7265\n",
            "Evaluating SNN:  47% 737/1563 [00:18<00:21, 39.16it/s]Eval Batch [740/1563] - Current Accuracy: 0.7261\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 39.20it/s]Eval Batch [760/1563] - Current Accuracy: 0.7274\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 39.17it/s]Eval Batch [780/1563] - Current Accuracy: 0.7273\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 39.10it/s]Eval Batch [800/1563] - Current Accuracy: 0.7268\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 39.00it/s]Eval Batch [820/1563] - Current Accuracy: 0.7248\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 38.99it/s]Eval Batch [840/1563] - Current Accuracy: 0.7201\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 38.81it/s]Eval Batch [860/1563] - Current Accuracy: 0.7170\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 38.65it/s]Eval Batch [880/1563] - Current Accuracy: 0.7147\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 38.62it/s]Eval Batch [900/1563] - Current Accuracy: 0.7103\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 38.65it/s]Eval Batch [920/1563] - Current Accuracy: 0.7072\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:16, 38.86it/s]Eval Batch [940/1563] - Current Accuracy: 0.7060\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 39.19it/s]Eval Batch [960/1563] - Current Accuracy: 0.7018\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:14, 39.30it/s]Eval Batch [980/1563] - Current Accuracy: 0.6989\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 39.01it/s]Eval Batch [1000/1563] - Current Accuracy: 0.6966\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:14, 38.87it/s]Eval Batch [1020/1563] - Current Accuracy: 0.6934\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 38.83it/s]Eval Batch [1040/1563] - Current Accuracy: 0.6904\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:13, 38.58it/s]Eval Batch [1060/1563] - Current Accuracy: 0.6881\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 38.79it/s]Eval Batch [1080/1563] - Current Accuracy: 0.6859\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:11, 38.87it/s]Eval Batch [1100/1563] - Current Accuracy: 0.6845\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 38.93it/s]Eval Batch [1120/1563] - Current Accuracy: 0.6825\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 39.07it/s]Eval Batch [1140/1563] - Current Accuracy: 0.6804\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 39.14it/s]Eval Batch [1160/1563] - Current Accuracy: 0.6790\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 39.12it/s]Eval Batch [1180/1563] - Current Accuracy: 0.6787\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 39.23it/s]Eval Batch [1200/1563] - Current Accuracy: 0.6755\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 39.21it/s]Eval Batch [1220/1563] - Current Accuracy: 0.6737\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 39.20it/s]Eval Batch [1240/1563] - Current Accuracy: 0.6714\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 39.24it/s]Eval Batch [1260/1563] - Current Accuracy: 0.6681\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 39.25it/s]Eval Batch [1280/1563] - Current Accuracy: 0.6682\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 39.02it/s]Eval Batch [1300/1563] - Current Accuracy: 0.6677\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 38.74it/s]Eval Batch [1320/1563] - Current Accuracy: 0.6663\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 38.73it/s]Eval Batch [1340/1563] - Current Accuracy: 0.6658\n",
            "Evaluating SNN:  87% 1357/1563 [00:34<00:05, 38.77it/s]Eval Batch [1360/1563] - Current Accuracy: 0.6642\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 38.88it/s]Eval Batch [1380/1563] - Current Accuracy: 0.6624\n",
            "Evaluating SNN:  89% 1397/1563 [00:35<00:04, 39.08it/s]Eval Batch [1400/1563] - Current Accuracy: 0.6613\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 39.15it/s]Eval Batch [1420/1563] - Current Accuracy: 0.6615\n",
            "Evaluating SNN:  92% 1437/1563 [00:36<00:03, 39.19it/s]Eval Batch [1440/1563] - Current Accuracy: 0.6603\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 39.16it/s]Eval Batch [1460/1563] - Current Accuracy: 0.6608\n",
            "Evaluating SNN:  94% 1477/1563 [00:37<00:02, 39.19it/s]Eval Batch [1480/1563] - Current Accuracy: 0.6604\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 39.10it/s]Eval Batch [1500/1563] - Current Accuracy: 0.6589\n",
            "Evaluating SNN:  97% 1517/1563 [00:38<00:01, 39.23it/s]Eval Batch [1520/1563] - Current Accuracy: 0.6585\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 39.16it/s]Eval Batch [1540/1563] - Current Accuracy: 0.6572\n",
            "Evaluating SNN: 100% 1557/1563 [00:40<00:00, 39.12it/s]Eval Batch [1560/1563] - Current Accuracy: 0.6568\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.89it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.6566\n",
            "INFO:root:Saved model weights at epoch 0 to: ./checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_0.pth\n",
            "[2025-09-24 17:53:39,698][main.py][line:639][INFO] Saved model weights at epoch 0 to: ./checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_0.pth\n",
            "INFO:root:Save directory exists: True\n",
            "[2025-09-24 17:53:39,698][main.py][line:640][INFO] Save directory exists: True\n",
            "INFO:root:File size: 265511595 bytes\n",
            "[2025-09-24 17:53:39,699][main.py][line:641][INFO] File size: 265511595 bytes\n",
            "INFO:root:SNNs training Epoch 0 [distilbert_base_qcfs]: Val_loss: 0.04911061215877533\n",
            "[2025-09-24 17:53:39,699][main.py][line:644][INFO] SNNs training Epoch 0 [distilbert_base_qcfs]: Val_loss: 0.04911061215877533\n",
            "INFO:root:SNNs training Epoch 0 [distilbert_base_qcfs]: Test Acc: 0.6566 Best Acc: 0.6566\n",
            "[2025-09-24 17:53:39,699][main.py][line:645][INFO] SNNs training Epoch 0 [distilbert_base_qcfs]: Test Acc: 0.6566 Best Acc: 0.6566\n",
            "Starting training with 1563 batches...\n",
            "Batch [10/1563] - Loss: 0.6826, Avg Loss: 0.0439\n",
            "Batch [20/1563] - Loss: 0.7248, Avg Loss: 0.0436\n",
            "Batch [30/1563] - Loss: 0.6801, Avg Loss: 0.0436\n",
            "Batch [40/1563] - Loss: 0.6994, Avg Loss: 0.0436\n",
            "Batch [50/1563] - Loss: 0.6997, Avg Loss: 0.0435\n",
            "Batch [60/1563] - Loss: 0.6891, Avg Loss: 0.0436\n",
            "Batch [70/1563] - Loss: 0.7238, Avg Loss: 0.0436\n",
            "Batch [80/1563] - Loss: 0.7208, Avg Loss: 0.0436\n",
            "Batch [90/1563] - Loss: 0.7191, Avg Loss: 0.0435\n",
            "Batch [100/1563] - Loss: 0.7082, Avg Loss: 0.0435\n",
            "Batch [110/1563] - Loss: 0.7164, Avg Loss: 0.0435\n",
            "Batch [120/1563] - Loss: 0.6765, Avg Loss: 0.0435\n",
            "Batch [130/1563] - Loss: 0.6948, Avg Loss: 0.0434\n",
            "Batch [140/1563] - Loss: 0.6724, Avg Loss: 0.0434\n",
            "Batch [150/1563] - Loss: 0.6758, Avg Loss: 0.0434\n",
            "Batch [160/1563] - Loss: 0.6874, Avg Loss: 0.0434\n",
            "Batch [170/1563] - Loss: 0.6663, Avg Loss: 0.0434\n",
            "Batch [180/1563] - Loss: 0.7001, Avg Loss: 0.0434\n",
            "Batch [190/1563] - Loss: 0.7048, Avg Loss: 0.0434\n",
            "Batch [200/1563] - Loss: 0.6808, Avg Loss: 0.0434\n",
            "Batch [210/1563] - Loss: 0.6754, Avg Loss: 0.0434\n",
            "Batch [220/1563] - Loss: 0.6884, Avg Loss: 0.0433\n",
            "Batch [230/1563] - Loss: 0.6945, Avg Loss: 0.0433\n",
            "Batch [240/1563] - Loss: 0.6982, Avg Loss: 0.0433\n",
            "Batch [250/1563] - Loss: 0.6724, Avg Loss: 0.0433\n",
            "Batch [260/1563] - Loss: 0.6755, Avg Loss: 0.0432\n",
            "Batch [270/1563] - Loss: 0.7013, Avg Loss: 0.0432\n",
            "Batch [280/1563] - Loss: 0.6884, Avg Loss: 0.0432\n",
            "Batch [290/1563] - Loss: 0.6297, Avg Loss: 0.0431\n",
            "Batch [300/1563] - Loss: 0.7213, Avg Loss: 0.0431\n",
            "Batch [310/1563] - Loss: 0.6809, Avg Loss: 0.0431\n",
            "Batch [320/1563] - Loss: 0.6684, Avg Loss: 0.0431\n",
            "Batch [330/1563] - Loss: 0.6720, Avg Loss: 0.0431\n",
            "Batch [340/1563] - Loss: 0.6692, Avg Loss: 0.0431\n",
            "Batch [350/1563] - Loss: 0.7133, Avg Loss: 0.0431\n",
            "Batch [360/1563] - Loss: 0.6501, Avg Loss: 0.0431\n",
            "Batch [370/1563] - Loss: 0.6855, Avg Loss: 0.0430\n",
            "Batch [380/1563] - Loss: 0.6914, Avg Loss: 0.0430\n",
            "Batch [390/1563] - Loss: 0.6812, Avg Loss: 0.0430\n",
            "Batch [400/1563] - Loss: 0.6740, Avg Loss: 0.0430\n",
            "Batch [410/1563] - Loss: 0.6754, Avg Loss: 0.0430\n",
            "Batch [420/1563] - Loss: 0.6527, Avg Loss: 0.0430\n",
            "Batch [430/1563] - Loss: 0.6550, Avg Loss: 0.0429\n",
            "Batch [440/1563] - Loss: 0.6901, Avg Loss: 0.0429\n",
            "Batch [450/1563] - Loss: 0.6612, Avg Loss: 0.0429\n",
            "Batch [460/1563] - Loss: 0.6716, Avg Loss: 0.0429\n",
            "Batch [470/1563] - Loss: 0.7245, Avg Loss: 0.0429\n",
            "Batch [480/1563] - Loss: 0.6694, Avg Loss: 0.0429\n",
            "Batch [490/1563] - Loss: 0.6842, Avg Loss: 0.0428\n",
            "Batch [500/1563] - Loss: 0.6981, Avg Loss: 0.0428\n",
            "Batch [510/1563] - Loss: 0.6875, Avg Loss: 0.0428\n",
            "Batch [520/1563] - Loss: 0.6715, Avg Loss: 0.0428\n",
            "Batch [530/1563] - Loss: 0.6772, Avg Loss: 0.0428\n",
            "Batch [540/1563] - Loss: 0.6548, Avg Loss: 0.0428\n",
            "Batch [550/1563] - Loss: 0.6612, Avg Loss: 0.0428\n",
            "Batch [560/1563] - Loss: 0.7342, Avg Loss: 0.0428\n",
            "Batch [570/1563] - Loss: 0.6401, Avg Loss: 0.0427\n",
            "Batch [580/1563] - Loss: 0.7034, Avg Loss: 0.0427\n",
            "Batch [590/1563] - Loss: 0.6789, Avg Loss: 0.0427\n",
            "Batch [600/1563] - Loss: 0.7002, Avg Loss: 0.0427\n",
            "Batch [610/1563] - Loss: 0.6570, Avg Loss: 0.0427\n",
            "Batch [620/1563] - Loss: 0.6608, Avg Loss: 0.0427\n",
            "Batch [630/1563] - Loss: 0.6707, Avg Loss: 0.0427\n",
            "Batch [640/1563] - Loss: 0.6437, Avg Loss: 0.0426\n",
            "Batch [650/1563] - Loss: 0.7035, Avg Loss: 0.0426\n",
            "Batch [660/1563] - Loss: 0.6755, Avg Loss: 0.0426\n",
            "Batch [670/1563] - Loss: 0.6821, Avg Loss: 0.0426\n",
            "Batch [680/1563] - Loss: 0.6566, Avg Loss: 0.0426\n",
            "Batch [690/1563] - Loss: 0.6980, Avg Loss: 0.0426\n",
            "Batch [700/1563] - Loss: 0.7038, Avg Loss: 0.0426\n",
            "Batch [710/1563] - Loss: 0.6119, Avg Loss: 0.0426\n",
            "Batch [720/1563] - Loss: 0.6203, Avg Loss: 0.0425\n",
            "Batch [730/1563] - Loss: 0.7105, Avg Loss: 0.0425\n",
            "Batch [740/1563] - Loss: 0.6996, Avg Loss: 0.0425\n",
            "Batch [750/1563] - Loss: 0.6675, Avg Loss: 0.0425\n",
            "Batch [760/1563] - Loss: 0.6849, Avg Loss: 0.0425\n",
            "Batch [770/1563] - Loss: 0.6121, Avg Loss: 0.0425\n",
            "Batch [780/1563] - Loss: 0.5901, Avg Loss: 0.0425\n",
            "Batch [790/1563] - Loss: 0.6443, Avg Loss: 0.0424\n",
            "Batch [800/1563] - Loss: 0.6818, Avg Loss: 0.0424\n",
            "Batch [810/1563] - Loss: 0.6752, Avg Loss: 0.0424\n",
            "Batch [820/1563] - Loss: 0.6525, Avg Loss: 0.0424\n",
            "Batch [830/1563] - Loss: 0.6405, Avg Loss: 0.0424\n",
            "Batch [840/1563] - Loss: 0.6348, Avg Loss: 0.0424\n",
            "Batch [850/1563] - Loss: 0.6909, Avg Loss: 0.0423\n",
            "Batch [860/1563] - Loss: 0.6637, Avg Loss: 0.0423\n",
            "Batch [870/1563] - Loss: 0.6382, Avg Loss: 0.0423\n",
            "Batch [880/1563] - Loss: 0.5966, Avg Loss: 0.0423\n",
            "Batch [890/1563] - Loss: 0.6393, Avg Loss: 0.0423\n",
            "Batch [900/1563] - Loss: 0.6763, Avg Loss: 0.0423\n",
            "Batch [910/1563] - Loss: 0.6719, Avg Loss: 0.0423\n",
            "Batch [920/1563] - Loss: 0.6913, Avg Loss: 0.0422\n",
            "Batch [930/1563] - Loss: 0.6835, Avg Loss: 0.0422\n",
            "Batch [940/1563] - Loss: 0.6205, Avg Loss: 0.0422\n",
            "Batch [950/1563] - Loss: 0.6629, Avg Loss: 0.0422\n",
            "Batch [960/1563] - Loss: 0.6197, Avg Loss: 0.0422\n",
            "Batch [970/1563] - Loss: 0.6164, Avg Loss: 0.0421\n",
            "Batch [980/1563] - Loss: 0.6101, Avg Loss: 0.0421\n",
            "Batch [990/1563] - Loss: 0.7049, Avg Loss: 0.0421\n",
            "Batch [1000/1563] - Loss: 0.6634, Avg Loss: 0.0421\n",
            "Batch [1010/1563] - Loss: 0.6201, Avg Loss: 0.0421\n",
            "Batch [1020/1563] - Loss: 0.6845, Avg Loss: 0.0421\n",
            "Batch [1030/1563] - Loss: 0.6190, Avg Loss: 0.0420\n",
            "Batch [1040/1563] - Loss: 0.6159, Avg Loss: 0.0420\n",
            "Batch [1050/1563] - Loss: 0.6964, Avg Loss: 0.0420\n",
            "Batch [1060/1563] - Loss: 0.6397, Avg Loss: 0.0420\n",
            "Batch [1070/1563] - Loss: 0.5639, Avg Loss: 0.0420\n",
            "Batch [1080/1563] - Loss: 0.5891, Avg Loss: 0.0419\n",
            "Batch [1090/1563] - Loss: 0.6657, Avg Loss: 0.0419\n",
            "Batch [1100/1563] - Loss: 0.5960, Avg Loss: 0.0419\n",
            "Batch [1110/1563] - Loss: 0.6639, Avg Loss: 0.0419\n",
            "Batch [1120/1563] - Loss: 0.6487, Avg Loss: 0.0419\n",
            "Batch [1130/1563] - Loss: 0.6435, Avg Loss: 0.0419\n",
            "Batch [1140/1563] - Loss: 0.7085, Avg Loss: 0.0419\n",
            "Batch [1150/1563] - Loss: 0.6208, Avg Loss: 0.0418\n",
            "Batch [1160/1563] - Loss: 0.6444, Avg Loss: 0.0418\n",
            "Batch [1170/1563] - Loss: 0.5702, Avg Loss: 0.0418\n",
            "Batch [1180/1563] - Loss: 0.6130, Avg Loss: 0.0418\n",
            "Batch [1190/1563] - Loss: 0.6500, Avg Loss: 0.0418\n",
            "Batch [1200/1563] - Loss: 0.5773, Avg Loss: 0.0418\n",
            "Batch [1210/1563] - Loss: 0.5845, Avg Loss: 0.0417\n",
            "Batch [1220/1563] - Loss: 0.6404, Avg Loss: 0.0417\n",
            "Batch [1230/1563] - Loss: 0.6533, Avg Loss: 0.0417\n",
            "Batch [1240/1563] - Loss: 0.5771, Avg Loss: 0.0417\n",
            "Batch [1250/1563] - Loss: 0.6653, Avg Loss: 0.0417\n",
            "Batch [1260/1563] - Loss: 0.6564, Avg Loss: 0.0417\n",
            "Batch [1270/1563] - Loss: 0.7050, Avg Loss: 0.0416\n",
            "Batch [1280/1563] - Loss: 0.6545, Avg Loss: 0.0416\n",
            "Batch [1290/1563] - Loss: 0.6393, Avg Loss: 0.0416\n",
            "Batch [1300/1563] - Loss: 0.6204, Avg Loss: 0.0416\n",
            "Batch [1310/1563] - Loss: 0.5911, Avg Loss: 0.0416\n",
            "Batch [1320/1563] - Loss: 0.6075, Avg Loss: 0.0415\n",
            "Batch [1330/1563] - Loss: 0.6489, Avg Loss: 0.0415\n",
            "Batch [1340/1563] - Loss: 0.6146, Avg Loss: 0.0415\n",
            "Batch [1350/1563] - Loss: 0.6208, Avg Loss: 0.0415\n",
            "Batch [1360/1563] - Loss: 0.6656, Avg Loss: 0.0415\n",
            "Batch [1370/1563] - Loss: 0.6198, Avg Loss: 0.0414\n",
            "Batch [1380/1563] - Loss: 0.6026, Avg Loss: 0.0414\n",
            "Batch [1390/1563] - Loss: 0.6241, Avg Loss: 0.0414\n",
            "Batch [1400/1563] - Loss: 0.6115, Avg Loss: 0.0414\n",
            "Batch [1410/1563] - Loss: 0.6000, Avg Loss: 0.0414\n",
            "Batch [1420/1563] - Loss: 0.6000, Avg Loss: 0.0413\n",
            "Batch [1430/1563] - Loss: 0.6326, Avg Loss: 0.0413\n",
            "Batch [1440/1563] - Loss: 0.5930, Avg Loss: 0.0413\n",
            "Batch [1450/1563] - Loss: 0.6532, Avg Loss: 0.0413\n",
            "Batch [1460/1563] - Loss: 0.5997, Avg Loss: 0.0413\n",
            "Batch [1470/1563] - Loss: 0.6166, Avg Loss: 0.0412\n",
            "Batch [1480/1563] - Loss: 0.6633, Avg Loss: 0.0412\n",
            "Batch [1490/1563] - Loss: 0.6048, Avg Loss: 0.0412\n",
            "Batch [1500/1563] - Loss: 0.7268, Avg Loss: 0.0412\n",
            "Batch [1510/1563] - Loss: 0.6693, Avg Loss: 0.0412\n",
            "Batch [1520/1563] - Loss: 0.6739, Avg Loss: 0.0411\n",
            "Batch [1530/1563] - Loss: 0.5631, Avg Loss: 0.0411\n",
            "Batch [1540/1563] - Loss: 0.6341, Avg Loss: 0.0411\n",
            "Batch [1550/1563] - Loss: 0.6204, Avg Loss: 0.0411\n",
            "Batch [1560/1563] - Loss: 0.5692, Avg Loss: 0.0411\n",
            "Epoch completed - Total samples: 25000, Average Loss: 0.0411\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:44, 35.11it/s]Eval Batch [20/1563] - Current Accuracy: 0.8125\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.46it/s]Eval Batch [40/1563] - Current Accuracy: 0.8047\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 39.00it/s]Eval Batch [60/1563] - Current Accuracy: 0.7844\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:38, 39.10it/s]Eval Batch [80/1563] - Current Accuracy: 0.7977\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 39.07it/s]Eval Batch [100/1563] - Current Accuracy: 0.8069\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:36, 39.17it/s]Eval Batch [120/1563] - Current Accuracy: 0.8010\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 38.92it/s]Eval Batch [140/1563] - Current Accuracy: 0.7982\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:36, 38.56it/s]Eval Batch [160/1563] - Current Accuracy: 0.7937\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:36, 38.49it/s]Eval Batch [180/1563] - Current Accuracy: 0.7948\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:35, 38.76it/s]Eval Batch [200/1563] - Current Accuracy: 0.7987\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 39.01it/s]Eval Batch [220/1563] - Current Accuracy: 0.7994\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:33, 39.18it/s]Eval Batch [240/1563] - Current Accuracy: 0.7987\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 39.13it/s]Eval Batch [260/1563] - Current Accuracy: 0.7993\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:33, 38.97it/s]Eval Batch [280/1563] - Current Accuracy: 0.8000\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 38.84it/s]Eval Batch [300/1563] - Current Accuracy: 0.7998\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:32, 38.55it/s]Eval Batch [320/1563] - Current Accuracy: 0.7996\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 38.56it/s]Eval Batch [340/1563] - Current Accuracy: 0.7982\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:31, 38.76it/s]Eval Batch [360/1563] - Current Accuracy: 0.8003\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 39.03it/s]Eval Batch [380/1563] - Current Accuracy: 0.7988\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:29, 39.11it/s]Eval Batch [400/1563] - Current Accuracy: 0.7950\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 39.16it/s]Eval Batch [420/1563] - Current Accuracy: 0.7972\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:28, 39.18it/s]Eval Batch [440/1563] - Current Accuracy: 0.7969\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 39.16it/s]Eval Batch [460/1563] - Current Accuracy: 0.7967\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:27, 39.13it/s]Eval Batch [480/1563] - Current Accuracy: 0.7971\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 39.08it/s]Eval Batch [500/1563] - Current Accuracy: 0.7961\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 39.15it/s]Eval Batch [520/1563] - Current Accuracy: 0.7962\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 39.13it/s]Eval Batch [540/1563] - Current Accuracy: 0.7955\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 39.09it/s]Eval Batch [560/1563] - Current Accuracy: 0.7949\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 39.14it/s]Eval Batch [580/1563] - Current Accuracy: 0.7909\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 39.25it/s]Eval Batch [600/1563] - Current Accuracy: 0.7907\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 39.10it/s]Eval Batch [620/1563] - Current Accuracy: 0.7896\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 39.15it/s]Eval Batch [640/1563] - Current Accuracy: 0.7903\n",
            "Evaluating SNN:  42% 657/1563 [00:16<00:23, 39.16it/s]Eval Batch [660/1563] - Current Accuracy: 0.7928\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 38.91it/s]Eval Batch [680/1563] - Current Accuracy: 0.7947\n",
            "Evaluating SNN:  45% 697/1563 [00:17<00:22, 39.09it/s]Eval Batch [700/1563] - Current Accuracy: 0.7953\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 38.96it/s]Eval Batch [720/1563] - Current Accuracy: 0.7945\n",
            "Evaluating SNN:  47% 737/1563 [00:19<00:21, 38.98it/s]Eval Batch [740/1563] - Current Accuracy: 0.7938\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 38.79it/s]Eval Batch [760/1563] - Current Accuracy: 0.7945\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 38.93it/s]Eval Batch [780/1563] - Current Accuracy: 0.7940\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 38.74it/s]Eval Batch [800/1563] - Current Accuracy: 0.7941\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 38.78it/s]Eval Batch [820/1563] - Current Accuracy: 0.7939\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 38.97it/s]Eval Batch [840/1563] - Current Accuracy: 0.7910\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 38.97it/s]Eval Batch [860/1563] - Current Accuracy: 0.7900\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 39.05it/s]Eval Batch [880/1563] - Current Accuracy: 0.7880\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 39.14it/s]Eval Batch [900/1563] - Current Accuracy: 0.7860\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 39.16it/s]Eval Batch [920/1563] - Current Accuracy: 0.7850\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:16, 39.01it/s]Eval Batch [940/1563] - Current Accuracy: 0.7843\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 38.83it/s]Eval Batch [960/1563] - Current Accuracy: 0.7826\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:15, 38.95it/s]Eval Batch [980/1563] - Current Accuracy: 0.7810\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 39.13it/s]Eval Batch [1000/1563] - Current Accuracy: 0.7805\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:13, 39.10it/s]Eval Batch [1020/1563] - Current Accuracy: 0.7776\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 39.12it/s]Eval Batch [1040/1563] - Current Accuracy: 0.7752\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:12, 39.17it/s]Eval Batch [1060/1563] - Current Accuracy: 0.7737\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 39.15it/s]Eval Batch [1080/1563] - Current Accuracy: 0.7719\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:11, 39.02it/s]Eval Batch [1100/1563] - Current Accuracy: 0.7717\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 39.02it/s]Eval Batch [1120/1563] - Current Accuracy: 0.7721\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 38.78it/s]Eval Batch [1140/1563] - Current Accuracy: 0.7705\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 38.90it/s]Eval Batch [1160/1563] - Current Accuracy: 0.7690\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 38.87it/s]Eval Batch [1180/1563] - Current Accuracy: 0.7685\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 38.68it/s]Eval Batch [1200/1563] - Current Accuracy: 0.7671\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 38.57it/s]Eval Batch [1220/1563] - Current Accuracy: 0.7657\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 38.73it/s]Eval Batch [1240/1563] - Current Accuracy: 0.7644\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 38.59it/s]Eval Batch [1260/1563] - Current Accuracy: 0.7627\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 38.84it/s]Eval Batch [1280/1563] - Current Accuracy: 0.7634\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 39.00it/s]Eval Batch [1300/1563] - Current Accuracy: 0.7638\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 38.89it/s]Eval Batch [1320/1563] - Current Accuracy: 0.7624\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 38.92it/s]Eval Batch [1340/1563] - Current Accuracy: 0.7618\n",
            "Evaluating SNN:  87% 1357/1563 [00:34<00:05, 39.13it/s]Eval Batch [1360/1563] - Current Accuracy: 0.7609\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 39.21it/s]Eval Batch [1380/1563] - Current Accuracy: 0.7597\n",
            "Evaluating SNN:  89% 1397/1563 [00:35<00:04, 39.18it/s]Eval Batch [1400/1563] - Current Accuracy: 0.7583\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 39.16it/s]Eval Batch [1420/1563] - Current Accuracy: 0.7585\n",
            "Evaluating SNN:  92% 1437/1563 [00:36<00:03, 39.17it/s]Eval Batch [1440/1563] - Current Accuracy: 0.7579\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 39.11it/s]Eval Batch [1460/1563] - Current Accuracy: 0.7585\n",
            "Evaluating SNN:  94% 1477/1563 [00:38<00:02, 39.12it/s]Eval Batch [1480/1563] - Current Accuracy: 0.7587\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 39.15it/s]Eval Batch [1500/1563] - Current Accuracy: 0.7578\n",
            "Evaluating SNN:  97% 1517/1563 [00:39<00:01, 39.13it/s]Eval Batch [1520/1563] - Current Accuracy: 0.7578\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 39.18it/s]Eval Batch [1540/1563] - Current Accuracy: 0.7568\n",
            "Evaluating SNN: 100% 1557/1563 [00:40<00:00, 39.23it/s]Eval Batch [1560/1563] - Current Accuracy: 0.7563\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.85it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.7560\n",
            "INFO:root:Saved model weights at epoch 1 to: ./checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_1.pth\n",
            "[2025-09-24 17:56:23,814][main.py][line:639][INFO] Saved model weights at epoch 1 to: ./checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_1.pth\n",
            "INFO:root:Save directory exists: True\n",
            "[2025-09-24 17:56:23,815][main.py][line:640][INFO] Save directory exists: True\n",
            "INFO:root:File size: 265511595 bytes\n",
            "[2025-09-24 17:56:23,815][main.py][line:641][INFO] File size: 265511595 bytes\n",
            "INFO:root:SNNs training Epoch 1 [distilbert_base_qcfs]: Val_loss: 0.0410663668680191\n",
            "[2025-09-24 17:56:23,815][main.py][line:644][INFO] SNNs training Epoch 1 [distilbert_base_qcfs]: Val_loss: 0.0410663668680191\n",
            "INFO:root:SNNs training Epoch 1 [distilbert_base_qcfs]: Test Acc: 0.7560 Best Acc: 0.7560\n",
            "[2025-09-24 17:56:23,815][main.py][line:645][INFO] SNNs training Epoch 1 [distilbert_base_qcfs]: Test Acc: 0.7560 Best Acc: 0.7560\n",
            "Starting training with 1563 batches...\n",
            "Batch [10/1563] - Loss: 0.6029, Avg Loss: 0.0357\n",
            "Batch [20/1563] - Loss: 0.5467, Avg Loss: 0.0366\n",
            "Batch [30/1563] - Loss: 0.6328, Avg Loss: 0.0368\n",
            "Batch [40/1563] - Loss: 0.5098, Avg Loss: 0.0370\n",
            "Batch [50/1563] - Loss: 0.5444, Avg Loss: 0.0370\n",
            "Batch [60/1563] - Loss: 0.6211, Avg Loss: 0.0371\n",
            "Batch [70/1563] - Loss: 0.5739, Avg Loss: 0.0372\n",
            "Batch [80/1563] - Loss: 0.5378, Avg Loss: 0.0372\n",
            "Batch [90/1563] - Loss: 0.5359, Avg Loss: 0.0374\n",
            "Batch [100/1563] - Loss: 0.5530, Avg Loss: 0.0373\n",
            "Batch [110/1563] - Loss: 0.5808, Avg Loss: 0.0374\n",
            "Batch [120/1563] - Loss: 0.6377, Avg Loss: 0.0374\n",
            "Batch [130/1563] - Loss: 0.5825, Avg Loss: 0.0375\n",
            "Batch [140/1563] - Loss: 0.6184, Avg Loss: 0.0375\n",
            "Batch [150/1563] - Loss: 0.6248, Avg Loss: 0.0374\n",
            "Batch [160/1563] - Loss: 0.6320, Avg Loss: 0.0374\n",
            "Batch [170/1563] - Loss: 0.5413, Avg Loss: 0.0372\n",
            "Batch [180/1563] - Loss: 0.5432, Avg Loss: 0.0373\n",
            "Batch [190/1563] - Loss: 0.5304, Avg Loss: 0.0371\n",
            "Batch [200/1563] - Loss: 0.5473, Avg Loss: 0.0371\n",
            "Batch [210/1563] - Loss: 0.5906, Avg Loss: 0.0371\n",
            "Batch [220/1563] - Loss: 0.6105, Avg Loss: 0.0370\n",
            "Batch [230/1563] - Loss: 0.5938, Avg Loss: 0.0369\n",
            "Batch [240/1563] - Loss: 0.6391, Avg Loss: 0.0369\n",
            "Batch [250/1563] - Loss: 0.6149, Avg Loss: 0.0369\n",
            "Batch [260/1563] - Loss: 0.5518, Avg Loss: 0.0368\n",
            "Batch [270/1563] - Loss: 0.5796, Avg Loss: 0.0368\n",
            "Batch [280/1563] - Loss: 0.5518, Avg Loss: 0.0368\n",
            "Batch [290/1563] - Loss: 0.5271, Avg Loss: 0.0367\n",
            "Batch [300/1563] - Loss: 0.4801, Avg Loss: 0.0367\n",
            "Batch [310/1563] - Loss: 0.5701, Avg Loss: 0.0367\n",
            "Batch [320/1563] - Loss: 0.6284, Avg Loss: 0.0367\n",
            "Batch [330/1563] - Loss: 0.5589, Avg Loss: 0.0367\n",
            "Batch [340/1563] - Loss: 0.5160, Avg Loss: 0.0366\n",
            "Batch [350/1563] - Loss: 0.5522, Avg Loss: 0.0366\n",
            "Batch [360/1563] - Loss: 0.5462, Avg Loss: 0.0366\n",
            "Batch [370/1563] - Loss: 0.5003, Avg Loss: 0.0365\n",
            "Batch [380/1563] - Loss: 0.6040, Avg Loss: 0.0366\n",
            "Batch [390/1563] - Loss: 0.5701, Avg Loss: 0.0366\n",
            "Batch [400/1563] - Loss: 0.5947, Avg Loss: 0.0365\n",
            "Batch [410/1563] - Loss: 0.5560, Avg Loss: 0.0365\n",
            "Batch [420/1563] - Loss: 0.5461, Avg Loss: 0.0364\n",
            "Batch [430/1563] - Loss: 0.5119, Avg Loss: 0.0364\n",
            "Batch [440/1563] - Loss: 0.5304, Avg Loss: 0.0364\n",
            "Batch [450/1563] - Loss: 0.5378, Avg Loss: 0.0364\n",
            "Batch [460/1563] - Loss: 0.4547, Avg Loss: 0.0364\n",
            "Batch [470/1563] - Loss: 0.5038, Avg Loss: 0.0363\n",
            "Batch [480/1563] - Loss: 0.5135, Avg Loss: 0.0363\n",
            "Batch [490/1563] - Loss: 0.5060, Avg Loss: 0.0363\n",
            "Batch [500/1563] - Loss: 0.5867, Avg Loss: 0.0363\n",
            "Batch [510/1563] - Loss: 0.5971, Avg Loss: 0.0362\n",
            "Batch [520/1563] - Loss: 0.5998, Avg Loss: 0.0362\n",
            "Batch [530/1563] - Loss: 0.6065, Avg Loss: 0.0362\n",
            "Batch [540/1563] - Loss: 0.5288, Avg Loss: 0.0362\n",
            "Batch [550/1563] - Loss: 0.5144, Avg Loss: 0.0361\n",
            "Batch [560/1563] - Loss: 0.5677, Avg Loss: 0.0361\n",
            "Batch [570/1563] - Loss: 0.6071, Avg Loss: 0.0361\n",
            "Batch [580/1563] - Loss: 0.6326, Avg Loss: 0.0360\n",
            "Batch [590/1563] - Loss: 0.5772, Avg Loss: 0.0360\n",
            "Batch [600/1563] - Loss: 0.7211, Avg Loss: 0.0360\n",
            "Batch [610/1563] - Loss: 0.5723, Avg Loss: 0.0359\n",
            "Batch [620/1563] - Loss: 0.5012, Avg Loss: 0.0359\n",
            "Batch [630/1563] - Loss: 0.5861, Avg Loss: 0.0359\n",
            "Batch [640/1563] - Loss: 0.5740, Avg Loss: 0.0359\n",
            "Batch [650/1563] - Loss: 0.5112, Avg Loss: 0.0359\n",
            "Batch [660/1563] - Loss: 0.5208, Avg Loss: 0.0358\n",
            "Batch [670/1563] - Loss: 0.5570, Avg Loss: 0.0358\n",
            "Batch [680/1563] - Loss: 0.5111, Avg Loss: 0.0357\n",
            "Batch [690/1563] - Loss: 0.6451, Avg Loss: 0.0357\n",
            "Batch [700/1563] - Loss: 0.5330, Avg Loss: 0.0357\n",
            "Batch [710/1563] - Loss: 0.4842, Avg Loss: 0.0357\n",
            "Batch [720/1563] - Loss: 0.5846, Avg Loss: 0.0356\n",
            "Batch [730/1563] - Loss: 0.5734, Avg Loss: 0.0356\n",
            "Batch [740/1563] - Loss: 0.5676, Avg Loss: 0.0356\n",
            "Batch [750/1563] - Loss: 0.4937, Avg Loss: 0.0355\n",
            "Batch [760/1563] - Loss: 0.7299, Avg Loss: 0.0355\n",
            "Batch [770/1563] - Loss: 0.5333, Avg Loss: 0.0355\n",
            "Batch [780/1563] - Loss: 0.6183, Avg Loss: 0.0355\n",
            "Batch [790/1563] - Loss: 0.4649, Avg Loss: 0.0355\n",
            "Batch [800/1563] - Loss: 0.5097, Avg Loss: 0.0355\n",
            "Batch [810/1563] - Loss: 0.5172, Avg Loss: 0.0354\n",
            "Batch [820/1563] - Loss: 0.5500, Avg Loss: 0.0354\n",
            "Batch [830/1563] - Loss: 0.5870, Avg Loss: 0.0353\n",
            "Batch [840/1563] - Loss: 0.4200, Avg Loss: 0.0353\n",
            "Batch [850/1563] - Loss: 0.5358, Avg Loss: 0.0353\n",
            "Batch [860/1563] - Loss: 0.4550, Avg Loss: 0.0352\n",
            "Batch [870/1563] - Loss: 0.5717, Avg Loss: 0.0352\n",
            "Batch [880/1563] - Loss: 0.5287, Avg Loss: 0.0351\n",
            "Batch [890/1563] - Loss: 0.4164, Avg Loss: 0.0351\n",
            "Batch [900/1563] - Loss: 0.4613, Avg Loss: 0.0351\n",
            "Batch [910/1563] - Loss: 0.4773, Avg Loss: 0.0350\n",
            "Batch [920/1563] - Loss: 0.4672, Avg Loss: 0.0350\n",
            "Batch [930/1563] - Loss: 0.4313, Avg Loss: 0.0349\n",
            "Batch [940/1563] - Loss: 0.5064, Avg Loss: 0.0349\n",
            "Batch [950/1563] - Loss: 0.4684, Avg Loss: 0.0349\n",
            "Batch [960/1563] - Loss: 0.6055, Avg Loss: 0.0349\n",
            "Batch [970/1563] - Loss: 0.5766, Avg Loss: 0.0348\n",
            "Batch [980/1563] - Loss: 0.4085, Avg Loss: 0.0348\n",
            "Batch [990/1563] - Loss: 0.4651, Avg Loss: 0.0348\n",
            "Batch [1000/1563] - Loss: 0.4905, Avg Loss: 0.0347\n",
            "Batch [1010/1563] - Loss: 0.5191, Avg Loss: 0.0347\n",
            "Batch [1020/1563] - Loss: 0.5166, Avg Loss: 0.0347\n",
            "Batch [1030/1563] - Loss: 0.5496, Avg Loss: 0.0346\n",
            "Batch [1040/1563] - Loss: 0.5651, Avg Loss: 0.0346\n",
            "Batch [1050/1563] - Loss: 0.4149, Avg Loss: 0.0346\n",
            "Batch [1060/1563] - Loss: 0.5084, Avg Loss: 0.0346\n",
            "Batch [1070/1563] - Loss: 0.4530, Avg Loss: 0.0345\n",
            "Batch [1080/1563] - Loss: 0.4703, Avg Loss: 0.0345\n",
            "Batch [1090/1563] - Loss: 0.5500, Avg Loss: 0.0345\n",
            "Batch [1100/1563] - Loss: 0.6435, Avg Loss: 0.0345\n",
            "Batch [1110/1563] - Loss: 0.4339, Avg Loss: 0.0344\n",
            "Batch [1120/1563] - Loss: 0.4220, Avg Loss: 0.0344\n",
            "Batch [1130/1563] - Loss: 0.5099, Avg Loss: 0.0343\n",
            "Batch [1140/1563] - Loss: 0.4304, Avg Loss: 0.0343\n",
            "Batch [1150/1563] - Loss: 0.4727, Avg Loss: 0.0343\n",
            "Batch [1160/1563] - Loss: 0.4957, Avg Loss: 0.0342\n",
            "Batch [1170/1563] - Loss: 0.5392, Avg Loss: 0.0342\n",
            "Batch [1180/1563] - Loss: 0.4455, Avg Loss: 0.0342\n",
            "Batch [1190/1563] - Loss: 0.4682, Avg Loss: 0.0342\n",
            "Batch [1200/1563] - Loss: 0.4919, Avg Loss: 0.0341\n",
            "Batch [1210/1563] - Loss: 0.4592, Avg Loss: 0.0341\n",
            "Batch [1220/1563] - Loss: 0.4799, Avg Loss: 0.0340\n",
            "Batch [1230/1563] - Loss: 0.3887, Avg Loss: 0.0340\n",
            "Batch [1240/1563] - Loss: 0.3799, Avg Loss: 0.0340\n",
            "Batch [1250/1563] - Loss: 0.5034, Avg Loss: 0.0339\n",
            "Batch [1260/1563] - Loss: 0.5468, Avg Loss: 0.0339\n",
            "Batch [1270/1563] - Loss: 0.5683, Avg Loss: 0.0339\n",
            "Batch [1280/1563] - Loss: 0.4861, Avg Loss: 0.0339\n",
            "Batch [1290/1563] - Loss: 0.4295, Avg Loss: 0.0339\n",
            "Batch [1300/1563] - Loss: 0.4633, Avg Loss: 0.0338\n",
            "Batch [1310/1563] - Loss: 0.4432, Avg Loss: 0.0338\n",
            "Batch [1320/1563] - Loss: 0.4865, Avg Loss: 0.0337\n",
            "Batch [1330/1563] - Loss: 0.5166, Avg Loss: 0.0337\n",
            "Batch [1340/1563] - Loss: 0.5119, Avg Loss: 0.0337\n",
            "Batch [1350/1563] - Loss: 0.4732, Avg Loss: 0.0337\n",
            "Batch [1360/1563] - Loss: 0.6136, Avg Loss: 0.0336\n",
            "Batch [1370/1563] - Loss: 0.5226, Avg Loss: 0.0336\n",
            "Batch [1380/1563] - Loss: 0.4445, Avg Loss: 0.0336\n",
            "Batch [1390/1563] - Loss: 0.4495, Avg Loss: 0.0335\n",
            "Batch [1400/1563] - Loss: 0.5561, Avg Loss: 0.0335\n",
            "Batch [1410/1563] - Loss: 0.3749, Avg Loss: 0.0335\n",
            "Batch [1420/1563] - Loss: 0.3883, Avg Loss: 0.0334\n",
            "Batch [1430/1563] - Loss: 0.3863, Avg Loss: 0.0334\n",
            "Batch [1440/1563] - Loss: 0.4384, Avg Loss: 0.0334\n",
            "Batch [1450/1563] - Loss: 0.4342, Avg Loss: 0.0333\n",
            "Batch [1460/1563] - Loss: 0.3928, Avg Loss: 0.0333\n",
            "Batch [1470/1563] - Loss: 0.5035, Avg Loss: 0.0333\n",
            "Batch [1480/1563] - Loss: 0.3561, Avg Loss: 0.0332\n",
            "Batch [1490/1563] - Loss: 0.4702, Avg Loss: 0.0332\n",
            "Batch [1500/1563] - Loss: 0.3695, Avg Loss: 0.0331\n",
            "Batch [1510/1563] - Loss: 0.4499, Avg Loss: 0.0331\n",
            "Batch [1520/1563] - Loss: 0.5512, Avg Loss: 0.0331\n",
            "Batch [1530/1563] - Loss: 0.3542, Avg Loss: 0.0330\n",
            "Batch [1540/1563] - Loss: 0.3237, Avg Loss: 0.0330\n",
            "Batch [1550/1563] - Loss: 0.3438, Avg Loss: 0.0330\n",
            "Batch [1560/1563] - Loss: 0.4012, Avg Loss: 0.0329\n",
            "Epoch completed - Total samples: 25000, Average Loss: 0.0329\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:43, 35.37it/s]Eval Batch [20/1563] - Current Accuracy: 0.8625\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.59it/s]Eval Batch [40/1563] - Current Accuracy: 0.8688\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 39.05it/s]Eval Batch [60/1563] - Current Accuracy: 0.8521\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:37, 39.12it/s]Eval Batch [80/1563] - Current Accuracy: 0.8602\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 38.96it/s]Eval Batch [100/1563] - Current Accuracy: 0.8662\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:37, 39.07it/s]Eval Batch [120/1563] - Current Accuracy: 0.8656\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 39.09it/s]Eval Batch [140/1563] - Current Accuracy: 0.8656\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:36, 39.05it/s]Eval Batch [160/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 38.93it/s]Eval Batch [180/1563] - Current Accuracy: 0.8566\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:35, 38.80it/s]Eval Batch [200/1563] - Current Accuracy: 0.8600\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 38.68it/s]Eval Batch [220/1563] - Current Accuracy: 0.8591\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:34, 38.55it/s]Eval Batch [240/1563] - Current Accuracy: 0.8607\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 38.91it/s]Eval Batch [260/1563] - Current Accuracy: 0.8625\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:32, 39.15it/s]Eval Batch [280/1563] - Current Accuracy: 0.8656\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 38.95it/s]Eval Batch [300/1563] - Current Accuracy: 0.8646\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:31, 39.08it/s]Eval Batch [320/1563] - Current Accuracy: 0.8658\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 39.09it/s]Eval Batch [340/1563] - Current Accuracy: 0.8642\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:30, 39.10it/s]Eval Batch [360/1563] - Current Accuracy: 0.8653\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 39.07it/s]Eval Batch [380/1563] - Current Accuracy: 0.8646\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:29, 39.09it/s]Eval Batch [400/1563] - Current Accuracy: 0.8628\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 39.11it/s]Eval Batch [420/1563] - Current Accuracy: 0.8634\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:29, 38.78it/s]Eval Batch [440/1563] - Current Accuracy: 0.8621\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 38.55it/s]Eval Batch [460/1563] - Current Accuracy: 0.8617\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:27, 38.96it/s]Eval Batch [480/1563] - Current Accuracy: 0.8626\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 38.50it/s]Eval Batch [500/1563] - Current Accuracy: 0.8624\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 38.82it/s]Eval Batch [520/1563] - Current Accuracy: 0.8624\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 38.65it/s]Eval Batch [540/1563] - Current Accuracy: 0.8620\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:26, 38.52it/s]Eval Batch [560/1563] - Current Accuracy: 0.8612\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 38.97it/s]Eval Batch [580/1563] - Current Accuracy: 0.8587\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 39.02it/s]Eval Batch [600/1563] - Current Accuracy: 0.8579\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 38.97it/s]Eval Batch [620/1563] - Current Accuracy: 0.8578\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 39.04it/s]Eval Batch [640/1563] - Current Accuracy: 0.8581\n",
            "Evaluating SNN:  42% 657/1563 [00:16<00:23, 39.00it/s]Eval Batch [660/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 38.99it/s]Eval Batch [680/1563] - Current Accuracy: 0.8603\n",
            "Evaluating SNN:  45% 697/1563 [00:18<00:22, 38.74it/s]Eval Batch [700/1563] - Current Accuracy: 0.8599\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 38.98it/s]Eval Batch [720/1563] - Current Accuracy: 0.8593\n",
            "Evaluating SNN:  47% 737/1563 [00:19<00:21, 39.10it/s]Eval Batch [740/1563] - Current Accuracy: 0.8590\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 39.17it/s]Eval Batch [760/1563] - Current Accuracy: 0.8594\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 39.02it/s]Eval Batch [780/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 38.98it/s]Eval Batch [800/1563] - Current Accuracy: 0.8582\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 38.88it/s]Eval Batch [820/1563] - Current Accuracy: 0.8579\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 38.77it/s]Eval Batch [840/1563] - Current Accuracy: 0.8553\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 38.83it/s]Eval Batch [860/1563] - Current Accuracy: 0.8547\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 38.98it/s]Eval Batch [880/1563] - Current Accuracy: 0.8528\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 39.17it/s]Eval Batch [900/1563] - Current Accuracy: 0.8515\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 39.17it/s]Eval Batch [920/1563] - Current Accuracy: 0.8514\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:16, 39.02it/s]Eval Batch [940/1563] - Current Accuracy: 0.8505\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 38.96it/s]Eval Batch [960/1563] - Current Accuracy: 0.8499\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:15, 38.98it/s]Eval Batch [980/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 38.98it/s]Eval Batch [1000/1563] - Current Accuracy: 0.8494\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:14, 38.73it/s]Eval Batch [1020/1563] - Current Accuracy: 0.8475\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 38.98it/s]Eval Batch [1040/1563] - Current Accuracy: 0.8462\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:13, 38.70it/s]Eval Batch [1060/1563] - Current Accuracy: 0.8452\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 39.00it/s]Eval Batch [1080/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:12, 38.73it/s]Eval Batch [1100/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 39.00it/s]Eval Batch [1120/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 38.91it/s]Eval Batch [1140/1563] - Current Accuracy: 0.8421\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 39.03it/s]Eval Batch [1160/1563] - Current Accuracy: 0.8409\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 39.02it/s]Eval Batch [1180/1563] - Current Accuracy: 0.8408\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 39.06it/s]Eval Batch [1200/1563] - Current Accuracy: 0.8402\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 39.06it/s]Eval Batch [1220/1563] - Current Accuracy: 0.8393\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 39.15it/s]Eval Batch [1240/1563] - Current Accuracy: 0.8387\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 39.11it/s]Eval Batch [1260/1563] - Current Accuracy: 0.8382\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 39.19it/s]Eval Batch [1280/1563] - Current Accuracy: 0.8387\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 39.16it/s]Eval Batch [1300/1563] - Current Accuracy: 0.8387\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 39.13it/s]Eval Batch [1320/1563] - Current Accuracy: 0.8379\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 39.03it/s]Eval Batch [1340/1563] - Current Accuracy: 0.8373\n",
            "Evaluating SNN:  87% 1357/1563 [00:34<00:05, 38.88it/s]Eval Batch [1360/1563] - Current Accuracy: 0.8372\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 38.94it/s]Eval Batch [1380/1563] - Current Accuracy: 0.8365\n",
            "Evaluating SNN:  89% 1397/1563 [00:35<00:04, 38.95it/s]Eval Batch [1400/1563] - Current Accuracy: 0.8352\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 38.86it/s]Eval Batch [1420/1563] - Current Accuracy: 0.8346\n",
            "Evaluating SNN:  92% 1437/1563 [00:36<00:03, 38.90it/s]Eval Batch [1440/1563] - Current Accuracy: 0.8342\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 38.92it/s]Eval Batch [1460/1563] - Current Accuracy: 0.8345\n",
            "Evaluating SNN:  94% 1477/1563 [00:38<00:02, 39.08it/s]Eval Batch [1480/1563] - Current Accuracy: 0.8347\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 38.73it/s]Eval Batch [1500/1563] - Current Accuracy: 0.8339\n",
            "Evaluating SNN:  97% 1517/1563 [00:39<00:01, 38.84it/s]Eval Batch [1520/1563] - Current Accuracy: 0.8337\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 38.93it/s]Eval Batch [1540/1563] - Current Accuracy: 0.8332\n",
            "Evaluating SNN: 100% 1557/1563 [00:40<00:00, 38.97it/s]Eval Batch [1560/1563] - Current Accuracy: 0.8324\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.83it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.8321\n",
            "INFO:root:Saved model weights at epoch 2 to: ./checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_2.pth\n",
            "[2025-09-24 17:59:07,962][main.py][line:639][INFO] Saved model weights at epoch 2 to: ./checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_2.pth\n",
            "INFO:root:Save directory exists: True\n",
            "[2025-09-24 17:59:07,963][main.py][line:640][INFO] Save directory exists: True\n",
            "INFO:root:File size: 265511595 bytes\n",
            "[2025-09-24 17:59:07,963][main.py][line:641][INFO] File size: 265511595 bytes\n",
            "INFO:root:SNNs training Epoch 2 [distilbert_base_qcfs]: Val_loss: 0.03292086598992348\n",
            "[2025-09-24 17:59:07,963][main.py][line:644][INFO] SNNs training Epoch 2 [distilbert_base_qcfs]: Val_loss: 0.03292086598992348\n",
            "INFO:root:SNNs training Epoch 2 [distilbert_base_qcfs]: Test Acc: 0.8321 Best Acc: 0.8321\n",
            "[2025-09-24 17:59:07,963][main.py][line:645][INFO] SNNs training Epoch 2 [distilbert_base_qcfs]: Test Acc: 0.8321 Best Acc: 0.8321\n",
            "Starting training with 1563 batches...\n",
            "Batch [10/1563] - Loss: 0.6499, Avg Loss: 0.0290\n",
            "Batch [20/1563] - Loss: 0.4736, Avg Loss: 0.0294\n",
            "Batch [30/1563] - Loss: 0.3961, Avg Loss: 0.0284\n",
            "Batch [40/1563] - Loss: 0.2943, Avg Loss: 0.0282\n",
            "Batch [50/1563] - Loss: 0.4372, Avg Loss: 0.0281\n",
            "Batch [60/1563] - Loss: 0.3443, Avg Loss: 0.0276\n",
            "Batch [70/1563] - Loss: 0.3393, Avg Loss: 0.0274\n",
            "Batch [80/1563] - Loss: 0.3660, Avg Loss: 0.0272\n",
            "Batch [90/1563] - Loss: 0.6048, Avg Loss: 0.0274\n",
            "Batch [100/1563] - Loss: 0.6488, Avg Loss: 0.0277\n",
            "Batch [110/1563] - Loss: 0.5413, Avg Loss: 0.0279\n",
            "Batch [120/1563] - Loss: 0.4674, Avg Loss: 0.0280\n",
            "Batch [130/1563] - Loss: 0.4905, Avg Loss: 0.0280\n",
            "Batch [140/1563] - Loss: 0.3012, Avg Loss: 0.0280\n",
            "Batch [150/1563] - Loss: 0.5704, Avg Loss: 0.0281\n",
            "Batch [160/1563] - Loss: 0.4565, Avg Loss: 0.0281\n",
            "Batch [170/1563] - Loss: 0.3470, Avg Loss: 0.0279\n",
            "Batch [180/1563] - Loss: 0.5994, Avg Loss: 0.0278\n",
            "Batch [190/1563] - Loss: 0.5992, Avg Loss: 0.0278\n",
            "Batch [200/1563] - Loss: 0.4551, Avg Loss: 0.0277\n",
            "Batch [210/1563] - Loss: 0.2323, Avg Loss: 0.0276\n",
            "Batch [220/1563] - Loss: 0.3830, Avg Loss: 0.0276\n",
            "Batch [230/1563] - Loss: 0.3936, Avg Loss: 0.0277\n",
            "Batch [240/1563] - Loss: 0.6467, Avg Loss: 0.0277\n",
            "Batch [250/1563] - Loss: 0.5711, Avg Loss: 0.0278\n",
            "Batch [260/1563] - Loss: 0.3405, Avg Loss: 0.0278\n",
            "Batch [270/1563] - Loss: 0.4874, Avg Loss: 0.0278\n",
            "Batch [280/1563] - Loss: 0.3451, Avg Loss: 0.0277\n",
            "Batch [290/1563] - Loss: 0.3548, Avg Loss: 0.0276\n",
            "Batch [300/1563] - Loss: 0.2626, Avg Loss: 0.0276\n",
            "Batch [310/1563] - Loss: 0.4558, Avg Loss: 0.0275\n",
            "Batch [320/1563] - Loss: 0.5003, Avg Loss: 0.0276\n",
            "Batch [330/1563] - Loss: 0.4795, Avg Loss: 0.0276\n",
            "Batch [340/1563] - Loss: 0.4418, Avg Loss: 0.0276\n",
            "Batch [350/1563] - Loss: 0.4057, Avg Loss: 0.0275\n",
            "Batch [360/1563] - Loss: 0.3593, Avg Loss: 0.0275\n",
            "Batch [370/1563] - Loss: 0.4999, Avg Loss: 0.0275\n",
            "Batch [380/1563] - Loss: 0.6301, Avg Loss: 0.0276\n",
            "Batch [390/1563] - Loss: 0.3341, Avg Loss: 0.0275\n",
            "Batch [400/1563] - Loss: 0.4673, Avg Loss: 0.0274\n",
            "Batch [410/1563] - Loss: 0.4161, Avg Loss: 0.0274\n",
            "Batch [420/1563] - Loss: 0.3617, Avg Loss: 0.0274\n",
            "Batch [430/1563] - Loss: 0.4328, Avg Loss: 0.0273\n",
            "Batch [440/1563] - Loss: 0.4263, Avg Loss: 0.0273\n",
            "Batch [450/1563] - Loss: 0.5884, Avg Loss: 0.0273\n",
            "Batch [460/1563] - Loss: 0.2961, Avg Loss: 0.0273\n",
            "Batch [470/1563] - Loss: 0.4627, Avg Loss: 0.0272\n",
            "Batch [480/1563] - Loss: 0.5937, Avg Loss: 0.0273\n",
            "Batch [490/1563] - Loss: 0.6122, Avg Loss: 0.0273\n",
            "Batch [500/1563] - Loss: 0.3038, Avg Loss: 0.0272\n",
            "Batch [510/1563] - Loss: 0.4194, Avg Loss: 0.0272\n",
            "Batch [520/1563] - Loss: 0.4526, Avg Loss: 0.0272\n",
            "Batch [530/1563] - Loss: 0.4932, Avg Loss: 0.0271\n",
            "Batch [540/1563] - Loss: 0.3395, Avg Loss: 0.0272\n",
            "Batch [550/1563] - Loss: 0.4370, Avg Loss: 0.0272\n",
            "Batch [560/1563] - Loss: 0.4344, Avg Loss: 0.0272\n",
            "Batch [570/1563] - Loss: 0.3693, Avg Loss: 0.0272\n",
            "Batch [580/1563] - Loss: 0.5364, Avg Loss: 0.0272\n",
            "Batch [590/1563] - Loss: 0.3327, Avg Loss: 0.0272\n",
            "Batch [600/1563] - Loss: 0.4063, Avg Loss: 0.0272\n",
            "Batch [610/1563] - Loss: 0.4774, Avg Loss: 0.0272\n",
            "Batch [620/1563] - Loss: 0.3835, Avg Loss: 0.0272\n",
            "Batch [630/1563] - Loss: 0.5508, Avg Loss: 0.0272\n",
            "Batch [640/1563] - Loss: 0.4962, Avg Loss: 0.0272\n",
            "Batch [650/1563] - Loss: 0.3617, Avg Loss: 0.0271\n",
            "Batch [660/1563] - Loss: 0.5186, Avg Loss: 0.0271\n",
            "Batch [670/1563] - Loss: 0.3796, Avg Loss: 0.0271\n",
            "Batch [680/1563] - Loss: 0.2975, Avg Loss: 0.0271\n",
            "Batch [690/1563] - Loss: 0.5392, Avg Loss: 0.0270\n",
            "Batch [700/1563] - Loss: 0.5030, Avg Loss: 0.0270\n",
            "Batch [710/1563] - Loss: 0.4861, Avg Loss: 0.0271\n",
            "Batch [720/1563] - Loss: 0.4462, Avg Loss: 0.0271\n",
            "Batch [730/1563] - Loss: 0.3675, Avg Loss: 0.0271\n",
            "Batch [740/1563] - Loss: 0.4699, Avg Loss: 0.0271\n",
            "Batch [750/1563] - Loss: 0.2631, Avg Loss: 0.0271\n",
            "Batch [760/1563] - Loss: 0.4714, Avg Loss: 0.0271\n",
            "Batch [770/1563] - Loss: 0.4314, Avg Loss: 0.0270\n",
            "Batch [780/1563] - Loss: 0.3832, Avg Loss: 0.0271\n",
            "Batch [790/1563] - Loss: 0.4020, Avg Loss: 0.0271\n",
            "Batch [800/1563] - Loss: 0.4159, Avg Loss: 0.0271\n",
            "Batch [810/1563] - Loss: 0.4002, Avg Loss: 0.0270\n",
            "Batch [820/1563] - Loss: 0.5626, Avg Loss: 0.0270\n",
            "Batch [830/1563] - Loss: 0.3382, Avg Loss: 0.0270\n",
            "Batch [840/1563] - Loss: 0.3123, Avg Loss: 0.0269\n",
            "Batch [850/1563] - Loss: 0.2766, Avg Loss: 0.0269\n",
            "Batch [860/1563] - Loss: 0.3186, Avg Loss: 0.0269\n",
            "Batch [870/1563] - Loss: 0.3551, Avg Loss: 0.0269\n",
            "Batch [880/1563] - Loss: 0.3255, Avg Loss: 0.0269\n",
            "Batch [890/1563] - Loss: 0.3927, Avg Loss: 0.0269\n",
            "Batch [900/1563] - Loss: 0.4091, Avg Loss: 0.0268\n",
            "Batch [910/1563] - Loss: 0.4475, Avg Loss: 0.0268\n",
            "Batch [920/1563] - Loss: 0.3370, Avg Loss: 0.0268\n",
            "Batch [930/1563] - Loss: 0.4246, Avg Loss: 0.0267\n",
            "Batch [940/1563] - Loss: 0.5265, Avg Loss: 0.0267\n",
            "Batch [950/1563] - Loss: 0.5137, Avg Loss: 0.0267\n",
            "Batch [960/1563] - Loss: 0.2768, Avg Loss: 0.0267\n",
            "Batch [970/1563] - Loss: 0.4283, Avg Loss: 0.0266\n",
            "Batch [980/1563] - Loss: 0.3204, Avg Loss: 0.0266\n",
            "Batch [990/1563] - Loss: 0.3893, Avg Loss: 0.0266\n",
            "Batch [1000/1563] - Loss: 0.3467, Avg Loss: 0.0266\n",
            "Batch [1010/1563] - Loss: 0.3139, Avg Loss: 0.0266\n",
            "Batch [1020/1563] - Loss: 0.5766, Avg Loss: 0.0266\n",
            "Batch [1030/1563] - Loss: 0.3577, Avg Loss: 0.0266\n",
            "Batch [1040/1563] - Loss: 0.2725, Avg Loss: 0.0265\n",
            "Batch [1050/1563] - Loss: 0.5307, Avg Loss: 0.0265\n",
            "Batch [1060/1563] - Loss: 0.3825, Avg Loss: 0.0265\n",
            "Batch [1070/1563] - Loss: 0.5167, Avg Loss: 0.0265\n",
            "Batch [1080/1563] - Loss: 0.2617, Avg Loss: 0.0264\n",
            "Batch [1090/1563] - Loss: 0.5328, Avg Loss: 0.0264\n",
            "Batch [1100/1563] - Loss: 0.4841, Avg Loss: 0.0264\n",
            "Batch [1110/1563] - Loss: 0.2937, Avg Loss: 0.0264\n",
            "Batch [1120/1563] - Loss: 0.4444, Avg Loss: 0.0264\n",
            "Batch [1130/1563] - Loss: 0.4353, Avg Loss: 0.0264\n",
            "Batch [1140/1563] - Loss: 0.5136, Avg Loss: 0.0263\n",
            "Batch [1150/1563] - Loss: 0.3535, Avg Loss: 0.0263\n",
            "Batch [1160/1563] - Loss: 0.3120, Avg Loss: 0.0263\n",
            "Batch [1170/1563] - Loss: 0.3043, Avg Loss: 0.0263\n",
            "Batch [1180/1563] - Loss: 0.4014, Avg Loss: 0.0263\n",
            "Batch [1190/1563] - Loss: 0.3758, Avg Loss: 0.0263\n",
            "Batch [1200/1563] - Loss: 0.4403, Avg Loss: 0.0262\n",
            "Batch [1210/1563] - Loss: 0.2311, Avg Loss: 0.0262\n",
            "Batch [1220/1563] - Loss: 0.2463, Avg Loss: 0.0262\n",
            "Batch [1230/1563] - Loss: 0.2525, Avg Loss: 0.0262\n",
            "Batch [1240/1563] - Loss: 0.4874, Avg Loss: 0.0262\n",
            "Batch [1250/1563] - Loss: 0.3049, Avg Loss: 0.0262\n",
            "Batch [1260/1563] - Loss: 0.4150, Avg Loss: 0.0262\n",
            "Batch [1270/1563] - Loss: 0.3525, Avg Loss: 0.0262\n",
            "Batch [1280/1563] - Loss: 0.3296, Avg Loss: 0.0262\n",
            "Batch [1290/1563] - Loss: 0.6080, Avg Loss: 0.0262\n",
            "Batch [1300/1563] - Loss: 0.2750, Avg Loss: 0.0261\n",
            "Batch [1310/1563] - Loss: 0.3538, Avg Loss: 0.0261\n",
            "Batch [1320/1563] - Loss: 0.4164, Avg Loss: 0.0261\n",
            "Batch [1330/1563] - Loss: 0.4853, Avg Loss: 0.0261\n",
            "Batch [1340/1563] - Loss: 0.4184, Avg Loss: 0.0261\n",
            "Batch [1350/1563] - Loss: 0.3105, Avg Loss: 0.0260\n",
            "Batch [1360/1563] - Loss: 0.6012, Avg Loss: 0.0261\n",
            "Batch [1370/1563] - Loss: 0.2462, Avg Loss: 0.0261\n",
            "Batch [1380/1563] - Loss: 0.3556, Avg Loss: 0.0260\n",
            "Batch [1390/1563] - Loss: 0.3497, Avg Loss: 0.0260\n",
            "Batch [1400/1563] - Loss: 0.5871, Avg Loss: 0.0260\n",
            "Batch [1410/1563] - Loss: 0.4532, Avg Loss: 0.0260\n",
            "Batch [1420/1563] - Loss: 0.5252, Avg Loss: 0.0261\n",
            "Batch [1430/1563] - Loss: 0.4083, Avg Loss: 0.0260\n",
            "Batch [1440/1563] - Loss: 0.2293, Avg Loss: 0.0260\n",
            "Batch [1450/1563] - Loss: 0.4698, Avg Loss: 0.0260\n",
            "Batch [1460/1563] - Loss: 0.2895, Avg Loss: 0.0260\n",
            "Batch [1470/1563] - Loss: 0.3124, Avg Loss: 0.0260\n",
            "Batch [1480/1563] - Loss: 0.3443, Avg Loss: 0.0260\n",
            "Batch [1490/1563] - Loss: 0.2837, Avg Loss: 0.0260\n",
            "Batch [1500/1563] - Loss: 0.3368, Avg Loss: 0.0259\n",
            "Batch [1510/1563] - Loss: 0.2563, Avg Loss: 0.0259\n",
            "Batch [1520/1563] - Loss: 0.4509, Avg Loss: 0.0259\n",
            "Batch [1530/1563] - Loss: 0.3001, Avg Loss: 0.0259\n",
            "Batch [1540/1563] - Loss: 0.3133, Avg Loss: 0.0259\n",
            "Batch [1550/1563] - Loss: 0.4159, Avg Loss: 0.0259\n",
            "Batch [1560/1563] - Loss: 0.4811, Avg Loss: 0.0259\n",
            "Epoch completed - Total samples: 25000, Average Loss: 0.0259\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:43, 35.28it/s]Eval Batch [20/1563] - Current Accuracy: 0.8281\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.29it/s]Eval Batch [40/1563] - Current Accuracy: 0.8359\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 38.95it/s]Eval Batch [60/1563] - Current Accuracy: 0.8260\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:38, 39.10it/s]Eval Batch [80/1563] - Current Accuracy: 0.8352\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 39.02it/s]Eval Batch [100/1563] - Current Accuracy: 0.8431\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:36, 39.11it/s]Eval Batch [120/1563] - Current Accuracy: 0.8438\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 38.85it/s]Eval Batch [140/1563] - Current Accuracy: 0.8442\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:36, 39.01it/s]Eval Batch [160/1563] - Current Accuracy: 0.8344\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 39.00it/s]Eval Batch [180/1563] - Current Accuracy: 0.8354\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:34, 39.17it/s]Eval Batch [200/1563] - Current Accuracy: 0.8394\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 39.08it/s]Eval Batch [220/1563] - Current Accuracy: 0.8401\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:33, 39.08it/s]Eval Batch [240/1563] - Current Accuracy: 0.8430\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 39.15it/s]Eval Batch [260/1563] - Current Accuracy: 0.8445\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:32, 39.21it/s]Eval Batch [280/1563] - Current Accuracy: 0.8462\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 39.16it/s]Eval Batch [300/1563] - Current Accuracy: 0.8454\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:31, 39.20it/s]Eval Batch [320/1563] - Current Accuracy: 0.8465\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 39.25it/s]Eval Batch [340/1563] - Current Accuracy: 0.8458\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:30, 38.98it/s]Eval Batch [360/1563] - Current Accuracy: 0.8458\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 38.92it/s]Eval Batch [380/1563] - Current Accuracy: 0.8442\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:29, 38.88it/s]Eval Batch [400/1563] - Current Accuracy: 0.8441\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 38.94it/s]Eval Batch [420/1563] - Current Accuracy: 0.8454\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:28, 38.84it/s]Eval Batch [440/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 38.96it/s]Eval Batch [460/1563] - Current Accuracy: 0.8423\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:27, 38.98it/s]Eval Batch [480/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 39.06it/s]Eval Batch [500/1563] - Current Accuracy: 0.8434\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 38.95it/s]Eval Batch [520/1563] - Current Accuracy: 0.8431\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 38.76it/s]Eval Batch [540/1563] - Current Accuracy: 0.8428\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 38.81it/s]Eval Batch [560/1563] - Current Accuracy: 0.8417\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 38.72it/s]Eval Batch [580/1563] - Current Accuracy: 0.8391\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 38.68it/s]Eval Batch [600/1563] - Current Accuracy: 0.8375\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 38.78it/s]Eval Batch [620/1563] - Current Accuracy: 0.8371\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 39.08it/s]Eval Batch [640/1563] - Current Accuracy: 0.8375\n",
            "Evaluating SNN:  42% 657/1563 [00:16<00:23, 39.22it/s]Eval Batch [660/1563] - Current Accuracy: 0.8384\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 39.17it/s]Eval Batch [680/1563] - Current Accuracy: 0.8398\n",
            "Evaluating SNN:  45% 697/1563 [00:17<00:22, 39.20it/s]Eval Batch [700/1563] - Current Accuracy: 0.8396\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 39.10it/s]Eval Batch [720/1563] - Current Accuracy: 0.8388\n",
            "Evaluating SNN:  47% 737/1563 [00:19<00:21, 39.26it/s]Eval Batch [740/1563] - Current Accuracy: 0.8387\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 39.12it/s]Eval Batch [760/1563] - Current Accuracy: 0.8390\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 38.87it/s]Eval Batch [780/1563] - Current Accuracy: 0.8387\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 38.97it/s]Eval Batch [800/1563] - Current Accuracy: 0.8394\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 39.18it/s]Eval Batch [820/1563] - Current Accuracy: 0.8405\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 39.11it/s]Eval Batch [840/1563] - Current Accuracy: 0.8398\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 38.91it/s]Eval Batch [860/1563] - Current Accuracy: 0.8412\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 38.94it/s]Eval Batch [880/1563] - Current Accuracy: 0.8403\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 39.00it/s]Eval Batch [900/1563] - Current Accuracy: 0.8403\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 38.97it/s]Eval Batch [920/1563] - Current Accuracy: 0.8411\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:16, 38.97it/s]Eval Batch [940/1563] - Current Accuracy: 0.8418\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 39.06it/s]Eval Batch [960/1563] - Current Accuracy: 0.8419\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:15, 39.04it/s]Eval Batch [980/1563] - Current Accuracy: 0.8426\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 39.04it/s]Eval Batch [1000/1563] - Current Accuracy: 0.8437\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:13, 39.11it/s]Eval Batch [1020/1563] - Current Accuracy: 0.8432\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 38.97it/s]Eval Batch [1040/1563] - Current Accuracy: 0.8427\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:12, 39.16it/s]Eval Batch [1060/1563] - Current Accuracy: 0.8422\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 39.13it/s]Eval Batch [1080/1563] - Current Accuracy: 0.8413\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:11, 39.11it/s]Eval Batch [1100/1563] - Current Accuracy: 0.8417\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 39.10it/s]Eval Batch [1120/1563] - Current Accuracy: 0.8425\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 39.13it/s]Eval Batch [1140/1563] - Current Accuracy: 0.8420\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 39.07it/s]Eval Batch [1160/1563] - Current Accuracy: 0.8418\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 39.15it/s]Eval Batch [1180/1563] - Current Accuracy: 0.8426\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 39.05it/s]Eval Batch [1200/1563] - Current Accuracy: 0.8427\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 39.19it/s]Eval Batch [1220/1563] - Current Accuracy: 0.8427\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 39.15it/s]Eval Batch [1240/1563] - Current Accuracy: 0.8428\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 38.98it/s]Eval Batch [1260/1563] - Current Accuracy: 0.8430\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 39.17it/s]Eval Batch [1280/1563] - Current Accuracy: 0.8437\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 39.21it/s]Eval Batch [1300/1563] - Current Accuracy: 0.8442\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 39.18it/s]Eval Batch [1320/1563] - Current Accuracy: 0.8441\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 39.09it/s]Eval Batch [1340/1563] - Current Accuracy: 0.8440\n",
            "Evaluating SNN:  87% 1357/1563 [00:34<00:05, 39.17it/s]Eval Batch [1360/1563] - Current Accuracy: 0.8448\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 39.00it/s]Eval Batch [1380/1563] - Current Accuracy: 0.8447\n",
            "Evaluating SNN:  89% 1397/1563 [00:35<00:04, 39.09it/s]Eval Batch [1400/1563] - Current Accuracy: 0.8440\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 39.08it/s]Eval Batch [1420/1563] - Current Accuracy: 0.8441\n",
            "Evaluating SNN:  92% 1437/1563 [00:36<00:03, 39.16it/s]Eval Batch [1440/1563] - Current Accuracy: 0.8442\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 39.17it/s]Eval Batch [1460/1563] - Current Accuracy: 0.8447\n",
            "Evaluating SNN:  94% 1477/1563 [00:37<00:02, 39.14it/s]Eval Batch [1480/1563] - Current Accuracy: 0.8451\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 38.86it/s]Eval Batch [1500/1563] - Current Accuracy: 0.8448\n",
            "Evaluating SNN:  97% 1517/1563 [00:38<00:01, 39.11it/s]Eval Batch [1520/1563] - Current Accuracy: 0.8450\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 39.15it/s]Eval Batch [1540/1563] - Current Accuracy: 0.8450\n",
            "Evaluating SNN: 100% 1557/1563 [00:39<00:00, 39.09it/s]Eval Batch [1560/1563] - Current Accuracy: 0.8443\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.91it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.8442\n",
            "INFO:root:Saved model weights at epoch 3 to: ./checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_3.pth\n",
            "[2025-09-24 18:01:52,190][main.py][line:639][INFO] Saved model weights at epoch 3 to: ./checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_3.pth\n",
            "INFO:root:Save directory exists: True\n",
            "[2025-09-24 18:01:52,191][main.py][line:640][INFO] Save directory exists: True\n",
            "INFO:root:File size: 265511595 bytes\n",
            "[2025-09-24 18:01:52,191][main.py][line:641][INFO] File size: 265511595 bytes\n",
            "INFO:root:SNNs training Epoch 3 [distilbert_base_qcfs]: Val_loss: 0.025914347260594366\n",
            "[2025-09-24 18:01:52,191][main.py][line:644][INFO] SNNs training Epoch 3 [distilbert_base_qcfs]: Val_loss: 0.025914347260594366\n",
            "INFO:root:SNNs training Epoch 3 [distilbert_base_qcfs]: Test Acc: 0.8442 Best Acc: 0.8442\n",
            "[2025-09-24 18:01:52,191][main.py][line:645][INFO] SNNs training Epoch 3 [distilbert_base_qcfs]: Test Acc: 0.8442 Best Acc: 0.8442\n",
            "Starting training with 1563 batches...\n",
            "Batch [10/1563] - Loss: 0.3407, Avg Loss: 0.0223\n",
            "Batch [20/1563] - Loss: 0.2979, Avg Loss: 0.0227\n",
            "Batch [30/1563] - Loss: 0.5220, Avg Loss: 0.0232\n",
            "Batch [40/1563] - Loss: 0.4249, Avg Loss: 0.0231\n",
            "Batch [50/1563] - Loss: 0.4997, Avg Loss: 0.0243\n",
            "Batch [60/1563] - Loss: 0.2994, Avg Loss: 0.0239\n",
            "Batch [70/1563] - Loss: 0.3533, Avg Loss: 0.0246\n",
            "Batch [80/1563] - Loss: 0.4220, Avg Loss: 0.0245\n",
            "Batch [90/1563] - Loss: 0.3226, Avg Loss: 0.0244\n",
            "Batch [100/1563] - Loss: 0.3199, Avg Loss: 0.0243\n",
            "Batch [110/1563] - Loss: 0.4844, Avg Loss: 0.0243\n",
            "Batch [120/1563] - Loss: 0.3082, Avg Loss: 0.0239\n",
            "Batch [130/1563] - Loss: 0.3415, Avg Loss: 0.0238\n",
            "Batch [140/1563] - Loss: 0.5420, Avg Loss: 0.0237\n",
            "Batch [150/1563] - Loss: 0.5129, Avg Loss: 0.0239\n",
            "Batch [160/1563] - Loss: 0.5072, Avg Loss: 0.0239\n",
            "Batch [170/1563] - Loss: 0.3610, Avg Loss: 0.0239\n",
            "Batch [180/1563] - Loss: 0.2751, Avg Loss: 0.0238\n",
            "Batch [190/1563] - Loss: 0.4133, Avg Loss: 0.0238\n",
            "Batch [200/1563] - Loss: 0.6535, Avg Loss: 0.0239\n",
            "Batch [210/1563] - Loss: 0.4155, Avg Loss: 0.0240\n",
            "Batch [220/1563] - Loss: 0.2513, Avg Loss: 0.0241\n",
            "Batch [230/1563] - Loss: 0.3715, Avg Loss: 0.0240\n",
            "Batch [240/1563] - Loss: 0.6575, Avg Loss: 0.0241\n",
            "Batch [250/1563] - Loss: 0.4505, Avg Loss: 0.0242\n",
            "Batch [260/1563] - Loss: 0.4505, Avg Loss: 0.0242\n",
            "Batch [270/1563] - Loss: 0.3807, Avg Loss: 0.0242\n",
            "Batch [280/1563] - Loss: 0.3148, Avg Loss: 0.0241\n",
            "Batch [290/1563] - Loss: 0.4323, Avg Loss: 0.0241\n",
            "Batch [300/1563] - Loss: 0.2559, Avg Loss: 0.0241\n",
            "Batch [310/1563] - Loss: 0.4100, Avg Loss: 0.0241\n",
            "Batch [320/1563] - Loss: 0.3043, Avg Loss: 0.0241\n",
            "Batch [330/1563] - Loss: 0.2755, Avg Loss: 0.0240\n",
            "Batch [340/1563] - Loss: 0.3133, Avg Loss: 0.0241\n",
            "Batch [350/1563] - Loss: 0.3464, Avg Loss: 0.0241\n",
            "Batch [360/1563] - Loss: 0.4174, Avg Loss: 0.0241\n",
            "Batch [370/1563] - Loss: 0.2882, Avg Loss: 0.0240\n",
            "Batch [380/1563] - Loss: 0.4410, Avg Loss: 0.0240\n",
            "Batch [390/1563] - Loss: 0.2966, Avg Loss: 0.0240\n",
            "Batch [400/1563] - Loss: 0.2876, Avg Loss: 0.0241\n",
            "Batch [410/1563] - Loss: 0.5072, Avg Loss: 0.0241\n",
            "Batch [420/1563] - Loss: 0.3758, Avg Loss: 0.0241\n",
            "Batch [430/1563] - Loss: 0.2245, Avg Loss: 0.0242\n",
            "Batch [440/1563] - Loss: 0.3972, Avg Loss: 0.0241\n",
            "Batch [450/1563] - Loss: 0.5047, Avg Loss: 0.0242\n",
            "Batch [460/1563] - Loss: 0.3249, Avg Loss: 0.0242\n",
            "Batch [470/1563] - Loss: 0.4560, Avg Loss: 0.0242\n",
            "Batch [480/1563] - Loss: 0.4559, Avg Loss: 0.0242\n",
            "Batch [490/1563] - Loss: 0.3570, Avg Loss: 0.0242\n",
            "Batch [500/1563] - Loss: 0.3450, Avg Loss: 0.0241\n",
            "Batch [510/1563] - Loss: 0.4144, Avg Loss: 0.0241\n",
            "Batch [520/1563] - Loss: 0.5205, Avg Loss: 0.0241\n",
            "Batch [530/1563] - Loss: 0.2518, Avg Loss: 0.0241\n",
            "Batch [540/1563] - Loss: 0.6340, Avg Loss: 0.0242\n",
            "Batch [550/1563] - Loss: 0.4704, Avg Loss: 0.0242\n",
            "Batch [560/1563] - Loss: 0.1949, Avg Loss: 0.0242\n",
            "Batch [570/1563] - Loss: 0.2839, Avg Loss: 0.0242\n",
            "Batch [580/1563] - Loss: 0.3947, Avg Loss: 0.0242\n",
            "Batch [590/1563] - Loss: 0.4345, Avg Loss: 0.0242\n",
            "Batch [600/1563] - Loss: 0.3928, Avg Loss: 0.0242\n",
            "Batch [610/1563] - Loss: 0.3690, Avg Loss: 0.0242\n",
            "Batch [620/1563] - Loss: 0.4232, Avg Loss: 0.0242\n",
            "Batch [630/1563] - Loss: 0.4796, Avg Loss: 0.0242\n",
            "Batch [640/1563] - Loss: 0.4680, Avg Loss: 0.0243\n",
            "Batch [650/1563] - Loss: 0.4091, Avg Loss: 0.0243\n",
            "Batch [660/1563] - Loss: 0.4742, Avg Loss: 0.0243\n",
            "Batch [670/1563] - Loss: 0.5659, Avg Loss: 0.0243\n",
            "Batch [680/1563] - Loss: 0.4463, Avg Loss: 0.0243\n",
            "Batch [690/1563] - Loss: 0.3568, Avg Loss: 0.0243\n",
            "Batch [700/1563] - Loss: 0.4095, Avg Loss: 0.0244\n",
            "Batch [710/1563] - Loss: 0.2041, Avg Loss: 0.0243\n",
            "Batch [720/1563] - Loss: 0.4321, Avg Loss: 0.0243\n",
            "Batch [730/1563] - Loss: 0.6009, Avg Loss: 0.0244\n",
            "Batch [740/1563] - Loss: 0.2401, Avg Loss: 0.0243\n",
            "Batch [750/1563] - Loss: 0.4535, Avg Loss: 0.0243\n",
            "Batch [760/1563] - Loss: 0.3036, Avg Loss: 0.0243\n",
            "Batch [770/1563] - Loss: 0.4561, Avg Loss: 0.0243\n",
            "Batch [780/1563] - Loss: 0.4249, Avg Loss: 0.0242\n",
            "Batch [790/1563] - Loss: 0.3127, Avg Loss: 0.0243\n",
            "Batch [800/1563] - Loss: 0.4089, Avg Loss: 0.0242\n",
            "Batch [810/1563] - Loss: 0.5382, Avg Loss: 0.0242\n",
            "Batch [820/1563] - Loss: 0.4368, Avg Loss: 0.0243\n",
            "Batch [830/1563] - Loss: 0.3901, Avg Loss: 0.0243\n",
            "Batch [840/1563] - Loss: 0.5166, Avg Loss: 0.0243\n",
            "Batch [850/1563] - Loss: 0.3586, Avg Loss: 0.0243\n",
            "Batch [860/1563] - Loss: 0.3964, Avg Loss: 0.0243\n",
            "Batch [870/1563] - Loss: 0.3413, Avg Loss: 0.0243\n",
            "Batch [880/1563] - Loss: 0.2761, Avg Loss: 0.0243\n",
            "Batch [890/1563] - Loss: 0.4711, Avg Loss: 0.0243\n",
            "Batch [900/1563] - Loss: 0.3215, Avg Loss: 0.0243\n",
            "Batch [910/1563] - Loss: 0.2783, Avg Loss: 0.0243\n",
            "Batch [920/1563] - Loss: 0.3171, Avg Loss: 0.0243\n",
            "Batch [930/1563] - Loss: 0.3477, Avg Loss: 0.0243\n",
            "Batch [940/1563] - Loss: 0.5825, Avg Loss: 0.0243\n",
            "Batch [950/1563] - Loss: 0.3769, Avg Loss: 0.0243\n",
            "Batch [960/1563] - Loss: 0.4846, Avg Loss: 0.0243\n",
            "Batch [970/1563] - Loss: 0.2245, Avg Loss: 0.0243\n",
            "Batch [980/1563] - Loss: 0.3507, Avg Loss: 0.0243\n",
            "Batch [990/1563] - Loss: 0.3131, Avg Loss: 0.0243\n",
            "Batch [1000/1563] - Loss: 0.4192, Avg Loss: 0.0243\n",
            "Batch [1010/1563] - Loss: 0.3541, Avg Loss: 0.0243\n",
            "Batch [1020/1563] - Loss: 0.3151, Avg Loss: 0.0243\n",
            "Batch [1030/1563] - Loss: 0.3196, Avg Loss: 0.0243\n",
            "Batch [1040/1563] - Loss: 0.2184, Avg Loss: 0.0243\n",
            "Batch [1050/1563] - Loss: 0.3987, Avg Loss: 0.0243\n",
            "Batch [1060/1563] - Loss: 0.1999, Avg Loss: 0.0243\n",
            "Batch [1070/1563] - Loss: 0.3533, Avg Loss: 0.0243\n",
            "Batch [1080/1563] - Loss: 0.3946, Avg Loss: 0.0243\n",
            "Batch [1090/1563] - Loss: 0.3160, Avg Loss: 0.0243\n",
            "Batch [1100/1563] - Loss: 0.3067, Avg Loss: 0.0243\n",
            "Batch [1110/1563] - Loss: 0.3597, Avg Loss: 0.0243\n",
            "Batch [1120/1563] - Loss: 0.4935, Avg Loss: 0.0243\n",
            "Batch [1130/1563] - Loss: 0.4479, Avg Loss: 0.0242\n",
            "Batch [1140/1563] - Loss: 0.5533, Avg Loss: 0.0242\n",
            "Batch [1150/1563] - Loss: 0.3770, Avg Loss: 0.0242\n",
            "Batch [1160/1563] - Loss: 0.3236, Avg Loss: 0.0242\n",
            "Batch [1170/1563] - Loss: 0.2643, Avg Loss: 0.0242\n",
            "Batch [1180/1563] - Loss: 0.5342, Avg Loss: 0.0242\n",
            "Batch [1190/1563] - Loss: 0.4358, Avg Loss: 0.0242\n",
            "Batch [1200/1563] - Loss: 0.3201, Avg Loss: 0.0242\n",
            "Batch [1210/1563] - Loss: 0.3075, Avg Loss: 0.0242\n",
            "Batch [1220/1563] - Loss: 0.2568, Avg Loss: 0.0242\n",
            "Batch [1230/1563] - Loss: 0.2342, Avg Loss: 0.0241\n",
            "Batch [1240/1563] - Loss: 0.3789, Avg Loss: 0.0242\n",
            "Batch [1250/1563] - Loss: 0.4253, Avg Loss: 0.0242\n",
            "Batch [1260/1563] - Loss: 0.4360, Avg Loss: 0.0242\n",
            "Batch [1270/1563] - Loss: 0.3488, Avg Loss: 0.0241\n",
            "Batch [1280/1563] - Loss: 0.2883, Avg Loss: 0.0241\n",
            "Batch [1290/1563] - Loss: 0.5812, Avg Loss: 0.0241\n",
            "Batch [1300/1563] - Loss: 0.2427, Avg Loss: 0.0241\n",
            "Batch [1310/1563] - Loss: 0.3027, Avg Loss: 0.0241\n",
            "Batch [1320/1563] - Loss: 0.4042, Avg Loss: 0.0241\n",
            "Batch [1330/1563] - Loss: 0.2114, Avg Loss: 0.0241\n",
            "Batch [1340/1563] - Loss: 0.2333, Avg Loss: 0.0241\n",
            "Batch [1350/1563] - Loss: 0.5497, Avg Loss: 0.0241\n",
            "Batch [1360/1563] - Loss: 0.3573, Avg Loss: 0.0241\n",
            "Batch [1370/1563] - Loss: 0.4309, Avg Loss: 0.0241\n",
            "Batch [1380/1563] - Loss: 0.3133, Avg Loss: 0.0241\n",
            "Batch [1390/1563] - Loss: 0.3742, Avg Loss: 0.0241\n",
            "Batch [1400/1563] - Loss: 0.4116, Avg Loss: 0.0241\n",
            "Batch [1410/1563] - Loss: 0.3679, Avg Loss: 0.0241\n",
            "Batch [1420/1563] - Loss: 0.7812, Avg Loss: 0.0241\n",
            "Batch [1430/1563] - Loss: 0.6656, Avg Loss: 0.0241\n",
            "Batch [1440/1563] - Loss: 0.3614, Avg Loss: 0.0241\n",
            "Batch [1450/1563] - Loss: 0.4103, Avg Loss: 0.0241\n",
            "Batch [1460/1563] - Loss: 0.4713, Avg Loss: 0.0241\n",
            "Batch [1470/1563] - Loss: 0.2510, Avg Loss: 0.0241\n",
            "Batch [1480/1563] - Loss: 0.7089, Avg Loss: 0.0241\n",
            "Batch [1490/1563] - Loss: 0.4950, Avg Loss: 0.0241\n",
            "Batch [1500/1563] - Loss: 0.2545, Avg Loss: 0.0241\n",
            "Batch [1510/1563] - Loss: 0.4847, Avg Loss: 0.0241\n",
            "Batch [1520/1563] - Loss: 0.2442, Avg Loss: 0.0240\n",
            "Batch [1530/1563] - Loss: 0.4300, Avg Loss: 0.0240\n",
            "Batch [1540/1563] - Loss: 0.2845, Avg Loss: 0.0240\n",
            "Batch [1550/1563] - Loss: 0.3725, Avg Loss: 0.0241\n",
            "Batch [1560/1563] - Loss: 0.6107, Avg Loss: 0.0241\n",
            "Epoch completed - Total samples: 25000, Average Loss: 0.0241\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:44, 35.10it/s]Eval Batch [20/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.34it/s]Eval Batch [40/1563] - Current Accuracy: 0.8594\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 38.95it/s]Eval Batch [60/1563] - Current Accuracy: 0.8448\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:38, 39.07it/s]Eval Batch [80/1563] - Current Accuracy: 0.8516\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 39.18it/s]Eval Batch [100/1563] - Current Accuracy: 0.8581\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:36, 39.09it/s]Eval Batch [120/1563] - Current Accuracy: 0.8599\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 39.12it/s]Eval Batch [140/1563] - Current Accuracy: 0.8603\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:35, 39.17it/s]Eval Batch [160/1563] - Current Accuracy: 0.8516\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 39.15it/s]Eval Batch [180/1563] - Current Accuracy: 0.8517\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:34, 39.13it/s]Eval Batch [200/1563] - Current Accuracy: 0.8550\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 39.19it/s]Eval Batch [220/1563] - Current Accuracy: 0.8551\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:34, 38.96it/s]Eval Batch [240/1563] - Current Accuracy: 0.8576\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 39.15it/s]Eval Batch [260/1563] - Current Accuracy: 0.8596\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:32, 39.11it/s]Eval Batch [280/1563] - Current Accuracy: 0.8612\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 39.10it/s]Eval Batch [300/1563] - Current Accuracy: 0.8598\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:31, 39.06it/s]Eval Batch [320/1563] - Current Accuracy: 0.8607\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 38.99it/s]Eval Batch [340/1563] - Current Accuracy: 0.8608\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:30, 39.02it/s]Eval Batch [360/1563] - Current Accuracy: 0.8611\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 39.12it/s]Eval Batch [380/1563] - Current Accuracy: 0.8597\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:29, 39.13it/s]Eval Batch [400/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 39.10it/s]Eval Batch [420/1563] - Current Accuracy: 0.8607\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:29, 38.70it/s]Eval Batch [440/1563] - Current Accuracy: 0.8594\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 38.98it/s]Eval Batch [460/1563] - Current Accuracy: 0.8584\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:28, 38.69it/s]Eval Batch [480/1563] - Current Accuracy: 0.8591\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 38.80it/s]Eval Batch [500/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 38.96it/s]Eval Batch [520/1563] - Current Accuracy: 0.8599\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 39.09it/s]Eval Batch [540/1563] - Current Accuracy: 0.8593\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 38.89it/s]Eval Batch [560/1563] - Current Accuracy: 0.8583\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 39.14it/s]Eval Batch [580/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 39.09it/s]Eval Batch [600/1563] - Current Accuracy: 0.8553\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 39.04it/s]Eval Batch [620/1563] - Current Accuracy: 0.8552\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 39.02it/s]Eval Batch [640/1563] - Current Accuracy: 0.8556\n",
            "Evaluating SNN:  42% 657/1563 [00:16<00:23, 38.91it/s]Eval Batch [660/1563] - Current Accuracy: 0.8566\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 39.11it/s]Eval Batch [680/1563] - Current Accuracy: 0.8579\n",
            "Evaluating SNN:  45% 697/1563 [00:17<00:22, 39.17it/s]Eval Batch [700/1563] - Current Accuracy: 0.8578\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 39.02it/s]Eval Batch [720/1563] - Current Accuracy: 0.8570\n",
            "Evaluating SNN:  47% 737/1563 [00:19<00:21, 38.79it/s]Eval Batch [740/1563] - Current Accuracy: 0.8568\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 38.84it/s]Eval Batch [760/1563] - Current Accuracy: 0.8571\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 38.89it/s]Eval Batch [780/1563] - Current Accuracy: 0.8567\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 38.74it/s]Eval Batch [800/1563] - Current Accuracy: 0.8567\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 38.90it/s]Eval Batch [820/1563] - Current Accuracy: 0.8572\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 38.83it/s]Eval Batch [840/1563] - Current Accuracy: 0.8554\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 38.84it/s]Eval Batch [860/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 38.75it/s]Eval Batch [880/1563] - Current Accuracy: 0.8548\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 38.99it/s]Eval Batch [900/1563] - Current Accuracy: 0.8542\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 38.98it/s]Eval Batch [920/1563] - Current Accuracy: 0.8545\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:16, 39.03it/s]Eval Batch [940/1563] - Current Accuracy: 0.8545\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 39.15it/s]Eval Batch [960/1563] - Current Accuracy: 0.8540\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:14, 39.21it/s]Eval Batch [980/1563] - Current Accuracy: 0.8541\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 39.23it/s]Eval Batch [1000/1563] - Current Accuracy: 0.8548\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:13, 39.26it/s]Eval Batch [1020/1563] - Current Accuracy: 0.8536\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 39.26it/s]Eval Batch [1040/1563] - Current Accuracy: 0.8526\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:12, 39.22it/s]Eval Batch [1060/1563] - Current Accuracy: 0.8519\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 39.24it/s]Eval Batch [1080/1563] - Current Accuracy: 0.8505\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:11, 39.26it/s]Eval Batch [1100/1563] - Current Accuracy: 0.8506\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 39.24it/s]Eval Batch [1120/1563] - Current Accuracy: 0.8512\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 39.25it/s]Eval Batch [1140/1563] - Current Accuracy: 0.8503\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 39.30it/s]Eval Batch [1160/1563] - Current Accuracy: 0.8495\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 39.12it/s]Eval Batch [1180/1563] - Current Accuracy: 0.8498\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 38.97it/s]Eval Batch [1200/1563] - Current Accuracy: 0.8498\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 39.05it/s]Eval Batch [1220/1563] - Current Accuracy: 0.8494\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 39.07it/s]Eval Batch [1240/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 39.00it/s]Eval Batch [1260/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 39.11it/s]Eval Batch [1280/1563] - Current Accuracy: 0.8497\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 39.16it/s]Eval Batch [1300/1563] - Current Accuracy: 0.8500\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 39.09it/s]Eval Batch [1320/1563] - Current Accuracy: 0.8496\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 39.04it/s]Eval Batch [1340/1563] - Current Accuracy: 0.8493\n",
            "Evaluating SNN:  87% 1357/1563 [00:34<00:05, 39.16it/s]Eval Batch [1360/1563] - Current Accuracy: 0.8499\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 38.98it/s]Eval Batch [1380/1563] - Current Accuracy: 0.8497\n",
            "Evaluating SNN:  89% 1397/1563 [00:35<00:04, 39.13it/s]Eval Batch [1400/1563] - Current Accuracy: 0.8488\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 39.15it/s]Eval Batch [1420/1563] - Current Accuracy: 0.8486\n",
            "Evaluating SNN:  92% 1437/1563 [00:36<00:03, 39.19it/s]Eval Batch [1440/1563] - Current Accuracy: 0.8484\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 39.24it/s]Eval Batch [1460/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  94% 1477/1563 [00:37<00:02, 39.15it/s]Eval Batch [1480/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 39.25it/s]Eval Batch [1500/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  97% 1517/1563 [00:38<00:01, 39.14it/s]Eval Batch [1520/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 39.16it/s]Eval Batch [1540/1563] - Current Accuracy: 0.8486\n",
            "Evaluating SNN: 100% 1557/1563 [00:39<00:00, 39.19it/s]Eval Batch [1560/1563] - Current Accuracy: 0.8478\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.92it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.8476\n",
            "INFO:root:Saved model weights at epoch 4 to: ./checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_4.pth\n",
            "[2025-09-24 18:04:37,668][main.py][line:639][INFO] Saved model weights at epoch 4 to: ./checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_4.pth\n",
            "INFO:root:Save directory exists: True\n",
            "[2025-09-24 18:04:37,669][main.py][line:640][INFO] Save directory exists: True\n",
            "INFO:root:File size: 265511595 bytes\n",
            "[2025-09-24 18:04:37,669][main.py][line:641][INFO] File size: 265511595 bytes\n",
            "INFO:root:SNNs training Epoch 4 [distilbert_base_qcfs]: Val_loss: 0.02406073837995529\n",
            "[2025-09-24 18:04:37,669][main.py][line:644][INFO] SNNs training Epoch 4 [distilbert_base_qcfs]: Val_loss: 0.02406073837995529\n",
            "INFO:root:SNNs training Epoch 4 [distilbert_base_qcfs]: Test Acc: 0.8476 Best Acc: 0.8476\n",
            "[2025-09-24 18:04:37,669][main.py][line:645][INFO] SNNs training Epoch 4 [distilbert_base_qcfs]: Test Acc: 0.8476 Best Acc: 0.8476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load checkpoint and run calibration\n",
        "!python Parallel_Conversion/main.py \\\n",
        "    --dataset TextCLS \\\n",
        "    --net_arch distilbert_base_qcfs \\\n",
        "    --neuron_type ParaInfNeuron \\\n",
        "    --text_dataset imdb \\\n",
        "    --text_max_len 256 \\\n",
        "    --time_step 4 \\\n",
        "    --batchsize 16 \\\n",
        "    --dev 0 \\\n",
        "    --calibrate_th \\\n",
        "    --direct_inference \\\n",
        "    --pretrained_model \\\n",
        "    --checkpoint_path /content/checkpointsTextCLS-distilbert_base_qcfs-T4/ParaInfNeuron_lr1e-05_wd0.0005_epoch5_mixup_False_weights_epoch_4.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuizkbYyCBsE",
        "outputId": "41e9976f-d361-4e81-efc6-d20e87584250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-24 18:04:45.295613: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-09-24 18:04:45.314225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758737085.335530   21929 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758737085.342006   21929 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758737085.358369   21929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758737085.358398   21929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758737085.358401   21929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758737085.358404   21929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-24 18:04:45.363255: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Not using distributed mode\n",
            "total parameters: 66.365956 M.\n",
            "INFO:root:Calibrate Inference (Text) [distilbert_base_qcfs]: Test Acc: 0.8494\n",
            "[2025-09-24 18:06:16,482][main.py][line:507][INFO] Calibrate Inference (Text) [distilbert_base_qcfs]: Test Acc: 0.8494\n",
            "Starting evaluation with 1563 batches...\n",
            "Evaluating SNN:   1% 17/1563 [00:00<00:43, 35.50it/s]Eval Batch [20/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:   2% 37/1563 [00:01<00:39, 38.54it/s]Eval Batch [40/1563] - Current Accuracy: 0.8594\n",
            "Evaluating SNN:   4% 57/1563 [00:01<00:38, 38.85it/s]Eval Batch [60/1563] - Current Accuracy: 0.8448\n",
            "Evaluating SNN:   5% 77/1563 [00:02<00:38, 38.91it/s]Eval Batch [80/1563] - Current Accuracy: 0.8516\n",
            "Evaluating SNN:   6% 97/1563 [00:02<00:37, 39.07it/s]Eval Batch [100/1563] - Current Accuracy: 0.8581\n",
            "Evaluating SNN:   7% 117/1563 [00:03<00:36, 39.14it/s]Eval Batch [120/1563] - Current Accuracy: 0.8599\n",
            "Evaluating SNN:   9% 137/1563 [00:03<00:36, 39.01it/s]Eval Batch [140/1563] - Current Accuracy: 0.8603\n",
            "Evaluating SNN:  10% 157/1563 [00:04<00:36, 39.00it/s]Eval Batch [160/1563] - Current Accuracy: 0.8516\n",
            "Evaluating SNN:  11% 177/1563 [00:04<00:35, 39.11it/s]Eval Batch [180/1563] - Current Accuracy: 0.8517\n",
            "Evaluating SNN:  13% 197/1563 [00:05<00:34, 39.05it/s]Eval Batch [200/1563] - Current Accuracy: 0.8550\n",
            "Evaluating SNN:  14% 217/1563 [00:05<00:34, 39.14it/s]Eval Batch [220/1563] - Current Accuracy: 0.8551\n",
            "Evaluating SNN:  15% 237/1563 [00:06<00:34, 38.94it/s]Eval Batch [240/1563] - Current Accuracy: 0.8576\n",
            "Evaluating SNN:  16% 257/1563 [00:06<00:33, 38.96it/s]Eval Batch [260/1563] - Current Accuracy: 0.8596\n",
            "Evaluating SNN:  18% 277/1563 [00:07<00:33, 38.78it/s]Eval Batch [280/1563] - Current Accuracy: 0.8612\n",
            "Evaluating SNN:  19% 297/1563 [00:07<00:32, 38.86it/s]Eval Batch [300/1563] - Current Accuracy: 0.8598\n",
            "Evaluating SNN:  20% 317/1563 [00:08<00:31, 39.02it/s]Eval Batch [320/1563] - Current Accuracy: 0.8607\n",
            "Evaluating SNN:  22% 337/1563 [00:08<00:31, 39.07it/s]Eval Batch [340/1563] - Current Accuracy: 0.8608\n",
            "Evaluating SNN:  23% 357/1563 [00:09<00:30, 39.13it/s]Eval Batch [360/1563] - Current Accuracy: 0.8611\n",
            "Evaluating SNN:  24% 377/1563 [00:09<00:30, 39.11it/s]Eval Batch [380/1563] - Current Accuracy: 0.8597\n",
            "Evaluating SNN:  25% 397/1563 [00:10<00:29, 39.09it/s]Eval Batch [400/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  27% 417/1563 [00:10<00:29, 39.12it/s]Eval Batch [420/1563] - Current Accuracy: 0.8607\n",
            "Evaluating SNN:  28% 437/1563 [00:11<00:28, 38.92it/s]Eval Batch [440/1563] - Current Accuracy: 0.8594\n",
            "Evaluating SNN:  29% 457/1563 [00:11<00:28, 39.02it/s]Eval Batch [460/1563] - Current Accuracy: 0.8584\n",
            "Evaluating SNN:  31% 477/1563 [00:12<00:28, 38.64it/s]Eval Batch [480/1563] - Current Accuracy: 0.8591\n",
            "Evaluating SNN:  32% 497/1563 [00:12<00:27, 38.63it/s]Eval Batch [500/1563] - Current Accuracy: 0.8592\n",
            "Evaluating SNN:  33% 517/1563 [00:13<00:26, 38.75it/s]Eval Batch [520/1563] - Current Accuracy: 0.8599\n",
            "Evaluating SNN:  34% 537/1563 [00:13<00:26, 38.85it/s]Eval Batch [540/1563] - Current Accuracy: 0.8593\n",
            "Evaluating SNN:  36% 557/1563 [00:14<00:25, 38.78it/s]Eval Batch [560/1563] - Current Accuracy: 0.8583\n",
            "Evaluating SNN:  37% 577/1563 [00:14<00:25, 38.82it/s]Eval Batch [580/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:  38% 597/1563 [00:15<00:24, 38.99it/s]Eval Batch [600/1563] - Current Accuracy: 0.8553\n",
            "Evaluating SNN:  39% 617/1563 [00:15<00:24, 39.15it/s]Eval Batch [620/1563] - Current Accuracy: 0.8552\n",
            "Evaluating SNN:  41% 637/1563 [00:16<00:23, 39.14it/s]Eval Batch [640/1563] - Current Accuracy: 0.8556\n",
            "Evaluating SNN:  42% 657/1563 [00:16<00:23, 39.15it/s]Eval Batch [660/1563] - Current Accuracy: 0.8566\n",
            "Evaluating SNN:  43% 677/1563 [00:17<00:22, 39.05it/s]Eval Batch [680/1563] - Current Accuracy: 0.8579\n",
            "Evaluating SNN:  45% 697/1563 [00:18<00:22, 38.85it/s]Eval Batch [700/1563] - Current Accuracy: 0.8578\n",
            "Evaluating SNN:  46% 717/1563 [00:18<00:21, 39.01it/s]Eval Batch [720/1563] - Current Accuracy: 0.8570\n",
            "Evaluating SNN:  47% 737/1563 [00:19<00:21, 38.68it/s]Eval Batch [740/1563] - Current Accuracy: 0.8568\n",
            "Evaluating SNN:  48% 757/1563 [00:19<00:20, 38.93it/s]Eval Batch [760/1563] - Current Accuracy: 0.8571\n",
            "Evaluating SNN:  50% 777/1563 [00:20<00:20, 38.73it/s]Eval Batch [780/1563] - Current Accuracy: 0.8567\n",
            "Evaluating SNN:  51% 797/1563 [00:20<00:19, 38.92it/s]Eval Batch [800/1563] - Current Accuracy: 0.8567\n",
            "Evaluating SNN:  52% 817/1563 [00:21<00:19, 38.98it/s]Eval Batch [820/1563] - Current Accuracy: 0.8572\n",
            "Evaluating SNN:  54% 837/1563 [00:21<00:18, 39.13it/s]Eval Batch [840/1563] - Current Accuracy: 0.8554\n",
            "Evaluating SNN:  55% 857/1563 [00:22<00:18, 39.05it/s]Eval Batch [860/1563] - Current Accuracy: 0.8562\n",
            "Evaluating SNN:  56% 877/1563 [00:22<00:17, 38.91it/s]Eval Batch [880/1563] - Current Accuracy: 0.8548\n",
            "Evaluating SNN:  57% 897/1563 [00:23<00:17, 39.03it/s]Eval Batch [900/1563] - Current Accuracy: 0.8542\n",
            "Evaluating SNN:  59% 917/1563 [00:23<00:16, 39.13it/s]Eval Batch [920/1563] - Current Accuracy: 0.8545\n",
            "Evaluating SNN:  60% 937/1563 [00:24<00:16, 39.06it/s]Eval Batch [940/1563] - Current Accuracy: 0.8545\n",
            "Evaluating SNN:  61% 957/1563 [00:24<00:15, 38.89it/s]Eval Batch [960/1563] - Current Accuracy: 0.8540\n",
            "Evaluating SNN:  63% 977/1563 [00:25<00:15, 38.95it/s]Eval Batch [980/1563] - Current Accuracy: 0.8541\n",
            "Evaluating SNN:  64% 997/1563 [00:25<00:14, 38.98it/s]Eval Batch [1000/1563] - Current Accuracy: 0.8548\n",
            "Evaluating SNN:  65% 1017/1563 [00:26<00:13, 39.09it/s]Eval Batch [1020/1563] - Current Accuracy: 0.8536\n",
            "Evaluating SNN:  66% 1037/1563 [00:26<00:13, 38.88it/s]Eval Batch [1040/1563] - Current Accuracy: 0.8526\n",
            "Evaluating SNN:  68% 1057/1563 [00:27<00:12, 39.09it/s]Eval Batch [1060/1563] - Current Accuracy: 0.8519\n",
            "Evaluating SNN:  69% 1077/1563 [00:27<00:12, 39.10it/s]Eval Batch [1080/1563] - Current Accuracy: 0.8505\n",
            "Evaluating SNN:  70% 1097/1563 [00:28<00:11, 39.14it/s]Eval Batch [1100/1563] - Current Accuracy: 0.8506\n",
            "Evaluating SNN:  71% 1117/1563 [00:28<00:11, 38.88it/s]Eval Batch [1120/1563] - Current Accuracy: 0.8512\n",
            "Evaluating SNN:  73% 1137/1563 [00:29<00:10, 38.81it/s]Eval Batch [1140/1563] - Current Accuracy: 0.8503\n",
            "Evaluating SNN:  74% 1157/1563 [00:29<00:10, 38.97it/s]Eval Batch [1160/1563] - Current Accuracy: 0.8495\n",
            "Evaluating SNN:  75% 1177/1563 [00:30<00:09, 39.00it/s]Eval Batch [1180/1563] - Current Accuracy: 0.8498\n",
            "Evaluating SNN:  77% 1197/1563 [00:30<00:09, 38.77it/s]Eval Batch [1200/1563] - Current Accuracy: 0.8498\n",
            "Evaluating SNN:  78% 1217/1563 [00:31<00:08, 39.10it/s]Eval Batch [1220/1563] - Current Accuracy: 0.8494\n",
            "Evaluating SNN:  79% 1237/1563 [00:31<00:08, 39.12it/s]Eval Batch [1240/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  80% 1257/1563 [00:32<00:07, 39.11it/s]Eval Batch [1260/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  82% 1277/1563 [00:32<00:07, 38.96it/s]Eval Batch [1280/1563] - Current Accuracy: 0.8497\n",
            "Evaluating SNN:  83% 1297/1563 [00:33<00:06, 39.08it/s]Eval Batch [1300/1563] - Current Accuracy: 0.8500\n",
            "Evaluating SNN:  84% 1317/1563 [00:33<00:06, 39.07it/s]Eval Batch [1320/1563] - Current Accuracy: 0.8496\n",
            "Evaluating SNN:  86% 1337/1563 [00:34<00:05, 39.17it/s]Eval Batch [1340/1563] - Current Accuracy: 0.8493\n",
            "Evaluating SNN:  87% 1357/1563 [00:34<00:05, 39.02it/s]Eval Batch [1360/1563] - Current Accuracy: 0.8499\n",
            "Evaluating SNN:  88% 1377/1563 [00:35<00:04, 39.23it/s]Eval Batch [1380/1563] - Current Accuracy: 0.8497\n",
            "Evaluating SNN:  89% 1397/1563 [00:35<00:04, 39.13it/s]Eval Batch [1400/1563] - Current Accuracy: 0.8488\n",
            "Evaluating SNN:  91% 1417/1563 [00:36<00:03, 39.09it/s]Eval Batch [1420/1563] - Current Accuracy: 0.8486\n",
            "Evaluating SNN:  92% 1437/1563 [00:36<00:03, 38.67it/s]Eval Batch [1440/1563] - Current Accuracy: 0.8484\n",
            "Evaluating SNN:  93% 1457/1563 [00:37<00:02, 38.78it/s]Eval Batch [1460/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  94% 1477/1563 [00:38<00:02, 38.85it/s]Eval Batch [1480/1563] - Current Accuracy: 0.8492\n",
            "Evaluating SNN:  96% 1497/1563 [00:38<00:01, 39.16it/s]Eval Batch [1500/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  97% 1517/1563 [00:39<00:01, 39.12it/s]Eval Batch [1520/1563] - Current Accuracy: 0.8489\n",
            "Evaluating SNN:  98% 1537/1563 [00:39<00:00, 39.02it/s]Eval Batch [1540/1563] - Current Accuracy: 0.8486\n",
            "Evaluating SNN: 100% 1557/1563 [00:40<00:00, 39.06it/s]Eval Batch [1560/1563] - Current Accuracy: 0.8478\n",
            "Evaluating SNN: 100% 1563/1563 [00:40<00:00, 38.84it/s]\n",
            "Evaluation completed - Total samples: 25000, Accuracy: 0.8476\n",
            "INFO:root:Standard BERT Inference [distilbert_base_qcfs]: Test Acc: 0.8476\n",
            "[2025-09-24 18:06:56,729][main.py][line:522][INFO] Standard BERT Inference [distilbert_base_qcfs]: Test Acc: 0.8476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# see the FLOPs"
      ],
      "metadata": {
        "id": "5mLElLL_1UFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save the model weight"
      ],
      "metadata": {
        "id": "eUif6rfgvg7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/checkpoints-bert-standardTextCLS-distilbert_base-T4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BDdaJW3sOvg",
        "outputId": "f0169804-78eb-46cb-b83c-b2c97306e9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "zip error: Nothing to do! (/content/checkpoints-bert-standardTextCLS-distilbert_base-T4.zip)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/checkpointsTextCLS-distilbert_base_qcfs-T4.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "y9wxVWTovpGC",
        "outputId": "bb1ccd5b-1d18-43d1-dab1-6d98b76f97a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a2e81149-adbc-4ab2-9bb5-ac45407f1944\", \"checkpointsTextCLS-distilbert_base_qcfs-T4.zip\", 2447275528)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}